{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The personal website of Ed Nunes This website serves as a repository and public display of my writings. Most of my writing tends to be around technology, or the process for making technology. On this site you will find work that is: * technical, such as the book I wrote on deploying data engineering infrastructure on GCP, * personal, such as my essay on how I switched careers to become technologist, and * contemplative, such as my article on learning from failure. As noted above, this is my personal website. The opinions expressed here are my own. About Me I'm currently living in central Massachusetts, in the small town I grew up in, though I've been around these past 15 years or so: Williamsburg, VA; Washington, DC; Richmond, VA; Greenville, SC; Billings, MT; Chicago, IL. That list doesn't look as long as it feels. I'm a bit tired of moving around and am on the lookout for a long-term home. I work as a Data Engineer for an e-commerce website, Zoro.com. I really enjoy being a data engineer, and spend a good bit of my free time reading about and playing with technology. I also like to write, occasionally, as you may have noticed. My other main hobbies right now are weight lifting and reading. If you want to get in touch with me the best way is through LinkedIn .","title":"Home"},{"location":"#the-personal-website-of-ed-nunes","text":"This website serves as a repository and public display of my writings. Most of my writing tends to be around technology, or the process for making technology. On this site you will find work that is: * technical, such as the book I wrote on deploying data engineering infrastructure on GCP, * personal, such as my essay on how I switched careers to become technologist, and * contemplative, such as my article on learning from failure. As noted above, this is my personal website. The opinions expressed here are my own.","title":"The personal website of Ed Nunes"},{"location":"#about-me","text":"I'm currently living in central Massachusetts, in the small town I grew up in, though I've been around these past 15 years or so: Williamsburg, VA; Washington, DC; Richmond, VA; Greenville, SC; Billings, MT; Chicago, IL. That list doesn't look as long as it feels. I'm a bit tired of moving around and am on the lookout for a long-term home. I work as a Data Engineer for an e-commerce website, Zoro.com. I really enjoy being a data engineer, and spend a good bit of my free time reading about and playing with technology. I also like to write, occasionally, as you may have noticed. My other main hobbies right now are weight lifting and reading. If you want to get in touch with me the best way is through LinkedIn .","title":"About Me"},{"location":"articles/i_was_a_lawyer/","text":"I was a Lawyer. Now I\u2019m a Data Engineer. This is my story. Published: 2018-04-12 I have a career that I genuinely enjoy, but it wasn\u2019t always this way. There was a time where I thought Same S?*! Different Day would be my motto until I retired in the distant future. It was a series of (seemingly) logical steps that led me towards a miserable career, and a series of minor decisions (and a fateful poker game) that led me to a career I love. This is how it happened. Lots of Education, but Not Too Smart Upon finishing high school, I prepared to enter a reasonably well-regarded state university. Around this time I had concluded, with all the confidence of the inexperienced, that math and science were hard and the humanities were easy. Having no clue what I wanted to do with myself, I decided to take easy classes (preferably that met in the afternoon) while I figured it out. After two years of college, I still hadn\u2019t decided on a career, but I did figure out how to kick the can down the road for three more years: law school. I saw my friends working on engineering and computer science degrees, and I had some inkling that maybe I should be doing something with technology. But at that point, I had adopted flawed thinking that would plague me for years: the Sunk Cost Fallacy. I had non-technical college courses under my belt, and switching to a technical major would make those courses worthless. It would be wasteful to pursue a technical degree, I reasoned, even if I found technology interesting. So, on I went with my education. I got into a prestigious law school, moved from Massachusetts to Virginia (no more shoveling snow!), and spent three years learning the law. During that time, I continued anticipating some sort of passion for the law to grab hold of me, but I was to be disappointed. In undergrad, I thought the enticement of a large salary would drive me to success, but as I got closer to starting my career, I realized that money didn\u2019t have the allure I thought it would. I was a walking contradiction: both my apprehension at pursuing a career I had marginal interest in, and my commitment to that career, grew. I remember scoffing at how foolish classmates were for dropping out after spending so much time and money on their law careers. Yet, I was ignoring how foolish it is to spend years pursuing a professional career I had marginal interest in. As graduation approached, I felt more trapped than ever \u2014 though I had trouble admitting it to myself. I graduated, got my law license, and got a job. Graduating in 2010, in the middle of the largest recession in a generation, meant that even the solace of a high wage for boring work was gone. I Went To Work My fears were realized: I became a cog doing boring work in a place where I did not feel appreciated. That I had seen this coming for so long made it all the more bitter. I ended up processing claims for a legal settlement of the 2010 Gulf Coast oil spill. When I eventually shook off my funk from feeling trapped in my career, I took a look around at the people who were getting promoted at my company. I noticed a common thread: everybody who was advancing was doing significant work in Excel. In my quarterly meeting with my supervisor (which was usually the only time we talked), I told him I wanted to do more work in Excel, and he agreed to send more Excel work my way. That simple request transformed my professional life. I did learn Excel and got quite good at it. It was challenging and exciting and powerful. I learned some VBA, and for the first time was using programming to complete my work. It felt like something had opened up in my life. Work became more than just a chore needed to pull in a paycheck. It became a place to learn and grow and build. When I moved to DC for my wife\u2019s new job, I made a decision that was a long time coming: the law was behind me and technology was my professional future. Bypassing the Resume Rat Race Over a Game of Cards Of course, the decision to pursue work as a data analyst ended up being much harder to realize than I thought. While I had essentially been working as a data analyst for three years, my title had been \u201cSpecial Counsel.\u201d Of the people that actually responded to my applications, the general response was skepticism at my qualifications. On paper, I was a lawyer, not a data analyst, and I rarely got the opportunity to explain further. After a month with only a few interviews and no offers, I got tired of eating eggs for dinner and resigned myself to taking whatever job I could find. All the hard work I put into my resume and cover letters never did pay off. Instead, a friend invited me to a poker game and it came up during conversation that I was looking for work. A person I\u2019d never met before had a friend looking to hire a lawyer. And then I was interviewing at the company two days later. At this point, I had assurances (but no formal offer) from another company that they could hire me as an analyst in a couple months. I didn\u2019t know what this new position that fell into my lap required, but I figured that I could tough it out for a couple months until the other job came through. In the interview, I found out that they were working in the same industry as my last job (mass claims administration) and were looking for someone with Excel skills to do some data analysis. I doubt there was a person more qualified in the entire city, but the job was essentially unlisted. If not for that poker game, I expect my career would have been quite different. So for six months, I did work similar to what I was doing before. My new job was a slight step down in seniority, but the culture was good, my boss approachable, and my colleagues smart and fun. I was assured there was the opportunity for rapid advancement, and that assurance held true. I Became a Data Engineer After six months a position opened up on the three-person Data Operations team in the same company. At that point, my analytical work had gained recognition throughout our department, and after a brief interview, the job was mine. For the first time, I was working in a position where my exclusive and explicit responsibility was to support data analytics products. Unlike my previous work, most of the work for this job was done in SQL. During the interview, I was informed that my lack of experience in SQL was not a problem and I would be trained by my other team members. Which is why it was disconcerting when, on my first day, my new team lead informed me he was putting in his two-weeks notice. Management scrambled to reorganize, and in two weeks I found myself on a team with two other people with similar skills and experience to my own and a simple mandate: the weekly deliverables must continue to go out and they must be of high quality. For everything else, like how to manage a codebase our team could barely read, we were left to our own devices. Facing this new and exciting and daunting task, I did the only thing that made sense to me: I immersed myself in SQL and didn\u2019t come up until I felt like I knew what was going on. I read every book I could get my hands on (my favorites being SQL Queries for Mere Mortals and T-SQL Fundamentals). I stayed late, came in on weekends, and got weird looks from pub patrons as I sat in the corner drinking a pint with a giant manual open in front of me. Like all technology, there\u2019s no magic, and with enough study and familiarity, the mysterious becomes mundane. That\u2019s not to say that all it took was a little hard work and I was an expert. In my ignorance, I made many mistakes and bad decisions, some of which linger on in obscure corners of our code base. Being thrown into the deep end gave me the opportunity to rise to new challenges, and the incentive to grow professionally. That being said, the value of having an experienced developer to guide a project cannot be understated. Later, my team would more closely integrate with our division\u2019s software developer and operations teams, and their insight continues to be invaluable. Now I use SQL, Python, and JavaScript in my regular work, among many other technologies. Knowing that investments in my skills will directly improve the capabilities of my team provides a rewarding experience that encourages me towards continual learning. Reading technical articles can be intimidating as I continue to realize everything I don\u2019t know. But at the same time, there\u2019s the promise that through my hard work I can empower myself to do amazing things. Looking Back While I don\u2019t particularly appreciate my monthly student loan payments, I can honestly say that my educational experience has been valuable to me. Communication and analytical thinking are a large part of my daily work, and my years in college helped hone those skills. So while I would not necessarily choose this path for anyone else, I can appreciate the value my diverse experience has given me. The biggest piece of advice I can glean from my unorthodox career path is this: don\u2019t worry about where you\u2019ve been, think about where you want to go, and then figure out how to get there.","title":"I was a lawyer. Now I'm a data engineer."},{"location":"articles/i_was_a_lawyer/#i-was-a-lawyer-now-im-a-data-engineer-this-is-my-story","text":"Published: 2018-04-12 I have a career that I genuinely enjoy, but it wasn\u2019t always this way. There was a time where I thought Same S?*! Different Day would be my motto until I retired in the distant future. It was a series of (seemingly) logical steps that led me towards a miserable career, and a series of minor decisions (and a fateful poker game) that led me to a career I love. This is how it happened.","title":"I was a Lawyer. Now I\u2019m a Data Engineer. This is my story."},{"location":"articles/i_was_a_lawyer/#lots-of-education-but-not-too-smart","text":"Upon finishing high school, I prepared to enter a reasonably well-regarded state university. Around this time I had concluded, with all the confidence of the inexperienced, that math and science were hard and the humanities were easy. Having no clue what I wanted to do with myself, I decided to take easy classes (preferably that met in the afternoon) while I figured it out. After two years of college, I still hadn\u2019t decided on a career, but I did figure out how to kick the can down the road for three more years: law school. I saw my friends working on engineering and computer science degrees, and I had some inkling that maybe I should be doing something with technology. But at that point, I had adopted flawed thinking that would plague me for years: the Sunk Cost Fallacy. I had non-technical college courses under my belt, and switching to a technical major would make those courses worthless. It would be wasteful to pursue a technical degree, I reasoned, even if I found technology interesting. So, on I went with my education. I got into a prestigious law school, moved from Massachusetts to Virginia (no more shoveling snow!), and spent three years learning the law. During that time, I continued anticipating some sort of passion for the law to grab hold of me, but I was to be disappointed. In undergrad, I thought the enticement of a large salary would drive me to success, but as I got closer to starting my career, I realized that money didn\u2019t have the allure I thought it would. I was a walking contradiction: both my apprehension at pursuing a career I had marginal interest in, and my commitment to that career, grew. I remember scoffing at how foolish classmates were for dropping out after spending so much time and money on their law careers. Yet, I was ignoring how foolish it is to spend years pursuing a professional career I had marginal interest in. As graduation approached, I felt more trapped than ever \u2014 though I had trouble admitting it to myself. I graduated, got my law license, and got a job. Graduating in 2010, in the middle of the largest recession in a generation, meant that even the solace of a high wage for boring work was gone.","title":"Lots of Education, but Not Too Smart"},{"location":"articles/i_was_a_lawyer/#i-went-to-work","text":"My fears were realized: I became a cog doing boring work in a place where I did not feel appreciated. That I had seen this coming for so long made it all the more bitter. I ended up processing claims for a legal settlement of the 2010 Gulf Coast oil spill. When I eventually shook off my funk from feeling trapped in my career, I took a look around at the people who were getting promoted at my company. I noticed a common thread: everybody who was advancing was doing significant work in Excel. In my quarterly meeting with my supervisor (which was usually the only time we talked), I told him I wanted to do more work in Excel, and he agreed to send more Excel work my way. That simple request transformed my professional life. I did learn Excel and got quite good at it. It was challenging and exciting and powerful. I learned some VBA, and for the first time was using programming to complete my work. It felt like something had opened up in my life. Work became more than just a chore needed to pull in a paycheck. It became a place to learn and grow and build. When I moved to DC for my wife\u2019s new job, I made a decision that was a long time coming: the law was behind me and technology was my professional future.","title":"I Went To Work"},{"location":"articles/i_was_a_lawyer/#bypassing-the-resume-rat-race-over-a-game-of-cards","text":"Of course, the decision to pursue work as a data analyst ended up being much harder to realize than I thought. While I had essentially been working as a data analyst for three years, my title had been \u201cSpecial Counsel.\u201d Of the people that actually responded to my applications, the general response was skepticism at my qualifications. On paper, I was a lawyer, not a data analyst, and I rarely got the opportunity to explain further. After a month with only a few interviews and no offers, I got tired of eating eggs for dinner and resigned myself to taking whatever job I could find. All the hard work I put into my resume and cover letters never did pay off. Instead, a friend invited me to a poker game and it came up during conversation that I was looking for work. A person I\u2019d never met before had a friend looking to hire a lawyer. And then I was interviewing at the company two days later. At this point, I had assurances (but no formal offer) from another company that they could hire me as an analyst in a couple months. I didn\u2019t know what this new position that fell into my lap required, but I figured that I could tough it out for a couple months until the other job came through. In the interview, I found out that they were working in the same industry as my last job (mass claims administration) and were looking for someone with Excel skills to do some data analysis. I doubt there was a person more qualified in the entire city, but the job was essentially unlisted. If not for that poker game, I expect my career would have been quite different. So for six months, I did work similar to what I was doing before. My new job was a slight step down in seniority, but the culture was good, my boss approachable, and my colleagues smart and fun. I was assured there was the opportunity for rapid advancement, and that assurance held true.","title":"Bypassing the Resume Rat Race Over a Game of Cards"},{"location":"articles/i_was_a_lawyer/#i-became-a-data-engineer","text":"After six months a position opened up on the three-person Data Operations team in the same company. At that point, my analytical work had gained recognition throughout our department, and after a brief interview, the job was mine. For the first time, I was working in a position where my exclusive and explicit responsibility was to support data analytics products. Unlike my previous work, most of the work for this job was done in SQL. During the interview, I was informed that my lack of experience in SQL was not a problem and I would be trained by my other team members. Which is why it was disconcerting when, on my first day, my new team lead informed me he was putting in his two-weeks notice. Management scrambled to reorganize, and in two weeks I found myself on a team with two other people with similar skills and experience to my own and a simple mandate: the weekly deliverables must continue to go out and they must be of high quality. For everything else, like how to manage a codebase our team could barely read, we were left to our own devices. Facing this new and exciting and daunting task, I did the only thing that made sense to me: I immersed myself in SQL and didn\u2019t come up until I felt like I knew what was going on. I read every book I could get my hands on (my favorites being SQL Queries for Mere Mortals and T-SQL Fundamentals). I stayed late, came in on weekends, and got weird looks from pub patrons as I sat in the corner drinking a pint with a giant manual open in front of me. Like all technology, there\u2019s no magic, and with enough study and familiarity, the mysterious becomes mundane. That\u2019s not to say that all it took was a little hard work and I was an expert. In my ignorance, I made many mistakes and bad decisions, some of which linger on in obscure corners of our code base. Being thrown into the deep end gave me the opportunity to rise to new challenges, and the incentive to grow professionally. That being said, the value of having an experienced developer to guide a project cannot be understated. Later, my team would more closely integrate with our division\u2019s software developer and operations teams, and their insight continues to be invaluable. Now I use SQL, Python, and JavaScript in my regular work, among many other technologies. Knowing that investments in my skills will directly improve the capabilities of my team provides a rewarding experience that encourages me towards continual learning. Reading technical articles can be intimidating as I continue to realize everything I don\u2019t know. But at the same time, there\u2019s the promise that through my hard work I can empower myself to do amazing things.","title":"I Became a Data Engineer"},{"location":"articles/i_was_a_lawyer/#looking-back","text":"While I don\u2019t particularly appreciate my monthly student loan payments, I can honestly say that my educational experience has been valuable to me. Communication and analytical thinking are a large part of my daily work, and my years in college helped hone those skills. So while I would not necessarily choose this path for anyone else, I can appreciate the value my diverse experience has given me. The biggest piece of advice I can glean from my unorthodox career path is this: don\u2019t worry about where you\u2019ve been, think about where you want to go, and then figure out how to get there.","title":"Looking Back"},{"location":"articles/learn_from_failure/","text":"How Great Teams Learn From Failure Published: 2018-06-11 Mistakes, when handled appropriately, can be pretty great. Your team has identified a failure point, addressed it, and everyone has learned from the experience. Making mistakes is one of the main ways people experience professional growth. All the benefits of mistakes go away if the team does not handle the failure effectively. Below is an explanation of how teams turn failure into a purely negative experience, followed by strategies for benefiting from failure. \u201cWhat are they going to do about it?\u201d I asked my wife after she described a mistake someone made at her work. She explained how senior leadership would perform an investigation that: 1. Would determine who was at fault for the error, and 2. Determine what repercussions the perpetrator should receive. And while this investigation is going on everyone else awkwardly avoids the topic. This is Bad. And because my wife works in healthcare, it\u2019s also Scary. Unfortunately, this is a common strategy that managers undertake to deal with failure. Correcting Bad Behavior In the above example the people with the bad behavior are the senior leadership conducting the investigation. The purpose of their investigation was to find a person to blame. This technique causes several problems: 1. Identifying a person gives the false impression that the point of failure has been addressed, allowing the failure to recur. 2. Morale is lowered as team members become adversarial towards each other in attempts to shift blame. 3. Team members are encouraged to work contrary to the goals of leadership, as they attempt to avoid punishment for failure. 4. Team members do not collaborate on how to correct the problem for fear of getting caught up in the investigation. 5. The opportunity for the team to learn from the failure is wasted. The result is a team that is worse off than before the investigation started. The first priority should be to ensure that the failure does not recur. This can only be done if the cause of the failure is correctly identified, and the systemic fixes are put in place to mitigate a reoccurrence. Instead of asking the question \u201cWho is to blame?\u201d, the investigators should ask \u201cWhat can be done to ensure this problem doesn\u2019t recur?\u201d Rather than having senior managers engage in an investigation by interviewing subordinates, management and subordinates should work together to identify and correct systemic problems. Mistakes Happen One of the problems that misguided managers make is to set an expectation that mistakes are not acceptable. Instead of establishing a culture of excellence on a team, this expectation encourages team members to hide their mistakes. This tendency can turn small learning moments into major problems. Punishing someone for making a mistake is like punishing someone for sneezing. Mistakes are inevitable. What is not inevitable is identifying mistakes and learning from them. Failures Are Systemic, not Personal People don\u2019t intend to fail. They\u2019d prefer to do good work that others appreciate. Failure happens despite this intention. The reason that a person may be unable to realize their intention of doing good work is always because of a systemic problem. You can tell if a manager doesn\u2019t get this concept if their conclusion for avoiding a particular failure in the future is to \u201cwork harder\u201d or \u201cbe more diligent\u201d. Take this hypothetical example: An analyst sends out incorrect information to a client. Upon examination, the analyst figures out she ran a query against the wrong database. The correct database would have been obvious from an examination of the query, but the analyst instead relied on an annotation above the query. The correct database is supposed to be identified in an annotation above the query, but this annotation was not properly updated by the last person to update this query. The entire team does not consistently update these annotations, despite knowing it\u2019s their responsibility to do so. The entire team has been working under tight deadlines and has prioritized writing code over keeping documentation up to date. Th cause of failure in this scenario is not a person. It\u2019s not the analyst that sent out the wrong data, nor is it the analyst that failed to update the annotation. Rather, there are two systemic failures: 1. annotations above queries can be unreliable, and 2. the team has failed to keep up with their technical debt. Fortunately, because failures are systemic it means that solutions are, too. In the above example, one solution might be to remove the annotations since the information can be gathered from the query, itself. Another solution might be to hire another analyst to free up team capacity to update documentation. Disciplining the analysts involved in this failure will not be beneficial to the team. In the very rare case that failure is the result of a malicious act by an employee, it\u2019s still a systemic problem. Examining hiring practices, employee termination procedures, workplace morale, and other issues can be used to mitigate this sort of failure. Blame Flows Up, Not Down Because failure is systemic, low-level workers are not in a position to correct the issue, even if the issue is from their own work. Their responsibility should be to communicate failures that occur to their manager or team lead. This requires trust between the worker and the manager, as an employee will be hesitant to identify problems that may be used by management as an indication that the employee is a poor performer. It is up to management to ensure that failures are addressed in a systemic fashion. This requires establishing a culture of trust between team members and management. Management has no one to blame but themselves if their team is repeating mistakes. A Better Way: Blameless Retrospectives Once a problem has been identified the people involved with the failure (not just management) should gather together to discuss it. The output of the meeting should be concrete steps that will reduce the likelihood the the failure recurring. The steps must be specific and actionable (e.g. \u201cBe better about annotating code\u201d is too vague. \u201cCreate a code review checklist instructing the reviewer to reject code that lacks annotations\u201d is actionable). The people assigned to the tasks must be given the time to carry them out. Most importantly, there should be no finger-pointing going on at the meeting. This can be hard, but is easier if everyone understands that mistakes happen and that failure is always systemic, not personal. Conclusion People have a strong desire to blame someone in the wake of failure, but that desire is misguided. A team\u2019s leadership must place the good of the team above their desire for retribution, even if that retribution seems justified. Establishing an environment where teams can best learn from their mistakes requires fostering of a culture of trust and excellence. It\u2019s hard work to get going, but it\u2019s an amazing experience to work with a great team.","title":"How Great Teams Learn from Failure"},{"location":"articles/learn_from_failure/#how-great-teams-learn-from-failure","text":"Published: 2018-06-11 Mistakes, when handled appropriately, can be pretty great. Your team has identified a failure point, addressed it, and everyone has learned from the experience. Making mistakes is one of the main ways people experience professional growth. All the benefits of mistakes go away if the team does not handle the failure effectively. Below is an explanation of how teams turn failure into a purely negative experience, followed by strategies for benefiting from failure. \u201cWhat are they going to do about it?\u201d I asked my wife after she described a mistake someone made at her work. She explained how senior leadership would perform an investigation that: 1. Would determine who was at fault for the error, and 2. Determine what repercussions the perpetrator should receive. And while this investigation is going on everyone else awkwardly avoids the topic. This is Bad. And because my wife works in healthcare, it\u2019s also Scary. Unfortunately, this is a common strategy that managers undertake to deal with failure.","title":"How Great Teams Learn From Failure"},{"location":"articles/learn_from_failure/#correcting-bad-behavior","text":"In the above example the people with the bad behavior are the senior leadership conducting the investigation. The purpose of their investigation was to find a person to blame. This technique causes several problems: 1. Identifying a person gives the false impression that the point of failure has been addressed, allowing the failure to recur. 2. Morale is lowered as team members become adversarial towards each other in attempts to shift blame. 3. Team members are encouraged to work contrary to the goals of leadership, as they attempt to avoid punishment for failure. 4. Team members do not collaborate on how to correct the problem for fear of getting caught up in the investigation. 5. The opportunity for the team to learn from the failure is wasted. The result is a team that is worse off than before the investigation started. The first priority should be to ensure that the failure does not recur. This can only be done if the cause of the failure is correctly identified, and the systemic fixes are put in place to mitigate a reoccurrence. Instead of asking the question \u201cWho is to blame?\u201d, the investigators should ask \u201cWhat can be done to ensure this problem doesn\u2019t recur?\u201d Rather than having senior managers engage in an investigation by interviewing subordinates, management and subordinates should work together to identify and correct systemic problems.","title":"Correcting Bad Behavior"},{"location":"articles/learn_from_failure/#mistakes-happen","text":"One of the problems that misguided managers make is to set an expectation that mistakes are not acceptable. Instead of establishing a culture of excellence on a team, this expectation encourages team members to hide their mistakes. This tendency can turn small learning moments into major problems. Punishing someone for making a mistake is like punishing someone for sneezing. Mistakes are inevitable. What is not inevitable is identifying mistakes and learning from them.","title":"Mistakes Happen"},{"location":"articles/learn_from_failure/#failures-are-systemic-not-personal","text":"People don\u2019t intend to fail. They\u2019d prefer to do good work that others appreciate. Failure happens despite this intention. The reason that a person may be unable to realize their intention of doing good work is always because of a systemic problem. You can tell if a manager doesn\u2019t get this concept if their conclusion for avoiding a particular failure in the future is to \u201cwork harder\u201d or \u201cbe more diligent\u201d. Take this hypothetical example: An analyst sends out incorrect information to a client. Upon examination, the analyst figures out she ran a query against the wrong database. The correct database would have been obvious from an examination of the query, but the analyst instead relied on an annotation above the query. The correct database is supposed to be identified in an annotation above the query, but this annotation was not properly updated by the last person to update this query. The entire team does not consistently update these annotations, despite knowing it\u2019s their responsibility to do so. The entire team has been working under tight deadlines and has prioritized writing code over keeping documentation up to date. Th cause of failure in this scenario is not a person. It\u2019s not the analyst that sent out the wrong data, nor is it the analyst that failed to update the annotation. Rather, there are two systemic failures: 1. annotations above queries can be unreliable, and 2. the team has failed to keep up with their technical debt. Fortunately, because failures are systemic it means that solutions are, too. In the above example, one solution might be to remove the annotations since the information can be gathered from the query, itself. Another solution might be to hire another analyst to free up team capacity to update documentation. Disciplining the analysts involved in this failure will not be beneficial to the team. In the very rare case that failure is the result of a malicious act by an employee, it\u2019s still a systemic problem. Examining hiring practices, employee termination procedures, workplace morale, and other issues can be used to mitigate this sort of failure.","title":"Failures Are Systemic, not Personal"},{"location":"articles/learn_from_failure/#blame-flows-up-not-down","text":"Because failure is systemic, low-level workers are not in a position to correct the issue, even if the issue is from their own work. Their responsibility should be to communicate failures that occur to their manager or team lead. This requires trust between the worker and the manager, as an employee will be hesitant to identify problems that may be used by management as an indication that the employee is a poor performer. It is up to management to ensure that failures are addressed in a systemic fashion. This requires establishing a culture of trust between team members and management. Management has no one to blame but themselves if their team is repeating mistakes. A Better Way: Blameless Retrospectives Once a problem has been identified the people involved with the failure (not just management) should gather together to discuss it. The output of the meeting should be concrete steps that will reduce the likelihood the the failure recurring. The steps must be specific and actionable (e.g. \u201cBe better about annotating code\u201d is too vague. \u201cCreate a code review checklist instructing the reviewer to reject code that lacks annotations\u201d is actionable). The people assigned to the tasks must be given the time to carry them out. Most importantly, there should be no finger-pointing going on at the meeting. This can be hard, but is easier if everyone understands that mistakes happen and that failure is always systemic, not personal.","title":"Blame Flows Up, Not Down"},{"location":"articles/learn_from_failure/#conclusion","text":"People have a strong desire to blame someone in the wake of failure, but that desire is misguided. A team\u2019s leadership must place the good of the team above their desire for retribution, even if that retribution seems justified. Establishing an environment where teams can best learn from their mistakes requires fostering of a culture of trust and excellence. It\u2019s hard work to get going, but it\u2019s an amazing experience to work with a great team.","title":"Conclusion"},{"location":"articles/trust_is_essential/","text":"Trust is an Essential Component of a Successful Development Team Published: 2021-01-11 \u201cTrust\u201d sounds like a pretty touchy-feely subject, but without it teams and their members can\u2019t reach their potential. If you wan\u2019t to get more out of your development team don\u2019t just look at improved ticketing and defining product owner roles. You need to do the hard work of building a high-trust environment. What is a culture of trust? Simply put, a culture of trust is an environment where employees and management feel comfortable being honest with each other. Employees in this environment don\u2019t \u201ccall in sick\u201d to get a mental health day, they tell their boss they\u2019re feeling stressed and need a day to recover. They tell their team lead that the deadline they set is unrealistic. They talk to their teammates about a bug they committed. They let their manager know they\u2019re overloaded with work. It\u2019s easy to be honest about the positive things, like the great feedback your users gave you. It\u2019s a lot harder to be honest about the bad stuff, like the code you pushed that caused an outage. Workers in a culture of trust are able to be honest about the hard stuff because they know that their colleagues, and especially their manager, will react with empathy. This empathetic response allows a team to work together to solve problems. Why trust is important A culture of trust leads to happier and more productive employees. People are happier because they don\u2019t have to feel guarded at work. They can be honest with their boss and colleagues about any troubles they\u2019re having, relying on them as a support network. This results in tighter social bonds within the group. People have a much higher opinion of their job if they enjoy the people they work with. People are more productive because they are happier. People who like their job are going to be more productive. But even more directly, people are more productive when they are in a culture of trust because it allows them highlight mistakes made by themselves and others. If a team knows that bugs reported in their code are going to hurt their chances of a raise or promotion, then you are going to get a team that reports remarkably few bugs. Sharing your mistakes with others is how engineering teams grow, but that learning is lost when engineers are fearful of sharing their failures. A lack of trust doesn\u2019t just mean a team loses out on these moments of learning from failure. It also means that management fails to get critical feedback from the experts it pays to do just that. When a product owner comes to a developer with an amazing new feature that they want developed the product owner needs honest feedback to be successful. An honest developer may say that the timeline is too optimistic, or that it\u2019s doubtful it could be implemented at all. But without a culture of trust, the developer is fearful of critiquing others and so the product owner only hears positive things about an initiative doomed to failure. Why establishing trust can be so hard Trust is hard earned and easily lost. A culture of trust is not just something you can declare in a mission statement and consider it done. It needs to be proven through action. There will come moments where colleagues will be vulnerable: asking for feedback, providing negative feedback, or admitting mistakes. Every time that happens the entire team must respond with empathy. If people open up about a mistake they made and get chewed out for it, then there\u2019s a good chance that employee will never be open about their mistakes again. But if every time a team member opens up they receive compassion and constructive advice, they will continue to open up. Trust is built on each one of those moments. How to develop trust An environment of trust is won by team members being vulnerable, and others responding positively to that vulnerability. If these moments of vulnerability aren\u2019t happening with your team, then you may have to take the lead and hope others reciprocate. There is no shortcut. Moments of vulnerability don\u2019t have to be some grand revelation about your inner child. They can be as simple as describing a bug in your code that you almost didn\u2019t catch. Or explaining about how you misunderstood a requirement on a recent project. Signaling that it is ok to talk about failures will allow the team to open up to each other. Responses to these moments of vulnerability must be genuinely empathetic. Managers must understand that mistakes happen and a developer coming forward with a mistake is a benefit for the team, not a point of concern about a developer. As mentioned above, the entire team must respond empathetically to their teammates. If a team member is being demeaning or rude in response to another team member\u2019s moment of vulnerability the manager must make it clear that the behavior will not be tolerated. A developer that writes excellent code but can\u2019t respond with empathy to their teammates is a drag on the whole team, no matter how individually productive they are. Conclusion Trust enables honest feedback, improved learning, continual improvement, and identification of problems. If you want to unlock these features on your team, then you must develop a culture of trust.","title":"Trust is Essential"},{"location":"articles/trust_is_essential/#trust-is-an-essential-component-of-a-successful-development-team","text":"Published: 2021-01-11 \u201cTrust\u201d sounds like a pretty touchy-feely subject, but without it teams and their members can\u2019t reach their potential. If you wan\u2019t to get more out of your development team don\u2019t just look at improved ticketing and defining product owner roles. You need to do the hard work of building a high-trust environment.","title":"Trust is an Essential Component of a Successful Development Team"},{"location":"articles/trust_is_essential/#what-is-a-culture-of-trust","text":"Simply put, a culture of trust is an environment where employees and management feel comfortable being honest with each other. Employees in this environment don\u2019t \u201ccall in sick\u201d to get a mental health day, they tell their boss they\u2019re feeling stressed and need a day to recover. They tell their team lead that the deadline they set is unrealistic. They talk to their teammates about a bug they committed. They let their manager know they\u2019re overloaded with work. It\u2019s easy to be honest about the positive things, like the great feedback your users gave you. It\u2019s a lot harder to be honest about the bad stuff, like the code you pushed that caused an outage. Workers in a culture of trust are able to be honest about the hard stuff because they know that their colleagues, and especially their manager, will react with empathy. This empathetic response allows a team to work together to solve problems.","title":"What is a culture of trust?"},{"location":"articles/trust_is_essential/#why-trust-is-important","text":"A culture of trust leads to happier and more productive employees. People are happier because they don\u2019t have to feel guarded at work. They can be honest with their boss and colleagues about any troubles they\u2019re having, relying on them as a support network. This results in tighter social bonds within the group. People have a much higher opinion of their job if they enjoy the people they work with. People are more productive because they are happier. People who like their job are going to be more productive. But even more directly, people are more productive when they are in a culture of trust because it allows them highlight mistakes made by themselves and others. If a team knows that bugs reported in their code are going to hurt their chances of a raise or promotion, then you are going to get a team that reports remarkably few bugs. Sharing your mistakes with others is how engineering teams grow, but that learning is lost when engineers are fearful of sharing their failures. A lack of trust doesn\u2019t just mean a team loses out on these moments of learning from failure. It also means that management fails to get critical feedback from the experts it pays to do just that. When a product owner comes to a developer with an amazing new feature that they want developed the product owner needs honest feedback to be successful. An honest developer may say that the timeline is too optimistic, or that it\u2019s doubtful it could be implemented at all. But without a culture of trust, the developer is fearful of critiquing others and so the product owner only hears positive things about an initiative doomed to failure.","title":"Why trust is important"},{"location":"articles/trust_is_essential/#why-establishing-trust-can-be-so-hard","text":"Trust is hard earned and easily lost. A culture of trust is not just something you can declare in a mission statement and consider it done. It needs to be proven through action. There will come moments where colleagues will be vulnerable: asking for feedback, providing negative feedback, or admitting mistakes. Every time that happens the entire team must respond with empathy. If people open up about a mistake they made and get chewed out for it, then there\u2019s a good chance that employee will never be open about their mistakes again. But if every time a team member opens up they receive compassion and constructive advice, they will continue to open up. Trust is built on each one of those moments.","title":"Why establishing trust can be so hard"},{"location":"articles/trust_is_essential/#how-to-develop-trust","text":"An environment of trust is won by team members being vulnerable, and others responding positively to that vulnerability. If these moments of vulnerability aren\u2019t happening with your team, then you may have to take the lead and hope others reciprocate. There is no shortcut. Moments of vulnerability don\u2019t have to be some grand revelation about your inner child. They can be as simple as describing a bug in your code that you almost didn\u2019t catch. Or explaining about how you misunderstood a requirement on a recent project. Signaling that it is ok to talk about failures will allow the team to open up to each other. Responses to these moments of vulnerability must be genuinely empathetic. Managers must understand that mistakes happen and a developer coming forward with a mistake is a benefit for the team, not a point of concern about a developer. As mentioned above, the entire team must respond empathetically to their teammates. If a team member is being demeaning or rude in response to another team member\u2019s moment of vulnerability the manager must make it clear that the behavior will not be tolerated. A developer that writes excellent code but can\u2019t respond with empathy to their teammates is a drag on the whole team, no matter how individually productive they are.","title":"How to develop trust"},{"location":"articles/trust_is_essential/#conclusion","text":"Trust enables honest feedback, improved learning, continual improvement, and identification of problems. If you want to unlock these features on your team, then you must develop a culture of trust.","title":"Conclusion"},{"location":"articles/you_cant_build/","text":"You can\u2019t build software without communication and teamwork. Published: 2020-02-10 The demand for software developers that are \u201crockstars\u201d, \u201cninjas\u201d, and \u201c10x\u201d are on the decline. This isn\u2019t because employers don\u2019t want amazing coders anymore. Rather, it\u2019s because they\u2019ve realized that they\u2019ve been looking for the best coders when they should have been looking for someone that is most likely to help achieve their organizational goals. Hiring for the skills you need: teamwork and communication Almost all software is produced by teams of people. Coordinating these teams is hard. There\u2019s the famous adage: \u2018what one developer can get done in one month, two developers can get done in two months.\u2019 What makes it harder is if the team is composed of people who lack the skills necessary to work well with others. If the team isn\u2019t working well together the product suffers. While a good manager can take steps to improve communication and teamwork, making sure a person has those skills before they are hired is an obvious strategy for success. These skills are critical to a product\u2019s success and should be evaluated during an interview as thoroughly as a candidate\u2019s technical ability. Hiring managers should challenge a candidate to gather (mock) requirements or write documentation. They should ask probing questions about how the candidate has handled interpersonal conflict. Being able demonstrate teamwork and communication skills should be on the mind of anyone seeking a job in the tech field. Shared goals There\u2019s a great scene from Saving Private Ryan when they are deciding to storm an enemy bunker: \u201cit\u2019s not our mission\u201d a soldier gripes. To which Tom Hanks replies: \u201cour mission is to win the war.\u201d A software developer\u2019s mission is to help make the best product they can. If a colleague is struggling to implement a feature it is the responsibility of the entire team to help that developer out. Software that is riddled with bugs is still a piece of junk, even if the code you worked on is flawless. Implementing shared goals isn\u2019t as easy as setting policy: it\u2019s about developing culture. Things like sensible policies are important, but good leadership is critical. Selfishness and backstabbing will destroy a team, and must be snubbed out quickly and firmly. If you are part of a team with leadership that tolerates selfishness then you should seriously consider changing jobs. Not only will that job be soul-sucking, but you\u2019re not likely to be a productive team, either. Everybody wins If you\u2019re able to be part of a team that works well together and has shared goals you\u2019re setting yourself up for success. And when failure does come, as it does periodically on all teams to some degree, you\u2019ll have positioned your team to learn from your collective mistakes. It\u2019s better to be known as one of the developers on a team that successfully launched a product, than the best coder on the team whose product flopped.","title":"You can\u2019t build software without communication and teamwork"},{"location":"articles/you_cant_build/#you-cant-build-software-without-communication-and-teamwork","text":"Published: 2020-02-10 The demand for software developers that are \u201crockstars\u201d, \u201cninjas\u201d, and \u201c10x\u201d are on the decline. This isn\u2019t because employers don\u2019t want amazing coders anymore. Rather, it\u2019s because they\u2019ve realized that they\u2019ve been looking for the best coders when they should have been looking for someone that is most likely to help achieve their organizational goals.","title":"You can\u2019t build software without communication and teamwork."},{"location":"articles/you_cant_build/#hiring-for-the-skills-you-need-teamwork-and-communication","text":"Almost all software is produced by teams of people. Coordinating these teams is hard. There\u2019s the famous adage: \u2018what one developer can get done in one month, two developers can get done in two months.\u2019 What makes it harder is if the team is composed of people who lack the skills necessary to work well with others. If the team isn\u2019t working well together the product suffers. While a good manager can take steps to improve communication and teamwork, making sure a person has those skills before they are hired is an obvious strategy for success. These skills are critical to a product\u2019s success and should be evaluated during an interview as thoroughly as a candidate\u2019s technical ability. Hiring managers should challenge a candidate to gather (mock) requirements or write documentation. They should ask probing questions about how the candidate has handled interpersonal conflict. Being able demonstrate teamwork and communication skills should be on the mind of anyone seeking a job in the tech field.","title":"Hiring for the skills you need: teamwork and communication"},{"location":"articles/you_cant_build/#shared-goals","text":"There\u2019s a great scene from Saving Private Ryan when they are deciding to storm an enemy bunker: \u201cit\u2019s not our mission\u201d a soldier gripes. To which Tom Hanks replies: \u201cour mission is to win the war.\u201d A software developer\u2019s mission is to help make the best product they can. If a colleague is struggling to implement a feature it is the responsibility of the entire team to help that developer out. Software that is riddled with bugs is still a piece of junk, even if the code you worked on is flawless. Implementing shared goals isn\u2019t as easy as setting policy: it\u2019s about developing culture. Things like sensible policies are important, but good leadership is critical. Selfishness and backstabbing will destroy a team, and must be snubbed out quickly and firmly. If you are part of a team with leadership that tolerates selfishness then you should seriously consider changing jobs. Not only will that job be soul-sucking, but you\u2019re not likely to be a productive team, either.","title":"Shared goals"},{"location":"articles/you_cant_build/#everybody-wins","text":"If you\u2019re able to be part of a team that works well together and has shared goals you\u2019re setting yourself up for success. And when failure does come, as it does periodically on all teams to some degree, you\u2019ll have positioned your team to learn from your collective mistakes. It\u2019s better to be known as one of the developers on a team that successfully launched a product, than the best coder on the team whose product flopped.","title":"Everybody wins"},{"location":"book_reviews/clean_code/","text":"Book Review: Clean Code by Robert C. Martin Published: 2018-03-13 If you browse lists of recommended books that every software developer should read, Clean Code by Robert \u201cUncle Bob\u201d Martin will come up again and again. My voice can now be added to the chorus in praise of this book. Uncle Bob subtitles his book A Handbook of Agile Software Craftsmanship , and the theme of craftsmanship is laced throughout the book. Uncle Bob laments that too many programmers think that functioning code is good code. He asserts that many people know how to write code, but far fewer know the craft of writing Clean Code. Uncle Bob urges the reader to take pride in their code, makes a business case for why well constructed \u201cclean\u201d code should always be sought, and then lays out in detail how to go about practicing the craft of professional software development. After reading this book it\u2019s hard not to look at my own code and be embarrassed. But the book also has left me motivated to make my code better. With Clean Code\u2019s numerous examples and detailed explanations I feel well equipped to write cleaner code, if not Clean Code . Making the Case Clean Code starts out by making the case for why writing clean code is important. This starting assertion is emphasized throughout the rest of the book as Uncle Bob gives examples of bad code that he proceeds to clean. While developers can be, at worst, indifferent to clean code, Uncle Bob makes a convincing argument why we should care more: clean code can be read, refactored, scaled, and debugged much more easily then dirty code. Having code with these qualities is worth the extra effort to transform functioning code to clean code. The term Agile is used in the subtitle, but I\u2019m not sure it\u2019s mentioned again in the book. Rather than singing the praises of agile methodologies, Uncle Bob emphasizes that the key to clean code is iteration and refactoring. Refreshingly, he doesn\u2019t talk about agile development in the abstract, but rather gives a no-nonsense explanation of how to practice it well. Teaching by Example Uncle Bob has no hesitation about diving into code to make his points. While there is plenty of exposition, including a lengthy and invaluable section on heuristics for writing clean code, all of the books major assertions are backed up by several lengthy sections of code. Clean Code follows a repeated pattern: an explanation of a bad coding practice, and example of code that follows that practice, and then examples of how Uncle Bob would iterate on that code until it is clean. This pattern makes the reading dense, but by showing his assertions in practice Uncle Bob both educates the reader on best practices and forestalls any objections to his assertions. I Hope You Know Java One of the things that made the book less useful for me is that all of the code examples are written in Java. Java is one of the most widely known coding languages out there, so it\u2019s no surprise, but I was somewhat disappointed that I was not forewarned that I would be digging through code I only vaguely understood. And while I can\u2019t speak for the printed versions, the kindle version lacked syntax highlighting, which could have made the code more readable even though I\u2019m not familiar with Java. While most of the book provides insight into general coding best practices, there are whole sections of the book dedicated specifically to Java implementations. Uncle Bob is also a strong advocate of Object Oriented Programming, and much of his suggestions are premised on that coding style. Programmers who prefer Functional Programming may still find Clean Code useful, but certainly less valuable. If you want to level up your coding skills, this book is worth the read. While I\u2019m still working on the craft of coding, Uncle Bob did a great job displaying what I should be aiming for and how to get there.","title":"Clean Code by Robert C. Martin"},{"location":"book_reviews/clean_code/#book-review-clean-code-by-robert-c-martin","text":"Published: 2018-03-13 If you browse lists of recommended books that every software developer should read, Clean Code by Robert \u201cUncle Bob\u201d Martin will come up again and again. My voice can now be added to the chorus in praise of this book. Uncle Bob subtitles his book A Handbook of Agile Software Craftsmanship , and the theme of craftsmanship is laced throughout the book. Uncle Bob laments that too many programmers think that functioning code is good code. He asserts that many people know how to write code, but far fewer know the craft of writing Clean Code. Uncle Bob urges the reader to take pride in their code, makes a business case for why well constructed \u201cclean\u201d code should always be sought, and then lays out in detail how to go about practicing the craft of professional software development. After reading this book it\u2019s hard not to look at my own code and be embarrassed. But the book also has left me motivated to make my code better. With Clean Code\u2019s numerous examples and detailed explanations I feel well equipped to write cleaner code, if not Clean Code .","title":"Book Review: Clean Code by Robert C. Martin"},{"location":"book_reviews/clean_code/#making-the-case","text":"Clean Code starts out by making the case for why writing clean code is important. This starting assertion is emphasized throughout the rest of the book as Uncle Bob gives examples of bad code that he proceeds to clean. While developers can be, at worst, indifferent to clean code, Uncle Bob makes a convincing argument why we should care more: clean code can be read, refactored, scaled, and debugged much more easily then dirty code. Having code with these qualities is worth the extra effort to transform functioning code to clean code. The term Agile is used in the subtitle, but I\u2019m not sure it\u2019s mentioned again in the book. Rather than singing the praises of agile methodologies, Uncle Bob emphasizes that the key to clean code is iteration and refactoring. Refreshingly, he doesn\u2019t talk about agile development in the abstract, but rather gives a no-nonsense explanation of how to practice it well.","title":"Making the Case"},{"location":"book_reviews/clean_code/#teaching-by-example","text":"Uncle Bob has no hesitation about diving into code to make his points. While there is plenty of exposition, including a lengthy and invaluable section on heuristics for writing clean code, all of the books major assertions are backed up by several lengthy sections of code. Clean Code follows a repeated pattern: an explanation of a bad coding practice, and example of code that follows that practice, and then examples of how Uncle Bob would iterate on that code until it is clean. This pattern makes the reading dense, but by showing his assertions in practice Uncle Bob both educates the reader on best practices and forestalls any objections to his assertions.","title":"Teaching by Example"},{"location":"book_reviews/clean_code/#i-hope-you-know-java","text":"One of the things that made the book less useful for me is that all of the code examples are written in Java. Java is one of the most widely known coding languages out there, so it\u2019s no surprise, but I was somewhat disappointed that I was not forewarned that I would be digging through code I only vaguely understood. And while I can\u2019t speak for the printed versions, the kindle version lacked syntax highlighting, which could have made the code more readable even though I\u2019m not familiar with Java. While most of the book provides insight into general coding best practices, there are whole sections of the book dedicated specifically to Java implementations. Uncle Bob is also a strong advocate of Object Oriented Programming, and much of his suggestions are premised on that coding style. Programmers who prefer Functional Programming may still find Clean Code useful, but certainly less valuable. If you want to level up your coding skills, this book is worth the read. While I\u2019m still working on the craft of coding, Uncle Bob did a great job displaying what I should be aiming for and how to get there.","title":"I Hope You Know Java"},{"location":"book_reviews/rework/","text":"Book Review: ReWork by Jason Fried and DHH Published: 2018-08-06 Reading ReWork, by Jason Fried and David Heinemeier Hansson, felt like reading the beginning of a conversation I had already joined half-way through. Many of the ideas advocated in this book I\u2019ve heard in other places and from other people, but this book neatly defines the counterargument to the startup ethos of \u201cmove fast and break things\u201d. This book is a good, quick read. The authors boast that they cut the length of the book in half for their final draft, which I appreciate from any book trying to tackle a potentially dry subject. Their conversational tone and mild humor keep the book engaging, while the short chapters make it easy to pick up when you have a few minutes to spare. Unfortunately, at least for me, the pithy writing didn\u2019t stick with me as well as I might have hoped. I found myself forgetting most of their specific suggestions hours after reading them. To be fair, if I was about to start a new business perhaps it would have been easier for me to retain the details of their advice. While the book provides lots of intriguing strategies for starting and running a business, the advice is unmistakably reactionary to the common tech startup imperative of \u2018growth above all else\u2019. The authors caution: be slow to hire, avoid outside investors, beware feature bloat, and be frugal. If you focus on creating a great product then success will follow, they argue. As backing for their assertions they point to their own company, 37signals, a highly successful software company with only a handful of employees. A pet peeve I have with this book is the confidence in which the authors declare that if you follow the path they have blazed it will inevitably lead you to success. Their success does not prove that their business philosophies will lead others to success. To make that assertion they would need to examine others with similar business practices and compare the outcomes, which the authors have not done. This is a form of survivorship bias, and while common for these sorts of books, it still frustrates me. In summary, this book is an easy recommendation to anyone working in a tech startup or looking to found a business. For all others there is still some value in reading the book, especially if you have an interest in management or software development. This book has already had an impact on my work, making me reevaluate the scope of a personal project. I wouldn\u2019t say this book has changed my life, but it definitely provided me with some valuable insight and was well worth the read.","title":"ReWork by Jason Fried and DHH"},{"location":"book_reviews/rework/#book-review-rework-by-jason-fried-and-dhh","text":"Published: 2018-08-06 Reading ReWork, by Jason Fried and David Heinemeier Hansson, felt like reading the beginning of a conversation I had already joined half-way through. Many of the ideas advocated in this book I\u2019ve heard in other places and from other people, but this book neatly defines the counterargument to the startup ethos of \u201cmove fast and break things\u201d. This book is a good, quick read. The authors boast that they cut the length of the book in half for their final draft, which I appreciate from any book trying to tackle a potentially dry subject. Their conversational tone and mild humor keep the book engaging, while the short chapters make it easy to pick up when you have a few minutes to spare. Unfortunately, at least for me, the pithy writing didn\u2019t stick with me as well as I might have hoped. I found myself forgetting most of their specific suggestions hours after reading them. To be fair, if I was about to start a new business perhaps it would have been easier for me to retain the details of their advice. While the book provides lots of intriguing strategies for starting and running a business, the advice is unmistakably reactionary to the common tech startup imperative of \u2018growth above all else\u2019. The authors caution: be slow to hire, avoid outside investors, beware feature bloat, and be frugal. If you focus on creating a great product then success will follow, they argue. As backing for their assertions they point to their own company, 37signals, a highly successful software company with only a handful of employees. A pet peeve I have with this book is the confidence in which the authors declare that if you follow the path they have blazed it will inevitably lead you to success. Their success does not prove that their business philosophies will lead others to success. To make that assertion they would need to examine others with similar business practices and compare the outcomes, which the authors have not done. This is a form of survivorship bias, and while common for these sorts of books, it still frustrates me. In summary, this book is an easy recommendation to anyone working in a tech startup or looking to found a business. For all others there is still some value in reading the book, especially if you have an interest in management or software development. This book has already had an impact on my work, making me reevaluate the scope of a personal project. I wouldn\u2019t say this book has changed my life, but it definitely provided me with some valuable insight and was well worth the read.","title":"Book Review: ReWork by Jason Fried and DHH"},{"location":"de-gcp-book/ch_00_preface/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . The original version of this book was written for and is hosted on GitHub here . I will strive to keep this version up-to-date with the GitHub version, but there may be some small differences between these versions. Additionally, the in-text links all still point to GitHub, not the version on this site. Use the navigation bar on the left to navigate between pages if you do not wish to be brought to the version hosted on GitHub. Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Preface This is a book designed to teach you how to set up and maintain a production-ready data engineering stack on Google Cloud Platform (GCP). In each chapter I will discuss an important component of Data Engineering infrastructure. I will give some background on what the component is for and why it's important, followed by how to implement that component on GCP. I'll conclude each chapter by referencing similar services offered by other cloud providers. By the end of this book you will know how to set up a complete tech stack for a Data Engineering team using GCP. Be warned that this book is opinionated. I've chosen a stack that has worked well for me and that I believe will work well for many data engineering teams, but it's entirely possible the infrastructure I describe in this book will not be a great fit for your team. If you think there's a better way than what I've laid out here, I'd love to hear about it. Please refer to the Contributions section, below. Who This Book Is For This book is for people with coding familiarity that are interested in setting up professional data pipelines and data warehouses using Google Cloud Platform. I expect the readers to include: * Data Engineers looking to learn more abut GCP. * Junior Data Engineers looking to learn best practices for building and working with data engineering infrastructure. * Software Engineers, DevOps Engineers, Data Scientists, Data Analysts, or anyone else that is tasked with performing Data Engineering functions to help them with their other work. This book assumes your familiarity with SQL and Python (if you're not familiar with Python, you should be able to muddle through with general programming experience). If you do not have experience with these languages (particularly SQL) it is recommended you learn these languages and then return to this book. This book covers a lot of ground. Many of the subjects we'll cover in just part of a chapter will have entire books written about them. I will provide references for further research. Just know that while this book is comprehensive in the sense that it provides all the information you need to get a stack up and running, there is still plenty of information a Data Engineer needs to know that I've omitted from this book. What is covered in this book is not THE data engineering stack, it is A data engineering stack. Even within GCP there are lots of ways to accomplish similar tasks. For example, I could have used Dataflow (based on Apache Beam) for the streaming solution. Or I could have gone with a completely different paradigm for how I store and query data, such as storing data as Parquet files in GCS and querying with Spark. I mention this here to make sure you understand that this book is not the complete guide to being a data engineer. Rather, it is an introduction to the types of problems a data engineer solves, and a sampling common tools in GCP used to solve those problems. Finally, there are a vast array of Data Engineering tools that are in use. I cover many popular tools for Data Engineering, but many more have been left out of this book due to brevity and my lack of experience with them. If you feel I left off something important, please read the Contributions section below. How to Read This Book This book is divided into chapters discussing major Data Engineering concepts and functions. Most chapters is then divided into three parts: an overview of the topic, implementation examples, and references to other articles and tools. If you're looking to use this book as a guid to set up your Data Engineering infrastructure from scratch, I recommend you read this book front-to-back. Each chapter describes a necessary component of the tech stack, and they are ordered such that the infrastructure described in one chapter builds of the previously described infrastructure. Likely, many people will find their way to this book trying to solve a specific problem (e.g. how to set up alerting on GCP's Composer/Airflow service). For these people I've tried to make each chapter as self-contained as possible. When I use infrastructure created in a previous chapter I'll always provide a link to the previous chapter where it's explained. The best way to learn is by doing, which is why each chapter provides code samples. I encourage you to build this infrastructure with me, as you read through the chapters. Included with this book in Appendix A I've provided an example of what your infrastructure as code will look like. A Note About the Code in the Book The code within the chapters is for demonstration purposes, and is not necessarily in the format you should be running in production. For the sake of clarity and brevity the code usually omits good practices such as type hinting, validation, error handling, and docstrings. If you want a better sense of what production-ready code looks like, review the code in Appendix A . Contributions You may have noticed: this book is hosted on GitHub. This results in three great things: 1. The book is hosted online and freely available. 2. You can make pull requests. 3. You can create issues. If you think the book is wrong, missing information, or otherwise needs to be edited, there are two options: 1. Make a pull request (the preferred option). If you think something needs to be changed, fork this repo, make the change yourself, then send me a pull request. I'll review it, discuss it with you, if needed, then add it in. Easy peasy. If you're not very familiar with GitHub, instructions for doing this are here . If your PR is merged it will be considered a donation of your work to this project. You agree to grant a Attribution 4.0 International (CC BY 4.0) license for your work. You will be added the the Contributors section on this page once your PR is merged. 2. Make an issue . Go to the Issues tab for this repo on GitHub, click to create a new issue, then tell me what you think is wrong, preferably including references to specific files and line numbers. I look forward to working with you all. Contributors Ed Nunes . Ed lives in Chicago and works as a Data Engineer for Zoro . Feel free to reach out to him on LinkedIn . License You are free to use this book under the Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license. Next Chapter: Chapter 1: Setting up a GCP Account","title":"Preface"},{"location":"de-gcp-book/ch_00_preface/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . The original version of this book was written for and is hosted on GitHub here . I will strive to keep this version up-to-date with the GitHub version, but there may be some small differences between these versions. Additionally, the in-text links all still point to GitHub, not the version on this site. Use the navigation bar on the left to navigate between pages if you do not wish to be brought to the version hosted on GitHub.","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_00_preface/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_00_preface/#preface","text":"This is a book designed to teach you how to set up and maintain a production-ready data engineering stack on Google Cloud Platform (GCP). In each chapter I will discuss an important component of Data Engineering infrastructure. I will give some background on what the component is for and why it's important, followed by how to implement that component on GCP. I'll conclude each chapter by referencing similar services offered by other cloud providers. By the end of this book you will know how to set up a complete tech stack for a Data Engineering team using GCP. Be warned that this book is opinionated. I've chosen a stack that has worked well for me and that I believe will work well for many data engineering teams, but it's entirely possible the infrastructure I describe in this book will not be a great fit for your team. If you think there's a better way than what I've laid out here, I'd love to hear about it. Please refer to the Contributions section, below.","title":"Preface"},{"location":"de-gcp-book/ch_00_preface/#who-this-book-is-for","text":"This book is for people with coding familiarity that are interested in setting up professional data pipelines and data warehouses using Google Cloud Platform. I expect the readers to include: * Data Engineers looking to learn more abut GCP. * Junior Data Engineers looking to learn best practices for building and working with data engineering infrastructure. * Software Engineers, DevOps Engineers, Data Scientists, Data Analysts, or anyone else that is tasked with performing Data Engineering functions to help them with their other work. This book assumes your familiarity with SQL and Python (if you're not familiar with Python, you should be able to muddle through with general programming experience). If you do not have experience with these languages (particularly SQL) it is recommended you learn these languages and then return to this book. This book covers a lot of ground. Many of the subjects we'll cover in just part of a chapter will have entire books written about them. I will provide references for further research. Just know that while this book is comprehensive in the sense that it provides all the information you need to get a stack up and running, there is still plenty of information a Data Engineer needs to know that I've omitted from this book. What is covered in this book is not THE data engineering stack, it is A data engineering stack. Even within GCP there are lots of ways to accomplish similar tasks. For example, I could have used Dataflow (based on Apache Beam) for the streaming solution. Or I could have gone with a completely different paradigm for how I store and query data, such as storing data as Parquet files in GCS and querying with Spark. I mention this here to make sure you understand that this book is not the complete guide to being a data engineer. Rather, it is an introduction to the types of problems a data engineer solves, and a sampling common tools in GCP used to solve those problems. Finally, there are a vast array of Data Engineering tools that are in use. I cover many popular tools for Data Engineering, but many more have been left out of this book due to brevity and my lack of experience with them. If you feel I left off something important, please read the Contributions section below.","title":"Who This Book Is For"},{"location":"de-gcp-book/ch_00_preface/#how-to-read-this-book","text":"This book is divided into chapters discussing major Data Engineering concepts and functions. Most chapters is then divided into three parts: an overview of the topic, implementation examples, and references to other articles and tools. If you're looking to use this book as a guid to set up your Data Engineering infrastructure from scratch, I recommend you read this book front-to-back. Each chapter describes a necessary component of the tech stack, and they are ordered such that the infrastructure described in one chapter builds of the previously described infrastructure. Likely, many people will find their way to this book trying to solve a specific problem (e.g. how to set up alerting on GCP's Composer/Airflow service). For these people I've tried to make each chapter as self-contained as possible. When I use infrastructure created in a previous chapter I'll always provide a link to the previous chapter where it's explained. The best way to learn is by doing, which is why each chapter provides code samples. I encourage you to build this infrastructure with me, as you read through the chapters. Included with this book in Appendix A I've provided an example of what your infrastructure as code will look like.","title":"How to Read This Book"},{"location":"de-gcp-book/ch_00_preface/#a-note-about-the-code-in-the-book","text":"The code within the chapters is for demonstration purposes, and is not necessarily in the format you should be running in production. For the sake of clarity and brevity the code usually omits good practices such as type hinting, validation, error handling, and docstrings. If you want a better sense of what production-ready code looks like, review the code in Appendix A .","title":"A Note About the Code in the Book"},{"location":"de-gcp-book/ch_00_preface/#contributions","text":"You may have noticed: this book is hosted on GitHub. This results in three great things: 1. The book is hosted online and freely available. 2. You can make pull requests. 3. You can create issues. If you think the book is wrong, missing information, or otherwise needs to be edited, there are two options: 1. Make a pull request (the preferred option). If you think something needs to be changed, fork this repo, make the change yourself, then send me a pull request. I'll review it, discuss it with you, if needed, then add it in. Easy peasy. If you're not very familiar with GitHub, instructions for doing this are here . If your PR is merged it will be considered a donation of your work to this project. You agree to grant a Attribution 4.0 International (CC BY 4.0) license for your work. You will be added the the Contributors section on this page once your PR is merged. 2. Make an issue . Go to the Issues tab for this repo on GitHub, click to create a new issue, then tell me what you think is wrong, preferably including references to specific files and line numbers. I look forward to working with you all.","title":"Contributions"},{"location":"de-gcp-book/ch_00_preface/#contributors","text":"Ed Nunes . Ed lives in Chicago and works as a Data Engineer for Zoro . Feel free to reach out to him on LinkedIn .","title":"Contributors"},{"location":"de-gcp-book/ch_00_preface/#license","text":"You are free to use this book under the Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license. Next Chapter: Chapter 1: Setting up a GCP Account","title":"License"},{"location":"de-gcp-book/ch_01_gcp_account/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 1: Setting up a GCP Account GCP usually allows you to perform the same task many different ways. They offer a console through their website, CLI tools, and libraries for a variety of languages. When possible, we'll stick to using CLI tools and Python libraries, as this allows us to put our commands for instantiating and configuring our infrastructure into version control. However, there are going to be some things that must be done through GCP's website, and setting up an account is one of those things. In this chapter I'll also show you how to install the command line tools, so we can avoid using the console in the future. Signing up for GCP Head to cloud.google.com and click \"Get Started for Free\". Next you'll link your GCP account to an existing Google account. GCP may suggest one by default. If you do not have a Google account you must create one before proceeding. GCP will then request personal information, including billing information. Unfortunately, you must provide billing information before you can create an account. However, new accounts receive a $300 credit towards any charges, and GCP will notify you before it starts charging you for cloud services. Creating a Project In GCP, a Project acts as a namespace under which services are organized, permissions are set, and billing is tracked. For example, if you wanted to copy a file in a GCS bucket you would need to provide the Project associated with that bucket, otherwise GCP will be unable to find the bucket. You can have resources from different Projects talk to each other without much difficulty, the Projects generally just act as namespaces to help you organize your infrastructure. For example, we'll be creating a Project for our development infrastructure, and a separate Project for our production infrastructure. On the GCP Console you'll always see your current project listed at the top of the screen. If you just created a new account then GCP has created the \"My First Project\" Project for you. Click on the Project name, then in the window that appears select \"Create New Project\". On this next page you'll provide your project name, which can be anything (I'm using de-book-dev ). This Project will be for our development environment. Also note that while your Project name can be anything, your Project ID must be unique across all of GCP, so GCP will auto-generate it based on the name you provide, but you can choose to pick your own. Click the \"Create\" button. Once we have our GCP command line tools set up (discussed below) we will be able to create a project from the terminal using the command: > gcloud projects create de-book-dev Installing the GCP Command Line Tools GCP offers several command line utilities, but fortunately they are all easily installed by installing and configuring Google Cloud SDK. Google Cloud SDK can be installed though your favorite package manager such as Homebrew or Snap , or though curl ( curl https://sdk.cloud.google.com | bash ). You can find more installation options here . Once installed, run: > gcloud init Following the prompts you will create a profile and link it to the Google account you used to log in to GCP. Your browser will open and authenticate using the credentials you set for your GCP account. At the prompt, select the Project you wish to work on (for me that's de-book-dev ). You can change this profile or create a new profile by running gcloud init again. You now have access to the gcloud , bq , and gsutil command line tools. gsutil is used for managing Google Cloud Storage, bq is used for managing BigQuery, and gcloud is a general purpose utility. All three will be used within this book. Setting up a Service Account We'll be using a Service Account to authenticate with GCP. You can set up a Service Account through the Console by selecting the hamburger menu in the upper-left, then going to \"IAM & Admin\" > \"Service Accounts\". From this page select \"CREATE SERVICE ACCOUNT\" at the top. Now you'll be prompted for the name of your service account and an optional description. Since this account is going to be for accessing Composer in my Dev environment I named my account \"composer-dev\". Clicking to the next page prompts us to select a role from a drop-down menu. Select \"Cloud Composer\" > \"Composer Administrator\", and then \"Continue\". We can ignore the last page and select \"Done\". You will be brought back to the main Service Accounts page, showing all your accounts, including the one you just created. On the account you just created select the three dots on the far right, and then select \"Create key\". On the menu that pops up select \"JSON\" and then \"CREATE\". You will be prompted to download a JSON file. Keep this file safe, as this is your key for accessing Composer. You can also set up a service account through the gsutil utility: > gcloud iam service-accounts create 'composer-dev' In order to set up a role for this account you'll need it's email, which can found with: > gcloud iam service-accounts list DISPLAY NAME EMAIL DISABLED composer-dev composer-dev@de-book-dev.iam.gserviceaccount.com False Finally, we download the JSON file that is our key: > gcloud iam service-accounts keys create ./keys/my_secret_composer_key.json \\ --iam-account='composer-dev@de-book-dev.iam.gserviceaccount.com' Now that we have our key saved locally we can define a variable in our shell so our command line tools can find our key: > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" This variable will only last as long as your terminal session. You may need to periodically execute the above command to run your GCP code. Next Chapter: Chapter 2: Batch Processing Orchestration with Composer and Airflow","title":"Chapter 1: Setting up a GCP Account"},{"location":"de-gcp-book/ch_01_gcp_account/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_01_gcp_account/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_01_gcp_account/#chapter-1-setting-up-a-gcp-account","text":"GCP usually allows you to perform the same task many different ways. They offer a console through their website, CLI tools, and libraries for a variety of languages. When possible, we'll stick to using CLI tools and Python libraries, as this allows us to put our commands for instantiating and configuring our infrastructure into version control. However, there are going to be some things that must be done through GCP's website, and setting up an account is one of those things. In this chapter I'll also show you how to install the command line tools, so we can avoid using the console in the future.","title":"Chapter 1: Setting up a GCP Account"},{"location":"de-gcp-book/ch_01_gcp_account/#signing-up-for-gcp","text":"Head to cloud.google.com and click \"Get Started for Free\". Next you'll link your GCP account to an existing Google account. GCP may suggest one by default. If you do not have a Google account you must create one before proceeding. GCP will then request personal information, including billing information. Unfortunately, you must provide billing information before you can create an account. However, new accounts receive a $300 credit towards any charges, and GCP will notify you before it starts charging you for cloud services.","title":"Signing up for GCP"},{"location":"de-gcp-book/ch_01_gcp_account/#creating-a-project","text":"In GCP, a Project acts as a namespace under which services are organized, permissions are set, and billing is tracked. For example, if you wanted to copy a file in a GCS bucket you would need to provide the Project associated with that bucket, otherwise GCP will be unable to find the bucket. You can have resources from different Projects talk to each other without much difficulty, the Projects generally just act as namespaces to help you organize your infrastructure. For example, we'll be creating a Project for our development infrastructure, and a separate Project for our production infrastructure. On the GCP Console you'll always see your current project listed at the top of the screen. If you just created a new account then GCP has created the \"My First Project\" Project for you. Click on the Project name, then in the window that appears select \"Create New Project\". On this next page you'll provide your project name, which can be anything (I'm using de-book-dev ). This Project will be for our development environment. Also note that while your Project name can be anything, your Project ID must be unique across all of GCP, so GCP will auto-generate it based on the name you provide, but you can choose to pick your own. Click the \"Create\" button. Once we have our GCP command line tools set up (discussed below) we will be able to create a project from the terminal using the command: > gcloud projects create de-book-dev","title":"Creating a Project"},{"location":"de-gcp-book/ch_01_gcp_account/#installing-the-gcp-command-line-tools","text":"GCP offers several command line utilities, but fortunately they are all easily installed by installing and configuring Google Cloud SDK. Google Cloud SDK can be installed though your favorite package manager such as Homebrew or Snap , or though curl ( curl https://sdk.cloud.google.com | bash ). You can find more installation options here . Once installed, run: > gcloud init Following the prompts you will create a profile and link it to the Google account you used to log in to GCP. Your browser will open and authenticate using the credentials you set for your GCP account. At the prompt, select the Project you wish to work on (for me that's de-book-dev ). You can change this profile or create a new profile by running gcloud init again. You now have access to the gcloud , bq , and gsutil command line tools. gsutil is used for managing Google Cloud Storage, bq is used for managing BigQuery, and gcloud is a general purpose utility. All three will be used within this book.","title":"Installing the GCP Command Line Tools"},{"location":"de-gcp-book/ch_01_gcp_account/#setting-up-a-service-account","text":"We'll be using a Service Account to authenticate with GCP. You can set up a Service Account through the Console by selecting the hamburger menu in the upper-left, then going to \"IAM & Admin\" > \"Service Accounts\". From this page select \"CREATE SERVICE ACCOUNT\" at the top. Now you'll be prompted for the name of your service account and an optional description. Since this account is going to be for accessing Composer in my Dev environment I named my account \"composer-dev\". Clicking to the next page prompts us to select a role from a drop-down menu. Select \"Cloud Composer\" > \"Composer Administrator\", and then \"Continue\". We can ignore the last page and select \"Done\". You will be brought back to the main Service Accounts page, showing all your accounts, including the one you just created. On the account you just created select the three dots on the far right, and then select \"Create key\". On the menu that pops up select \"JSON\" and then \"CREATE\". You will be prompted to download a JSON file. Keep this file safe, as this is your key for accessing Composer. You can also set up a service account through the gsutil utility: > gcloud iam service-accounts create 'composer-dev' In order to set up a role for this account you'll need it's email, which can found with: > gcloud iam service-accounts list DISPLAY NAME EMAIL DISABLED composer-dev composer-dev@de-book-dev.iam.gserviceaccount.com False Finally, we download the JSON file that is our key: > gcloud iam service-accounts keys create ./keys/my_secret_composer_key.json \\ --iam-account='composer-dev@de-book-dev.iam.gserviceaccount.com' Now that we have our key saved locally we can define a variable in our shell so our command line tools can find our key: > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" This variable will only last as long as your terminal session. You may need to periodically execute the above command to run your GCP code. Next Chapter: Chapter 2: Batch Processing Orchestration with Composer and Airflow","title":"Setting up a Service Account"},{"location":"de-gcp-book/ch_02_orchestration/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 2 : Setting up Batch Processing Orchestration with Composer and Airflow Apache Airflow Overview Apache Airflow is a widely used open-source tool for orchestrating data pipelines written in Python. Airflow has a lot of great features, but two things it's particularly handy for is: 1. Allowing you to schedule data processing jobs. 2. Organize dependencies for your data processing job. Scheduling can be a trickier problem than it seems. What happens if a job isn't finished before it is scheduled to run again? What happens when the scheduling application is down at the time a job is scheduled. What if you realize you have a bug in your data processing code, and you need to reprocess all of your data for your scheduled runs for the last month? Airflow's scheduling has answers and configuration options for these sorts of scheduling problems. It's worth noting that GCP has a dedicated scheduling service, called Cloud Scheduler , but Airflow works better for our needs. Managing processing dependencies is a big part of creating a robust data pipeline (this is distinct from environment dependencies, such as whether Python is installed on your machine). If we're moving data from the Google Analytics API to a BigQuery table we know that we better download that data first before we try to load it into BigQuery. One way to manage this dependency is within our code, and every time we want this data updated we run our script. But suppose it takes an hour to download the data, and 5 minutes to upload it to BigQuery. If the script fails during the upload then you may be stuck running the whole script again, waiting another hour to download data that's already on your local disk. Alternatively, you could go about developing custom error handling for each of your pipelines that allows it to be restarted in pieces. If you've got a lot of pipelines, that means a lot of code to write and maintain. Airflow addresses this problem through the use of \"Tasks\", which are chunks of work that Airflow manages. These Tasks are organized into a Directed Acyclic Graph (DAG), which is a group of Tasks with dependencies defined between these Tasks. So you might have a DAG called \"update_google_analytics_table\" that has two Tasks: \"download_data_from_google_analytics\" and \"upload_data_to_bq\". We can tell airflow that \"download_data_from_google_analytics\" must complete successfully before \"upload_data_to_bq\" is run, and if either Task fails Airflow is to retry running the Task. After a designated number of retries, if the Task still has not succeeded it will mark itself as failed (in Chapter 13 we'll discuss setting up alerts for these failures). Another nice feature of Airflow is that it has a browser-based GUI that is useful for managing and monitoring your DAGs. Whole books have been written about Apache Airflow, and we've only scratched the surface. But this chapter is just focused on configuring Airflow to run, we'll revisit Airflow in Chapter 5 , where we'll discuss programming DAGs to run your data pipelines. Google Cloud Composer Overview Cloud Composer is Google's fully managed Airflow service. So rather than renting compute instances and installing Airflow yourself, Composer manages the compute instances for you under the hood. While offloading some of DevOps work to GCP is nice, it does provide a complication: Because GCP is a managed service, you are not able to run it locally. So your options are to create a composer instance on GCP for every developer, or set up a dockerized Airflow instance to run locally for development. The rest of this chapter will be dedicated to setting up your own Cloud Composer instance on GCP. Setting up Cloud Composer on GCP Apache Airflow, whether installed yourself or managed by GCP, requires a collection of infrastructure pieces that coordinate to make the application work. GCP calls an Airflow instance an \"Environment\" because what you are launching is the environment for all these pieces to work together. Cloud Composer uses the following GCP services to run: Cloud SQL, App Engine, Cloud Storage, Kubernetes Engine, Cloud Logging, Cloud Monitoring, and Pub/Sub. Fortunately GCP handles all that infrastructure for us. It's also important to be aware that unlike some other services by GCP, Composer does not auto-scale. You are required to designate the number and size of the machines you want to use, with more compute power assigned meaning an increased bill from GCP. If you need to change your assigned compute power you must do so manually. Creating the Composer Instance In Chapter 1 I discussed installing the GCP command line tools. You'll need them for this section. You're first step is to enable Cloud Composer, which you can do here . Select your Project from the drop-down and click \"Continue\". You'll be taken to a page prompting you to set up your credentials. GCP is reminding you that you should set up a Service Account that will allow you to access the Composer API that you just enabled. We already set up our Service Account in Chapter 1, but now we can grant the Service Account permission to set up a Composer Environment: > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/composer.worker' As stated above, a Composer \"Environment\" is equivalent to a managed Airflow instance. You create an Environment through the console and through the gcloud utility. In Chapter 11: Deployment Pipelines with Cloud Build I will go over managing your GCP infrastructure with Terraform, including managing Composer Environments. We can create a Composer Environment with the following command (WARNING: it can take up to a half hour to create the Environment): > gcloud composer environments create my-dev-environment \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com I've specified a few common options, but there are many more options that you can read about here . To verify your Environment is running you can execute: > gcloud composer environments list --locations us-central1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 NAME \u2502 LOCATION \u2502 STATE \u2502 CREATE_TIME \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 my-dev-environment \u2502 us-central1 \u2502 RUNNING \u2502 2020-10-16T04:04:19.264Z \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 > gcloud composer environments describe my-dev-environment --location us-central1 Testing a DAG The point of the Airflow instance is to orchestrate your DAGs, which is how you'll organize your batch data processing. I'll be talking a lot more about how to make DAGs in Chapter 5 (after we talk about GCS and BigQuery), but I'll go over a quick example here. A DAG is defined in a Python file that Airflow monitors and executes when scheduled. We'll create a DAG that has two tasks: one task will download a list of (mock) products and the other task will print a message indicating the task completed. In Chapter 11: Deployment Pipelines with Cloud Build I will discuss how to automate the deployment of these files to GCP, where they will run, but for now we can do that manually. So let's make our Python file: # my_first_dag.py import requests import datetime from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonOperator default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'my_first_dag', schedule_interval=\"6 * * * *\", # run every day at 6am UTC max_active_runs=1, catchup=False, default_args=default_args ) # A function to download product data from a web API. def get_product_data() -> str: url = 'https://gorest.co.in/public-api/products' result = requests.get(url) data = result.json() products = data['data'] return f'We downloaded {len(products)} products!' # A task to download product data from a web API. t_get_product_data = PythonOperator( task_id='get_product_data', python_callable=get_product_data, dag=dag ) # A task to print that the product data has been downloaded. t_print_message = BashOperator( task_id='print_message', bash_command='echo \"Product data has been downloaded. Congrats on your first DAG!!!!!!!\"', dag=dag ) # Setting the first task as a dependency for the second task. t_print_message.set_upstream(t_get_product_data) # In a more realistic DAG we would be saving this data to GCS, then updating BigQuery. # We'll dive deeper into building DAGs in [Chapter 5](https://github.com/Nunie123/data_engineering_on_gcp_book/blob/master/ch_05_dags.md). Now we need to put our DAG file where our Composer Environment can find it. GCP handles this by sticking all of the DAGs in a GCS bucket. We can find the bucket by running: > gcloud composer environments describe my-dev-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-dev-environm-63db6d2e-bucket/dags You can access the bucket with your DAGs just like any other bucket (we talk more about GCS in Chapter 3 ), but we don't actually need to access the bucket directly to add our DAG. Instead we can use the command: > gcloud composer environments storage dags import \\ --environment my-dev-environment \\ --location us-central1 \\ --source my_first_dag.py Now lets view the Airflow web interface so we can see our DAG running. We can see the address by running: > gcloud composer environments describe my-dev-environment \\ --location us-central1 \\ --format=\"get(config.airflowUri)\" Copy that address to your browser, and authenticate if required. We'll talk more about the Airflow web interface in Chapter 5 . For now lets click on \"my_first_dag\". From here we can see that our tasks completed successfully. Cleaning Up GCP charges us for using the services we set up in this chapter. We will be using this Composer Environment again in Chapter 5 , so if you don't feel like setting it up again you can keep it running. Just be aware of your costs for Composer and GCS . When we set up our Composer Environment GCP also set up resources in GCS for us, which is convenient for setting Airflow up. We just have to remember we have more to shut down than Composer when we are cleaning up. We can delete the Composer Environment by running: > gcloud composer environments delete my-dev-environment --location us-central1 I already provided the command for finding the related bucket above. You can also find it by looking through all your buckets for the one named after your Composer Environment: > gsutil list gs://us-central1-my-dev-environm-63db6d2e-bucket/ Now we can delete the bucket with: > gsutil rm -r gs://us-central1-my-dev-environm-63db6d2e-bucket/ Next Chapter: Chapter 3: Building a Data Lake with Google Cloud Storage (GCS)","title":"Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow"},{"location":"de-gcp-book/ch_02_orchestration/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_02_orchestration/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_02_orchestration/#chapter-2-setting-up-batch-processing-orchestration-with-composer-and-airflow","text":"","title":"Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow"},{"location":"de-gcp-book/ch_02_orchestration/#apache-airflow-overview","text":"Apache Airflow is a widely used open-source tool for orchestrating data pipelines written in Python. Airflow has a lot of great features, but two things it's particularly handy for is: 1. Allowing you to schedule data processing jobs. 2. Organize dependencies for your data processing job. Scheduling can be a trickier problem than it seems. What happens if a job isn't finished before it is scheduled to run again? What happens when the scheduling application is down at the time a job is scheduled. What if you realize you have a bug in your data processing code, and you need to reprocess all of your data for your scheduled runs for the last month? Airflow's scheduling has answers and configuration options for these sorts of scheduling problems. It's worth noting that GCP has a dedicated scheduling service, called Cloud Scheduler , but Airflow works better for our needs. Managing processing dependencies is a big part of creating a robust data pipeline (this is distinct from environment dependencies, such as whether Python is installed on your machine). If we're moving data from the Google Analytics API to a BigQuery table we know that we better download that data first before we try to load it into BigQuery. One way to manage this dependency is within our code, and every time we want this data updated we run our script. But suppose it takes an hour to download the data, and 5 minutes to upload it to BigQuery. If the script fails during the upload then you may be stuck running the whole script again, waiting another hour to download data that's already on your local disk. Alternatively, you could go about developing custom error handling for each of your pipelines that allows it to be restarted in pieces. If you've got a lot of pipelines, that means a lot of code to write and maintain. Airflow addresses this problem through the use of \"Tasks\", which are chunks of work that Airflow manages. These Tasks are organized into a Directed Acyclic Graph (DAG), which is a group of Tasks with dependencies defined between these Tasks. So you might have a DAG called \"update_google_analytics_table\" that has two Tasks: \"download_data_from_google_analytics\" and \"upload_data_to_bq\". We can tell airflow that \"download_data_from_google_analytics\" must complete successfully before \"upload_data_to_bq\" is run, and if either Task fails Airflow is to retry running the Task. After a designated number of retries, if the Task still has not succeeded it will mark itself as failed (in Chapter 13 we'll discuss setting up alerts for these failures). Another nice feature of Airflow is that it has a browser-based GUI that is useful for managing and monitoring your DAGs. Whole books have been written about Apache Airflow, and we've only scratched the surface. But this chapter is just focused on configuring Airflow to run, we'll revisit Airflow in Chapter 5 , where we'll discuss programming DAGs to run your data pipelines.","title":"Apache Airflow Overview"},{"location":"de-gcp-book/ch_02_orchestration/#google-cloud-composer-overview","text":"Cloud Composer is Google's fully managed Airflow service. So rather than renting compute instances and installing Airflow yourself, Composer manages the compute instances for you under the hood. While offloading some of DevOps work to GCP is nice, it does provide a complication: Because GCP is a managed service, you are not able to run it locally. So your options are to create a composer instance on GCP for every developer, or set up a dockerized Airflow instance to run locally for development. The rest of this chapter will be dedicated to setting up your own Cloud Composer instance on GCP.","title":"Google Cloud Composer Overview"},{"location":"de-gcp-book/ch_02_orchestration/#setting-up-cloud-composer-on-gcp","text":"Apache Airflow, whether installed yourself or managed by GCP, requires a collection of infrastructure pieces that coordinate to make the application work. GCP calls an Airflow instance an \"Environment\" because what you are launching is the environment for all these pieces to work together. Cloud Composer uses the following GCP services to run: Cloud SQL, App Engine, Cloud Storage, Kubernetes Engine, Cloud Logging, Cloud Monitoring, and Pub/Sub. Fortunately GCP handles all that infrastructure for us. It's also important to be aware that unlike some other services by GCP, Composer does not auto-scale. You are required to designate the number and size of the machines you want to use, with more compute power assigned meaning an increased bill from GCP. If you need to change your assigned compute power you must do so manually.","title":"Setting up Cloud Composer on GCP"},{"location":"de-gcp-book/ch_02_orchestration/#creating-the-composer-instance","text":"In Chapter 1 I discussed installing the GCP command line tools. You'll need them for this section. You're first step is to enable Cloud Composer, which you can do here . Select your Project from the drop-down and click \"Continue\". You'll be taken to a page prompting you to set up your credentials. GCP is reminding you that you should set up a Service Account that will allow you to access the Composer API that you just enabled. We already set up our Service Account in Chapter 1, but now we can grant the Service Account permission to set up a Composer Environment: > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/composer.worker' As stated above, a Composer \"Environment\" is equivalent to a managed Airflow instance. You create an Environment through the console and through the gcloud utility. In Chapter 11: Deployment Pipelines with Cloud Build I will go over managing your GCP infrastructure with Terraform, including managing Composer Environments. We can create a Composer Environment with the following command (WARNING: it can take up to a half hour to create the Environment): > gcloud composer environments create my-dev-environment \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com I've specified a few common options, but there are many more options that you can read about here . To verify your Environment is running you can execute: > gcloud composer environments list --locations us-central1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 NAME \u2502 LOCATION \u2502 STATE \u2502 CREATE_TIME \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 my-dev-environment \u2502 us-central1 \u2502 RUNNING \u2502 2020-10-16T04:04:19.264Z \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 > gcloud composer environments describe my-dev-environment --location us-central1","title":"Creating the Composer Instance"},{"location":"de-gcp-book/ch_02_orchestration/#testing-a-dag","text":"The point of the Airflow instance is to orchestrate your DAGs, which is how you'll organize your batch data processing. I'll be talking a lot more about how to make DAGs in Chapter 5 (after we talk about GCS and BigQuery), but I'll go over a quick example here. A DAG is defined in a Python file that Airflow monitors and executes when scheduled. We'll create a DAG that has two tasks: one task will download a list of (mock) products and the other task will print a message indicating the task completed. In Chapter 11: Deployment Pipelines with Cloud Build I will discuss how to automate the deployment of these files to GCP, where they will run, but for now we can do that manually. So let's make our Python file: # my_first_dag.py import requests import datetime from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonOperator default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'my_first_dag', schedule_interval=\"6 * * * *\", # run every day at 6am UTC max_active_runs=1, catchup=False, default_args=default_args ) # A function to download product data from a web API. def get_product_data() -> str: url = 'https://gorest.co.in/public-api/products' result = requests.get(url) data = result.json() products = data['data'] return f'We downloaded {len(products)} products!' # A task to download product data from a web API. t_get_product_data = PythonOperator( task_id='get_product_data', python_callable=get_product_data, dag=dag ) # A task to print that the product data has been downloaded. t_print_message = BashOperator( task_id='print_message', bash_command='echo \"Product data has been downloaded. Congrats on your first DAG!!!!!!!\"', dag=dag ) # Setting the first task as a dependency for the second task. t_print_message.set_upstream(t_get_product_data) # In a more realistic DAG we would be saving this data to GCS, then updating BigQuery. # We'll dive deeper into building DAGs in [Chapter 5](https://github.com/Nunie123/data_engineering_on_gcp_book/blob/master/ch_05_dags.md). Now we need to put our DAG file where our Composer Environment can find it. GCP handles this by sticking all of the DAGs in a GCS bucket. We can find the bucket by running: > gcloud composer environments describe my-dev-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-dev-environm-63db6d2e-bucket/dags You can access the bucket with your DAGs just like any other bucket (we talk more about GCS in Chapter 3 ), but we don't actually need to access the bucket directly to add our DAG. Instead we can use the command: > gcloud composer environments storage dags import \\ --environment my-dev-environment \\ --location us-central1 \\ --source my_first_dag.py Now lets view the Airflow web interface so we can see our DAG running. We can see the address by running: > gcloud composer environments describe my-dev-environment \\ --location us-central1 \\ --format=\"get(config.airflowUri)\" Copy that address to your browser, and authenticate if required. We'll talk more about the Airflow web interface in Chapter 5 . For now lets click on \"my_first_dag\". From here we can see that our tasks completed successfully.","title":"Testing a DAG"},{"location":"de-gcp-book/ch_02_orchestration/#cleaning-up","text":"GCP charges us for using the services we set up in this chapter. We will be using this Composer Environment again in Chapter 5 , so if you don't feel like setting it up again you can keep it running. Just be aware of your costs for Composer and GCS . When we set up our Composer Environment GCP also set up resources in GCS for us, which is convenient for setting Airflow up. We just have to remember we have more to shut down than Composer when we are cleaning up. We can delete the Composer Environment by running: > gcloud composer environments delete my-dev-environment --location us-central1 I already provided the command for finding the related bucket above. You can also find it by looking through all your buckets for the one named after your Composer Environment: > gsutil list gs://us-central1-my-dev-environm-63db6d2e-bucket/ Now we can delete the bucket with: > gsutil rm -r gs://us-central1-my-dev-environm-63db6d2e-bucket/ Next Chapter: Chapter 3: Building a Data Lake with Google Cloud Storage (GCS)","title":"Cleaning Up"},{"location":"de-gcp-book/ch_03_data_lake/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 3 : Building a Data Lake with Google Cloud Storage (GCS) GCS provides cheap and easily accessible storage for any type of file. It is quite similar to AWS Simple Storage Solution (S3), and even has dedicated commands to be interoperable with S3. What's a Data Lake? A Data Lake is a schemaless data repository used for storing source files in their native format. If you're completely unfamiliar with the concept, you can think of it as a folder where all the data you use is saved together. While a Data Warehouse imposes a schema on the data in the form of tables and relationships between tables (see Chapter 4 for more information on Data Warehousing), a Data Lake imposes no schema on the data it stores. Some benefits are: 1. It's easy to ingest source data if the entire ingestion process is just to save the data unedited from the source system. 2. It ensures data is not lost as a result of data transformations. The original data is always accessible for downstream processing. Some drawbacks are: 1. It's hard to query. The schemaless structure can make it difficult to understand how data relates to each other. 2. It can be messy. Source system data may benefit from cleaning (de-duplication field value normalization, etc.)before it is used downstream. Technologies allowing users query Data Lakes directly continues to improve. While there may be some scenarios where it makes sense to use a Data Lake without a Data Warehouse, such as organizations that are focused purely on Machine Learning and are not using Data Engineers to support Analytics, it is more common for an organization to use a Data Lake and Data Warehouse together. While Data Lakes are generally considered to be schemaless, there is usually the ability to add some structure. Below we'll discuss how to use multiple GCS buckets and bucket sub-folders to organize our Data Lake. Using GCS as a Data Lake GCS works well as a Data Lake because it is cheap, easy to access, and integrates well with other GCP services, particularly BigQuery. Every file saved in GCS is called an Object (also referred to as a \"Blob\"). Every Object is contained within a Bucket. Technically, every Object is saved at the top level of the Bucket. There is no hierarchical structure like would be used in a Windows or MacOS file system. However, GCS allows the Objects to be named as if they are in sub-directories, so for practical purposes we can save files in sub-directories inside a Bucket. You can manage GCS through the Console, the gsutil command line tool, through various code libraries, or through a REST API. I'll discuss gsutil and the Python library, since those integrate easiest with Airflow (I'll demonstrate using GCS with Airflow/Composer in Chapter 5 ). Using the gsutil command line tool In Chapter 1 I discussed installing the GCP command line tools. You'll need them for this section. Creating a bucket is quite easy: > gsutil mb gs://de-book-dev The name of a Bucket must be unique across all GCP accounts, so you may find that the Bucket name you want to use is not available. > gsutil mb gs://de-book-dev Creating gs://de-book-bucket/... ServiceException: 409 Bucket de-book-dev already exists. We can see all our Buckets by running: > gsutil ls Now let's create some files and then copy them into our Bucket: > mkdir files > echo \"This is text\" > ./files/a_text_file.txt > echo '{\"this_is_json: true}' > ./files/a_json_file.json > gsutil cp ./files/a_text_file.txt gs://de-book-dev [...] > gsutil cp -r ./files gs://de-book-dev [...] > gsutil cp ./files/*.json gs://de-book-dev/json_files/ [...] The three cp commands above demonstrate: 1. Copying a single file into a Bucket. 2. Copying an entire directory into a Bucket. 3. Copying all files with a \".json\" extension into a Bucket's sub-directory. Let's take a look at the Bucket to make sure the Objects made it in: > gsutil ls gs://de-book-dev gs://de-book-dev/a_text_file.txt gs://de-book-dev/json_files/ gs://de-book-dev/files/ > gsutil ls gs://de-book-dev/files gs://de-book-dev/files/a_json_file.json gs://de-book-dev/files/a_text_file.txt > gsutil ls gs://de-book-dev/json_files gs://de-book-dev/json_files > gsutil ls gs://de-book-dev/* [...] We can just as easily use the cp command to download the files from GCS by switching the source and destination arguments in the examples above. The cp command can also be used to copy files between GCS buckets. The gsutil rm and gsutil mv commands, among others , are also available for use. One command we should discuss that isn't based on a UNIX command like cp is the gsutil rsync command. Using rsync on a local folder will ensure that any file in that folder is also added to the designated Bucket, if it doesn't yet exist there: > gsutil rsync ./files gs://de-book-dev/synced_files/ Another nice feature of GCS is that you can use the gsutil cp and gsutil rsync commands directly with AWS S3: > gsutil cp s3://my-aws-bucket/some_file.txt gs://my-gcs-bucket For this to work you'll need to make a Boto configuration file to manage your AWS credentials. Details are here . Using the google.cloud.storage Python Library To access this library will need have the Python google-cloud-package library installed and configured . Inside your Python virtual environment run: > pip install --upgrade google-cloud-storage Now we need to configure our credentials. In Chapter 2 we set up a Service Account and generated a secret key. You'll need that service account and key file to continue. You can see all your service accounts with: > gcloud iam service-accounts list DISPLAY NAME EMAIL DISABLED composer-dev composer-dev@de-book-dev.iam.gserviceaccount.com False Compute Engine default service account 204024561480-compute@developer.gserviceaccount.com False We now need to give our composer-dev service account permission to manage GCS: > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/storage.admin' Finally, we need to set the GOOGLE_APPLICATION_CREDENTIALS variable in our shell, referencing our secret key file that we saved in Chapter 2 : > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" If you like, you can save this variable definition in your .bash_profile file, so that it will be set by default every time your terminal loads. Now we'll create some handy functions using the create_bucket() method from the Client object, and the upload_from_filename() method from the Blob object. from google.cloud import storage def create_bucket(bucket_name: str) -> None: \"\"\" This function creates a bucket with the provided bucket name. The project and location are set to default. \"\"\" client = storage.Client() client.create_bucket(bucket_name) def upload_file_to_bucket(source_filepath: str, bucket_name: str, blob_name: str) -> None: \"\"\" This function uploads a file to a bucket. The blob_name will be the name of the file in GCS. \"\"\" client = storage.Client() bucket = client.bucket(bucket_name) blob = bucket.blob(blob_name) blob.upload_from_filename(source_filepath) create_bucket('de-book-test-bucket') upload_file_to_bucket('./files/a_text_file.txt', 'de-book-test-bucket', 'my_renamed_file.txt') Documentation for the Python storage library is here . Cleaning Up While the prices for storage in GCS are pretty cheap, it's still worth cleaning up our Buckets. We can see all our Buckets with this command: > gsutil ls gs://de-book-dev/ gs://de-book-test-bucket/ If you didn't clean up after Chapter 2 , you should see some Buckets related to the Composer Environment we set up, in addition to the Buckets we created this chapter. To delete a Bucket and everything inside run: > gsutil rm -r gs://de-book-test-bucket To delete all the objects in a Bucket, but leave the Bucket itself, run: > gsutil rm gs://de-book-dev/** Next Chapter: Chapter 4: Building a Data Warehouse with BigQuery","title":"Chapter 3: Building a Data Lake with Google Cloud Storage (GCS)"},{"location":"de-gcp-book/ch_03_data_lake/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_03_data_lake/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_03_data_lake/#chapter-3-building-a-data-lake-with-google-cloud-storage-gcs","text":"GCS provides cheap and easily accessible storage for any type of file. It is quite similar to AWS Simple Storage Solution (S3), and even has dedicated commands to be interoperable with S3.","title":"Chapter 3 : Building a Data Lake with Google Cloud Storage (GCS)"},{"location":"de-gcp-book/ch_03_data_lake/#whats-a-data-lake","text":"A Data Lake is a schemaless data repository used for storing source files in their native format. If you're completely unfamiliar with the concept, you can think of it as a folder where all the data you use is saved together. While a Data Warehouse imposes a schema on the data in the form of tables and relationships between tables (see Chapter 4 for more information on Data Warehousing), a Data Lake imposes no schema on the data it stores. Some benefits are: 1. It's easy to ingest source data if the entire ingestion process is just to save the data unedited from the source system. 2. It ensures data is not lost as a result of data transformations. The original data is always accessible for downstream processing. Some drawbacks are: 1. It's hard to query. The schemaless structure can make it difficult to understand how data relates to each other. 2. It can be messy. Source system data may benefit from cleaning (de-duplication field value normalization, etc.)before it is used downstream. Technologies allowing users query Data Lakes directly continues to improve. While there may be some scenarios where it makes sense to use a Data Lake without a Data Warehouse, such as organizations that are focused purely on Machine Learning and are not using Data Engineers to support Analytics, it is more common for an organization to use a Data Lake and Data Warehouse together. While Data Lakes are generally considered to be schemaless, there is usually the ability to add some structure. Below we'll discuss how to use multiple GCS buckets and bucket sub-folders to organize our Data Lake.","title":"What's a Data Lake?"},{"location":"de-gcp-book/ch_03_data_lake/#using-gcs-as-a-data-lake","text":"GCS works well as a Data Lake because it is cheap, easy to access, and integrates well with other GCP services, particularly BigQuery. Every file saved in GCS is called an Object (also referred to as a \"Blob\"). Every Object is contained within a Bucket. Technically, every Object is saved at the top level of the Bucket. There is no hierarchical structure like would be used in a Windows or MacOS file system. However, GCS allows the Objects to be named as if they are in sub-directories, so for practical purposes we can save files in sub-directories inside a Bucket. You can manage GCS through the Console, the gsutil command line tool, through various code libraries, or through a REST API. I'll discuss gsutil and the Python library, since those integrate easiest with Airflow (I'll demonstrate using GCS with Airflow/Composer in Chapter 5 ).","title":"Using GCS as a Data Lake"},{"location":"de-gcp-book/ch_03_data_lake/#using-the-gsutil-command-line-tool","text":"In Chapter 1 I discussed installing the GCP command line tools. You'll need them for this section. Creating a bucket is quite easy: > gsutil mb gs://de-book-dev The name of a Bucket must be unique across all GCP accounts, so you may find that the Bucket name you want to use is not available. > gsutil mb gs://de-book-dev Creating gs://de-book-bucket/... ServiceException: 409 Bucket de-book-dev already exists. We can see all our Buckets by running: > gsutil ls Now let's create some files and then copy them into our Bucket: > mkdir files > echo \"This is text\" > ./files/a_text_file.txt > echo '{\"this_is_json: true}' > ./files/a_json_file.json > gsutil cp ./files/a_text_file.txt gs://de-book-dev [...] > gsutil cp -r ./files gs://de-book-dev [...] > gsutil cp ./files/*.json gs://de-book-dev/json_files/ [...] The three cp commands above demonstrate: 1. Copying a single file into a Bucket. 2. Copying an entire directory into a Bucket. 3. Copying all files with a \".json\" extension into a Bucket's sub-directory. Let's take a look at the Bucket to make sure the Objects made it in: > gsutil ls gs://de-book-dev gs://de-book-dev/a_text_file.txt gs://de-book-dev/json_files/ gs://de-book-dev/files/ > gsutil ls gs://de-book-dev/files gs://de-book-dev/files/a_json_file.json gs://de-book-dev/files/a_text_file.txt > gsutil ls gs://de-book-dev/json_files gs://de-book-dev/json_files > gsutil ls gs://de-book-dev/* [...] We can just as easily use the cp command to download the files from GCS by switching the source and destination arguments in the examples above. The cp command can also be used to copy files between GCS buckets. The gsutil rm and gsutil mv commands, among others , are also available for use. One command we should discuss that isn't based on a UNIX command like cp is the gsutil rsync command. Using rsync on a local folder will ensure that any file in that folder is also added to the designated Bucket, if it doesn't yet exist there: > gsutil rsync ./files gs://de-book-dev/synced_files/ Another nice feature of GCS is that you can use the gsutil cp and gsutil rsync commands directly with AWS S3: > gsutil cp s3://my-aws-bucket/some_file.txt gs://my-gcs-bucket For this to work you'll need to make a Boto configuration file to manage your AWS credentials. Details are here .","title":"Using the gsutil command line tool"},{"location":"de-gcp-book/ch_03_data_lake/#using-the-googlecloudstorage-python-library","text":"To access this library will need have the Python google-cloud-package library installed and configured . Inside your Python virtual environment run: > pip install --upgrade google-cloud-storage Now we need to configure our credentials. In Chapter 2 we set up a Service Account and generated a secret key. You'll need that service account and key file to continue. You can see all your service accounts with: > gcloud iam service-accounts list DISPLAY NAME EMAIL DISABLED composer-dev composer-dev@de-book-dev.iam.gserviceaccount.com False Compute Engine default service account 204024561480-compute@developer.gserviceaccount.com False We now need to give our composer-dev service account permission to manage GCS: > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/storage.admin' Finally, we need to set the GOOGLE_APPLICATION_CREDENTIALS variable in our shell, referencing our secret key file that we saved in Chapter 2 : > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" If you like, you can save this variable definition in your .bash_profile file, so that it will be set by default every time your terminal loads. Now we'll create some handy functions using the create_bucket() method from the Client object, and the upload_from_filename() method from the Blob object. from google.cloud import storage def create_bucket(bucket_name: str) -> None: \"\"\" This function creates a bucket with the provided bucket name. The project and location are set to default. \"\"\" client = storage.Client() client.create_bucket(bucket_name) def upload_file_to_bucket(source_filepath: str, bucket_name: str, blob_name: str) -> None: \"\"\" This function uploads a file to a bucket. The blob_name will be the name of the file in GCS. \"\"\" client = storage.Client() bucket = client.bucket(bucket_name) blob = bucket.blob(blob_name) blob.upload_from_filename(source_filepath) create_bucket('de-book-test-bucket') upload_file_to_bucket('./files/a_text_file.txt', 'de-book-test-bucket', 'my_renamed_file.txt') Documentation for the Python storage library is here .","title":"Using the google.cloud.storage Python Library"},{"location":"de-gcp-book/ch_03_data_lake/#cleaning-up","text":"While the prices for storage in GCS are pretty cheap, it's still worth cleaning up our Buckets. We can see all our Buckets with this command: > gsutil ls gs://de-book-dev/ gs://de-book-test-bucket/ If you didn't clean up after Chapter 2 , you should see some Buckets related to the Composer Environment we set up, in addition to the Buckets we created this chapter. To delete a Bucket and everything inside run: > gsutil rm -r gs://de-book-test-bucket To delete all the objects in a Bucket, but leave the Bucket itself, run: > gsutil rm gs://de-book-dev/** Next Chapter: Chapter 4: Building a Data Warehouse with BigQuery","title":"Cleaning Up"},{"location":"de-gcp-book/ch_04_data_warehouse/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 4 : Building a Data Warehouse with BigQuery BigQuery is GCP's fully managed Data Warehouse service. There are no steps to take to get it running, as long as you've got a GCP account you can just start using it. While GCP's Data Engineering infrastructure services are often quite similar to corresponding AWS services, BigQuery is actually a significant departure from AWS's Data Warehouse service: Redshift. It's features and quirks include: 1. BigQuery is fully-managed, which means that there are no options for setting compute power, storage size, or any other parameters you might be used to setting for a cloud-based database. BigQuery will auto-scale under-the-hood, so you never have to worry about whether you need to increase CPU or storage. Consequently, the billing is quite different. Rather than paying for up-time, like you will with Composer, you are billed for BigQuery based on how much data you are querying, and how much data you are storing. This can be a little dangerous, as it's not hard to accidentally run a lot of expensive queries and run up your bill. 2. BigQuery behaves like a relational database in most respects, but does allow nested fields. For example, you can define a \"users\" table that has a \"phone_numbers\" field. That field could have multiple repeated sub-fields such as \"phone_number\" and \"phone_type\", allowing you to store multiple phone numbers with a single \"user\" record. 3. BigQuery lacks primary keys and indexing, but does allow partitioning and clustering. Partitioning is useful because full table scans can cost a lot of money on large tables. Filtering by partition allows you to scan less data, reducing the cost of the query while improving performance. Partitions can still be somewhat limiting, however, as they can only be designated on a single column per table and the column must be a date, datetime, timestamp, or integer. Clustering can be used more similarly to how indexing is used on a RDBMS, improving performance by specifying a column or columns that are used frequently in queries. 4. BigQuery can be managed by its Data Definition Language (DDL) and Data Manipulation Language (DML), but can also use be managed using the bq command line tool and dedicated libraries for a variety of programming languages. 5. BigQuery tables are created inside \"Datasets\", which are just name-spaces for groups of tables. All Datasets are associated with a particular Project (the same way Composer Environments and GCS Buckets must be associated with a Project). BigQuery Query Editor BigQuery works well with GCS and Composer, allowing you to load and transform your data in your Warehouse using Python and the bq command line tool. However, you're still probably going to want a SQL client to debug, prototype, and explore the data in your Warehouse. Fortunately, the GCP Console includes a BigQuery SQL client for you to use. If you want to use your own SQL client, you can connect via JDBC and ODBC drivers, as described here . While you can do SQL scripting inside the Query Editor, you're going to want to use the bq command line tool and BigQuery Python library, described below. Using the bq Command Line Tool In Chapter 1 I discussed installing the GCP command line tools. You'll need them for this section. We'll also need to give our service account permission to access BigQuery: > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/bigquery.admin' [...] > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" Creating Tables from the Command Line All tables must have a Dataset inside which they can be created. So we start by creating a Dataset, then verifying it was created: > bq mk --dataset my_dataset > bq ls -d Now let's create a table in our new Dataset: > bq mk --table my_dataset.inventory product_name:STRING,product_count:INT64,price:FLOAT64 > bq ls --format=pretty my_dataset +-----------+-------+--------+-------------------+------------------+ | tableId | Type | Labels | Time Partitioning | Clustered Fields | +-----------+-------+--------+-------------------+------------------+ | inventory | TABLE | | | | +-----------+-------+--------+-------------------+------------------+ > bq show --schema my_dataset.inventory [{\"name\":\"product_name\",\"type\":\"STRING\"},{\"name\":\"product_count\",\"type\":\"INTEGER\"},{\"name\":\"price\",\"type\":\"FLOAT\"}] In the above code we created the table by providing the schema in-line with the command. Defining the schema in-line is usually impractical, so instead we'll create a file with the schema defined as JSON: > echo ' [ { \"description\": \"The name of the item being sold.\", \"mode\": \"REQUIRED\", \"name\": \"product_name\", \"type\": \"STRING\" }, { \"description\": \"The count of all items in inventory.\", \"mode\": \"NULLABLE\", \"name\": \"product_count\", \"type\": \"INT64\" }, { \"description\": \"The price the item is sold for, in USD.\", \"mode\": \"NULLABLE\", \"name\": \"price\", \"type\": \"FLOAT64\" } ]' > inventory_schema.json And now we'll create a table based on the schema file: > bq mk --table my_dataset.inventory_2 inventory_schema.json > bq ls --format=pretty my_dataset +-------------+-------+--------+-------------------+------------------+ | tableId | Type | Labels | Time Partitioning | Clustered Fields | +-------------+-------+--------+-------------------+------------------+ | inventory | TABLE | | | | | inventory_2 | TABLE | | | | +-------------+-------+--------+-------------------+------------------+ bq show --schema my_dataset.inventory_2 [{\"name\":\"product_name\",\"type\":\"STRING\",\"mode\":\"REQUIRED\",\"description\":\"The name of the item being sold.\"},{\"name\":\"product_count\",\"type\":\"INTEGER\",\"mode\":\"NULLABLE\",\"description\":\"The count of all items in inventory.\"},{\"name\":\"price\",\"type\":\"FLOAT\",\"mode\":\"NULLABLE\",\"description\":\"The price the item is sold for, in USD.\"}] More information on defining the schema is available here . You can also define a new table by making based on a query from an existing table. > bq query --destination_table my_dataset.inventory_3 \\ > --use_legacy_sql=false \\ > 'select * from my_dataset.inventory' Finally, you can create a new table as part of the bq load command. I'll talk more about loading data in the next section. > bq load \\ --source_format=NEWLINE_DELIMITED_JSON \\ > my_dataset.my_table \\ > gs://path/to/blob/in/bucket/file.json \\ > my_schema.json Loading Data from the Command Line The major pieces of information you'll need to know for your load operation are: * The destination project, dataset, and table * The source file location * The source file format ( Avro , Parquet , ORC , CSV , JSON ) * The schema of the destination table * Whether you are replacing or appending to the destination table ( --replace and --noreplace flags) * How you are choosing to partition the table Let's start by creating our raw data file to be loaded into BigQuery: > echo ' [ { \"product_id\": 1, \"product_name\": \"hammer\", \"product_tags\": [ { \"tag_id\": 1, \"tag_name\": \"sale\" }, { \"tag_id\": 2, \"tag_name\": \"tool\" } ], \"created_on\": \"2020-10-19\" }, { \"product_id\": 2, \"product_name\": \"pen\", \"product_tags\": [ { \"tag_id\": 1, \"tag_name\": \"sale\" }, { \"tag_id\": 3, \"tag_name\": \"stationary\" } ], \"created_on\": \"2020-10-19\" }, { \"product_id\": 3, \"product_name\": \"drill\", \"product_tags\": [ { \"tag_id\": 2, \"tag_name\": \"tool\" } ], \"created_on\": \"2020-10-19\" } ]' > products.json While BigQuery can ingest JSON files, they must be newline delimited JSON (also called JSON Lines) files. This means that each record is separated by a newline character. Many source systems will export to newline delimited JSON, but if you're stuck with a JSON blob like we have above you'll have to convert it yourself. Let's convert the above file using the jq command line tool: > cat products.json | jq -c '.[]' > products.jsonl Note that I used the \".jsonl\" extension for the destination file. I could have stuck with the \".json\" extension, as BigQuery doesn't care what the extension is. Now that we have our data we can define our schema. As mentioned in above, the load command will create a table if one does not exist, so providing the schema here will define your table. The --autodetect flag can be used to have BigQuery infer your schema from the data, but that should generally be avoided outside of development and prototyping. Let's create a schema definition file: > echo ' [ { \"name\": \"product_id\", \"type\": \"INT64\", \"mode\": \"REQUIRED\", \"description\": \"The unique ID for the product.\" },{ \"name\": \"product_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\", \"description\": \"The name of the product.\" },{ \"name\": \"product_tags\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"description\": \"All tags associated with the product.\", \"fields\": [{ \"name\": \"tag_id\", \"type\": \"INT64\", \"mode\": \"NULLABLE\", \"description\": \"The unique ID for the tag.\" }, { \"name\": \"tag_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\", \"description\": \"The name of the tag.\" }] },{ \"name\": \"created_on\", \"type\": \"DATE\", \"mode\": \"REQUIRED\", \"description\": \"The date the product was added to inventory.\" } ]' > products_schema.json You'll notice that this schema defines a nested field, called a \"REPEATED\" field in BigQuery. Fields can be nested up to 15 layers deep. The schema above includes the \"product_tags\" field, a field of \"REPEATED\" \"RECORDS\" which is the equivalent of an array or objects in JSON. BigQuery also supports structures like an array of integers, or an array of dates, but it does not support an array of arrays. Now that we have our schema file, let's load our data. > bq load \\ --source_format=NEWLINE_DELIMITED_JSON \\ --replace \\ --time_partitioning_type=DAY \\ --time_partitioning_field created_on \\ > my_dataset.products \\ > products.jsonl \\ > products_schema.json Upload complete. Waiting on bqjob_r3f8263b30008fbdf_00000175442a455c_1 ... (1s) Current status: DONE > bq head --table my_dataset.products +------------+--------------+---------------------------------------------------------------------------+------------+ | product_id | product_name | product_tags | created_on | +------------+--------------+---------------------------------------------------------------------------+------------+ | 1 | hammer | [{\"tag_id\":\"1\",\"tag_name\":\"sale\"},{\"tag_id\":\"2\",\"tag_name\":\"tool\"}] | 2020-10-19 | | 2 | pen | [{\"tag_id\":\"1\",\"tag_name\":\"sale\"},{\"tag_id\":\"3\",\"tag_name\":\"stationary\"}] | 2020-10-19 | | 3 | drill | [{\"tag_id\":\"2\",\"tag_name\":\"tool\"}] | 2020-10-19 | +------------+--------------+---------------------------------------------------------------------------+------------+ I specified that the table is partitioned on the created_on field, so right now this. This means that each distinct date in that field will be treated as a distinct partition, improving performance when filtering on that field. Partitions are always optional, but are useful when you know you will be filtering on a particular field (e.g. querying for all products created in the last seven days). If I included --time_partitioning_type=DAY but did not provide a field to partition on, BigQuery would have automatically assigned a partition date of the date the record was ingested into BigQuery. We can filter on this automatically generated partition date by using BigQuery's pseudo-columns : _PARTITIONDATE , and _PARTITIONTIME . Using the google.cloud.bigquery Python Library We can also interact with BigQuery in Python. In Chapter 2 we used the google.cloud.storage library, and now for BigQuery we'll need to install the google.cloud.bigquery library: > pip install --upgrade google-cloud-bigquery Creating Tables from Python from google.cloud import bigquery def create_dataset(dataset_name: str, project_name: str) -> None: dataset_id = f'{project_name}.{dataset_name}' dataset_obj = bigquery.Dataset(dataset_id) client = bigquery.Client() dataset = client.create_dataset(dataset_obj) def create_table(table_name: str, dataset_name: str, project_name: str, schema: list) -> None: table_id = f'{project_name}.{dataset_name}.{table_name}' table_obj = bigquery.Table(table_id, schema) client = bigquery.Client() table = client.create_table(table_obj) project_name = 'de-book-dev' dataset_name = 'my_other_dataset' table_name = 'user_purchases' schema = [ bigquery.SchemaField('user_name', 'STRING', mode='REQUIRED'), bigquery.SchemaField('items_purchased', 'INT64', mode='REQUIRED'), bigquery.SchemaField('dollars_spent', 'FLOAT64', mode='REQUIRED') ] create_dataset(dataset_name, project_name) create_table(table_name, dataset_name, project_name, schema) We can also create a table from a query: from google.cloud import bigquery def create_table_from_query(table_name: str, dataset_name: str, project_name: str, raw_sql: str) -> None: table_id = f'{project_name}.{dataset_name}.{table_name}' job_config = bigquery.QueryJobConfig(destination=table_id) client = bigquery.Client() query_job = client.query(raw_sql, job_config=job_config) query_job.result() project_name = 'de-book-dev' dataset_name = 'my_other_dataset' table_name = 'user_purchases_copy' raw_sql = 'select user_name, items_purchased, dollars_spent from my_other_dataset.user_purchases' create_table_from_query(table_name, dataset_name, project_name, raw_sql) You can find the documentation for the Python API here . Loading Data from Python Now let's load our data using the \"products.json\" file we created above: import json from google.cloud import bigquery def load_json_data(table_id: str, source_file_location: str, schema: list) -> None: client = bigquery.Client() job_config = bigquery.LoadJobConfig( schema=schema, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, ) with open(source_file_location, 'r') as f: data = json.load(f) load_job = client.load_table_from_json( data, table_id, job_config=job_config, ) load_job.result() table_id = 'de-book-dev.my_other_dataset.products' source_file_location = 'products.json' schema = [ bigquery.SchemaField('product_id', 'INT64', mode='REQUIRED'), bigquery.SchemaField('product_name', 'STRING', mode='NULLABLE'), bigquery.SchemaField('product_tags' , 'RECORD' , mode='REPEATED' , fields=[ bigquery.SchemaField('tag_id', 'INT64', mode='REQUIRED'), bigquery.SchemaField('tag_name', 'STRING', mode='NULLABLE'), ]), bigquery.SchemaField('created_on', 'DATE', mode='REQUIRED') ] load_json_data(table_id, source_file_location, schema) Just like above, we're loading data with a nested field. Unlike the bq load command, if you wish to use the BigQuery's Python API to load data into a partitioned table you must create the table with the specified partition first, then load the data. Cleaning Up Because BigQuery bills based on data processed and storage quantity, rather than uptime, leaving our test data in there from this chapter will incur almost no costs. Nonetheless, it's good to clean up so we can have a blank start when we start creating and loading tables as part of our DAGs in Chapter 5 . Let's start by listing all our datasets: > bq ls -d my_dataset my_other_dataset Now let's delete them. We use the -r flag to indicate we also want the associated tables deleted: > bq rm -d -r my_dataset > bq rm -d -r my_other_dataset Next Chapter: Chapter 5: Setting up DAGs in Composer and Airflow","title":"Chapter 4: Building a Data Warehouse with BigQuery"},{"location":"de-gcp-book/ch_04_data_warehouse/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_04_data_warehouse/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_04_data_warehouse/#chapter-4-building-a-data-warehouse-with-bigquery","text":"BigQuery is GCP's fully managed Data Warehouse service. There are no steps to take to get it running, as long as you've got a GCP account you can just start using it. While GCP's Data Engineering infrastructure services are often quite similar to corresponding AWS services, BigQuery is actually a significant departure from AWS's Data Warehouse service: Redshift. It's features and quirks include: 1. BigQuery is fully-managed, which means that there are no options for setting compute power, storage size, or any other parameters you might be used to setting for a cloud-based database. BigQuery will auto-scale under-the-hood, so you never have to worry about whether you need to increase CPU or storage. Consequently, the billing is quite different. Rather than paying for up-time, like you will with Composer, you are billed for BigQuery based on how much data you are querying, and how much data you are storing. This can be a little dangerous, as it's not hard to accidentally run a lot of expensive queries and run up your bill. 2. BigQuery behaves like a relational database in most respects, but does allow nested fields. For example, you can define a \"users\" table that has a \"phone_numbers\" field. That field could have multiple repeated sub-fields such as \"phone_number\" and \"phone_type\", allowing you to store multiple phone numbers with a single \"user\" record. 3. BigQuery lacks primary keys and indexing, but does allow partitioning and clustering. Partitioning is useful because full table scans can cost a lot of money on large tables. Filtering by partition allows you to scan less data, reducing the cost of the query while improving performance. Partitions can still be somewhat limiting, however, as they can only be designated on a single column per table and the column must be a date, datetime, timestamp, or integer. Clustering can be used more similarly to how indexing is used on a RDBMS, improving performance by specifying a column or columns that are used frequently in queries. 4. BigQuery can be managed by its Data Definition Language (DDL) and Data Manipulation Language (DML), but can also use be managed using the bq command line tool and dedicated libraries for a variety of programming languages. 5. BigQuery tables are created inside \"Datasets\", which are just name-spaces for groups of tables. All Datasets are associated with a particular Project (the same way Composer Environments and GCS Buckets must be associated with a Project).","title":"Chapter 4: Building a Data Warehouse with BigQuery"},{"location":"de-gcp-book/ch_04_data_warehouse/#bigquery-query-editor","text":"BigQuery works well with GCS and Composer, allowing you to load and transform your data in your Warehouse using Python and the bq command line tool. However, you're still probably going to want a SQL client to debug, prototype, and explore the data in your Warehouse. Fortunately, the GCP Console includes a BigQuery SQL client for you to use. If you want to use your own SQL client, you can connect via JDBC and ODBC drivers, as described here . While you can do SQL scripting inside the Query Editor, you're going to want to use the bq command line tool and BigQuery Python library, described below.","title":"BigQuery Query Editor"},{"location":"de-gcp-book/ch_04_data_warehouse/#using-the-bq-command-line-tool","text":"In Chapter 1 I discussed installing the GCP command line tools. You'll need them for this section. We'll also need to give our service account permission to access BigQuery: > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/bigquery.admin' [...] > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\"","title":"Using the bq Command Line Tool"},{"location":"de-gcp-book/ch_04_data_warehouse/#creating-tables-from-the-command-line","text":"All tables must have a Dataset inside which they can be created. So we start by creating a Dataset, then verifying it was created: > bq mk --dataset my_dataset > bq ls -d Now let's create a table in our new Dataset: > bq mk --table my_dataset.inventory product_name:STRING,product_count:INT64,price:FLOAT64 > bq ls --format=pretty my_dataset +-----------+-------+--------+-------------------+------------------+ | tableId | Type | Labels | Time Partitioning | Clustered Fields | +-----------+-------+--------+-------------------+------------------+ | inventory | TABLE | | | | +-----------+-------+--------+-------------------+------------------+ > bq show --schema my_dataset.inventory [{\"name\":\"product_name\",\"type\":\"STRING\"},{\"name\":\"product_count\",\"type\":\"INTEGER\"},{\"name\":\"price\",\"type\":\"FLOAT\"}] In the above code we created the table by providing the schema in-line with the command. Defining the schema in-line is usually impractical, so instead we'll create a file with the schema defined as JSON: > echo ' [ { \"description\": \"The name of the item being sold.\", \"mode\": \"REQUIRED\", \"name\": \"product_name\", \"type\": \"STRING\" }, { \"description\": \"The count of all items in inventory.\", \"mode\": \"NULLABLE\", \"name\": \"product_count\", \"type\": \"INT64\" }, { \"description\": \"The price the item is sold for, in USD.\", \"mode\": \"NULLABLE\", \"name\": \"price\", \"type\": \"FLOAT64\" } ]' > inventory_schema.json And now we'll create a table based on the schema file: > bq mk --table my_dataset.inventory_2 inventory_schema.json > bq ls --format=pretty my_dataset +-------------+-------+--------+-------------------+------------------+ | tableId | Type | Labels | Time Partitioning | Clustered Fields | +-------------+-------+--------+-------------------+------------------+ | inventory | TABLE | | | | | inventory_2 | TABLE | | | | +-------------+-------+--------+-------------------+------------------+ bq show --schema my_dataset.inventory_2 [{\"name\":\"product_name\",\"type\":\"STRING\",\"mode\":\"REQUIRED\",\"description\":\"The name of the item being sold.\"},{\"name\":\"product_count\",\"type\":\"INTEGER\",\"mode\":\"NULLABLE\",\"description\":\"The count of all items in inventory.\"},{\"name\":\"price\",\"type\":\"FLOAT\",\"mode\":\"NULLABLE\",\"description\":\"The price the item is sold for, in USD.\"}] More information on defining the schema is available here . You can also define a new table by making based on a query from an existing table. > bq query --destination_table my_dataset.inventory_3 \\ > --use_legacy_sql=false \\ > 'select * from my_dataset.inventory' Finally, you can create a new table as part of the bq load command. I'll talk more about loading data in the next section. > bq load \\ --source_format=NEWLINE_DELIMITED_JSON \\ > my_dataset.my_table \\ > gs://path/to/blob/in/bucket/file.json \\ > my_schema.json","title":"Creating Tables from the Command Line"},{"location":"de-gcp-book/ch_04_data_warehouse/#loading-data-from-the-command-line","text":"The major pieces of information you'll need to know for your load operation are: * The destination project, dataset, and table * The source file location * The source file format ( Avro , Parquet , ORC , CSV , JSON ) * The schema of the destination table * Whether you are replacing or appending to the destination table ( --replace and --noreplace flags) * How you are choosing to partition the table Let's start by creating our raw data file to be loaded into BigQuery: > echo ' [ { \"product_id\": 1, \"product_name\": \"hammer\", \"product_tags\": [ { \"tag_id\": 1, \"tag_name\": \"sale\" }, { \"tag_id\": 2, \"tag_name\": \"tool\" } ], \"created_on\": \"2020-10-19\" }, { \"product_id\": 2, \"product_name\": \"pen\", \"product_tags\": [ { \"tag_id\": 1, \"tag_name\": \"sale\" }, { \"tag_id\": 3, \"tag_name\": \"stationary\" } ], \"created_on\": \"2020-10-19\" }, { \"product_id\": 3, \"product_name\": \"drill\", \"product_tags\": [ { \"tag_id\": 2, \"tag_name\": \"tool\" } ], \"created_on\": \"2020-10-19\" } ]' > products.json While BigQuery can ingest JSON files, they must be newline delimited JSON (also called JSON Lines) files. This means that each record is separated by a newline character. Many source systems will export to newline delimited JSON, but if you're stuck with a JSON blob like we have above you'll have to convert it yourself. Let's convert the above file using the jq command line tool: > cat products.json | jq -c '.[]' > products.jsonl Note that I used the \".jsonl\" extension for the destination file. I could have stuck with the \".json\" extension, as BigQuery doesn't care what the extension is. Now that we have our data we can define our schema. As mentioned in above, the load command will create a table if one does not exist, so providing the schema here will define your table. The --autodetect flag can be used to have BigQuery infer your schema from the data, but that should generally be avoided outside of development and prototyping. Let's create a schema definition file: > echo ' [ { \"name\": \"product_id\", \"type\": \"INT64\", \"mode\": \"REQUIRED\", \"description\": \"The unique ID for the product.\" },{ \"name\": \"product_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\", \"description\": \"The name of the product.\" },{ \"name\": \"product_tags\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"description\": \"All tags associated with the product.\", \"fields\": [{ \"name\": \"tag_id\", \"type\": \"INT64\", \"mode\": \"NULLABLE\", \"description\": \"The unique ID for the tag.\" }, { \"name\": \"tag_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\", \"description\": \"The name of the tag.\" }] },{ \"name\": \"created_on\", \"type\": \"DATE\", \"mode\": \"REQUIRED\", \"description\": \"The date the product was added to inventory.\" } ]' > products_schema.json You'll notice that this schema defines a nested field, called a \"REPEATED\" field in BigQuery. Fields can be nested up to 15 layers deep. The schema above includes the \"product_tags\" field, a field of \"REPEATED\" \"RECORDS\" which is the equivalent of an array or objects in JSON. BigQuery also supports structures like an array of integers, or an array of dates, but it does not support an array of arrays. Now that we have our schema file, let's load our data. > bq load \\ --source_format=NEWLINE_DELIMITED_JSON \\ --replace \\ --time_partitioning_type=DAY \\ --time_partitioning_field created_on \\ > my_dataset.products \\ > products.jsonl \\ > products_schema.json Upload complete. Waiting on bqjob_r3f8263b30008fbdf_00000175442a455c_1 ... (1s) Current status: DONE > bq head --table my_dataset.products +------------+--------------+---------------------------------------------------------------------------+------------+ | product_id | product_name | product_tags | created_on | +------------+--------------+---------------------------------------------------------------------------+------------+ | 1 | hammer | [{\"tag_id\":\"1\",\"tag_name\":\"sale\"},{\"tag_id\":\"2\",\"tag_name\":\"tool\"}] | 2020-10-19 | | 2 | pen | [{\"tag_id\":\"1\",\"tag_name\":\"sale\"},{\"tag_id\":\"3\",\"tag_name\":\"stationary\"}] | 2020-10-19 | | 3 | drill | [{\"tag_id\":\"2\",\"tag_name\":\"tool\"}] | 2020-10-19 | +------------+--------------+---------------------------------------------------------------------------+------------+ I specified that the table is partitioned on the created_on field, so right now this. This means that each distinct date in that field will be treated as a distinct partition, improving performance when filtering on that field. Partitions are always optional, but are useful when you know you will be filtering on a particular field (e.g. querying for all products created in the last seven days). If I included --time_partitioning_type=DAY but did not provide a field to partition on, BigQuery would have automatically assigned a partition date of the date the record was ingested into BigQuery. We can filter on this automatically generated partition date by using BigQuery's pseudo-columns : _PARTITIONDATE , and _PARTITIONTIME .","title":"Loading Data from the Command Line"},{"location":"de-gcp-book/ch_04_data_warehouse/#using-the-googlecloudbigquery-python-library","text":"We can also interact with BigQuery in Python. In Chapter 2 we used the google.cloud.storage library, and now for BigQuery we'll need to install the google.cloud.bigquery library: > pip install --upgrade google-cloud-bigquery","title":"Using the google.cloud.bigquery Python Library"},{"location":"de-gcp-book/ch_04_data_warehouse/#creating-tables-from-python","text":"from google.cloud import bigquery def create_dataset(dataset_name: str, project_name: str) -> None: dataset_id = f'{project_name}.{dataset_name}' dataset_obj = bigquery.Dataset(dataset_id) client = bigquery.Client() dataset = client.create_dataset(dataset_obj) def create_table(table_name: str, dataset_name: str, project_name: str, schema: list) -> None: table_id = f'{project_name}.{dataset_name}.{table_name}' table_obj = bigquery.Table(table_id, schema) client = bigquery.Client() table = client.create_table(table_obj) project_name = 'de-book-dev' dataset_name = 'my_other_dataset' table_name = 'user_purchases' schema = [ bigquery.SchemaField('user_name', 'STRING', mode='REQUIRED'), bigquery.SchemaField('items_purchased', 'INT64', mode='REQUIRED'), bigquery.SchemaField('dollars_spent', 'FLOAT64', mode='REQUIRED') ] create_dataset(dataset_name, project_name) create_table(table_name, dataset_name, project_name, schema) We can also create a table from a query: from google.cloud import bigquery def create_table_from_query(table_name: str, dataset_name: str, project_name: str, raw_sql: str) -> None: table_id = f'{project_name}.{dataset_name}.{table_name}' job_config = bigquery.QueryJobConfig(destination=table_id) client = bigquery.Client() query_job = client.query(raw_sql, job_config=job_config) query_job.result() project_name = 'de-book-dev' dataset_name = 'my_other_dataset' table_name = 'user_purchases_copy' raw_sql = 'select user_name, items_purchased, dollars_spent from my_other_dataset.user_purchases' create_table_from_query(table_name, dataset_name, project_name, raw_sql) You can find the documentation for the Python API here .","title":"Creating Tables from Python"},{"location":"de-gcp-book/ch_04_data_warehouse/#loading-data-from-python","text":"Now let's load our data using the \"products.json\" file we created above: import json from google.cloud import bigquery def load_json_data(table_id: str, source_file_location: str, schema: list) -> None: client = bigquery.Client() job_config = bigquery.LoadJobConfig( schema=schema, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, ) with open(source_file_location, 'r') as f: data = json.load(f) load_job = client.load_table_from_json( data, table_id, job_config=job_config, ) load_job.result() table_id = 'de-book-dev.my_other_dataset.products' source_file_location = 'products.json' schema = [ bigquery.SchemaField('product_id', 'INT64', mode='REQUIRED'), bigquery.SchemaField('product_name', 'STRING', mode='NULLABLE'), bigquery.SchemaField('product_tags' , 'RECORD' , mode='REPEATED' , fields=[ bigquery.SchemaField('tag_id', 'INT64', mode='REQUIRED'), bigquery.SchemaField('tag_name', 'STRING', mode='NULLABLE'), ]), bigquery.SchemaField('created_on', 'DATE', mode='REQUIRED') ] load_json_data(table_id, source_file_location, schema) Just like above, we're loading data with a nested field. Unlike the bq load command, if you wish to use the BigQuery's Python API to load data into a partitioned table you must create the table with the specified partition first, then load the data.","title":"Loading Data from Python"},{"location":"de-gcp-book/ch_04_data_warehouse/#cleaning-up","text":"Because BigQuery bills based on data processed and storage quantity, rather than uptime, leaving our test data in there from this chapter will incur almost no costs. Nonetheless, it's good to clean up so we can have a blank start when we start creating and loading tables as part of our DAGs in Chapter 5 . Let's start by listing all our datasets: > bq ls -d my_dataset my_other_dataset Now let's delete them. We use the -r flag to indicate we also want the associated tables deleted: > bq rm -d -r my_dataset > bq rm -d -r my_other_dataset Next Chapter: Chapter 5: Setting up DAGs in Composer and Airflow","title":"Cleaning Up"},{"location":"de-gcp-book/ch_05_dags/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 5 : Setting up DAGs in Composer and Airflow In previous chapters we focused on specific technologies: Composer, GCS, and BigQuery. In this chapter we'll be pulling together everything we've learned to build a realistic data pipeline. We're going to be setting up a Data Warehouse to allow our users to analyze cryptocurrency data. We'll be building a DAG to pull data into our Data Lake from a web API. We'll then be moving our data into BigQuery. Finally, we'll be running transformations on our data so that it is easy for our users wrangle. Initializing the GCP Infrastructure For this chapter we are going to need a Composer Environment, a GCS Bucket, and a couple BigQuery Datasets. In Chapter 11: Deployment Pipelines with Cloud Build I'll show how to manage each of these pieces of infrastructure in your code using Terraform. But for now, we're going to set them up using the command line tools. First, we need to make sure GCP uses our service account: > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" The Composer Environment can take a good while to spin up after we issue the command, so we'll start that first: > gcloud composer environments create bitcoin-dev \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com For more details on setting up a Composer Environment check out Chapter 2 . Now let's create our GCS Buckets: > gsutil mb gs://de-book-bitcoin-web-data Finally, let's create our BigQuery Datasets: > bq mk --dataset bitcoin_landing > bq mk --dataset bitcoin_mart The \"bitcoin_landing\" Dataset will be where we will load our data from GCS. The \"bitcoin_mart\" Dataset with be where we expose our transformed data to our users. While we're waiting for our Composer Environment to finish building we can start working on our DAGs. CoinGecko Data Pipeline CoinGecko provides numerous free web API endpoints providing data on cryptocurrency. We are going to be getting data from their /coins/markets endpoint to get information about the top 100 currencies they track. The response is in standard JSON format, so we'll convert that to JSON Lines format, then save it in GCS. Finally, we'll load the data into BigQuery and remove duplicate records. You've already seen how to do almost every part of this pipeline in Chapters 1-3, this will just be bringing it all together. Creating the Tasks As I described in Chapter 2 , Airflow manages a series of Tasks that do the work in your pipeline. Here I've chosen to break this pipeline into five Tasks: 1. Download and save the source data. 2. Copy the file to GCS. 3. Load the data into BigQuery. 4. Create the table to be exposed to users. 5. Load the data into the final table. There are no strict rules on how to break up the work. One of the main benefits of using Airflow is the ability to break a pipeline down into discrete Tasks, so it is generally a good idea to keep the Tasks small. Task 1: Download and Save the Source Data In this Task we will make a GET request to the web API, then save the returned JSON data as a JSON Lines file. We're using the PythonVirtualenvOperator, which allows us to execute a Python function from within a virtual environment. We need the virtual environment to import the pandas and requests libraries. We're using requests to make the HTTP request, and we're using pandas to convert the data to a JSON Lines file. In this step we're saving the file locally, which Composer has mapped to a GCS Bucket. filename = 'coin_gecko.json' filepath = f'/home/airflow/gcs/data/{filename}' def download_currency_data(filename: str) -> None: import pandas as pd # importing within the function is required for PythonVirtualenvOperator import requests url = 'https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100&page=1&sparkline=false' result = requests.get(url) result.raise_for_status() # raise an exception if bad response returned data = result.json() df = pd.DataFrame(data) df.to_json(filename, orient='records', lines=True) # saving as JSON Lines file t_download_currency_data = PythonVirtualenvOperator( Task_id=\"download_currency_data\", python_version=\"3\", requirements=[\"requests==2.7.0\", \"pandas==1.1.3\"], python_callable=download_currency_data, op_kwargs={\"filename\": filepath}, dag=dag, ) Task 2: Copy the File to GCS Now that we have the file saved \"locally\", we need to move it to the designated GCS folder that will act as our Data Lake. We are putting the file inside the \"de-book-bitcoin-web-data\" Bucket we created above. We're using the strange syntax {{{{ ds_nodash }}}} in the path name. This refers to an Airflow macro , which is basically string interpolation within the Airflow DAG context. In this case, it is being used to provide the execution date of the DAG in the format YYYMMDD. While using Python's datetime.date.today() can be useful in some circumstances, it can also be problematic as it is evaluated at the time each Task is run. So if there is a date change between one Task and the next your value for today's date will change. Using the DAG execution date can be tricky, too, as the execution date for a manually run DAG is surprisingly different than when the DAG is executed on a schedule. All that is to say: if you need to dynamically generate a date for a Task, be thoughtful and careful. t_save_file_to_gcs = BashOperator( task_id=\"save_file_to_gcs\", bash_command=f\"gsutil cp {filepath} gs://de-book-bitcoin-web-data/coingecko/dt={{{{ ds_nodash }}}}/coin_gecko_{time.time()}.json\", dag=dag ) t_save_file_to_gcs.set_upstream(t_download_currency_data) We're using the set_upstream() method after each Task to define Task dependencies. Task 3: Load the Data into BigQuery As I mentioned in Chapter 4 , the bq load command can be used to create a table if one does not already exist. So we do not need to create a separate Task for creating the table. However, we'll still need to specify a schema: [ { \"name\": \"id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\" }, { \"name\": \"symbol\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"image\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"current_price\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap\", \"type\": \"INT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap_rank\", \"type\": \"INT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"fully_diluted_valuation\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"total_volume\", \"type\": \"INT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"high_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"low_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"price_change_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"price_change_percentage_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap_change_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap_change_percentage_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"circulating_supply\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"total_supply\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"max_supply\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"ath\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"ath_change_percentage\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"ath_date\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\" }, { \"name\": \"atl\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"atl_change_percentage\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"atl_date\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\" }, { \"name\": \"roi\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [ { \"name\": \"times\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"currency\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"percentage\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" } ] }, { \"name\": \"last_updated\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\" } ] We'll set up our table to be partitioned by the last_updated field, so that it will be easy to filter our data by the date it was updated. The web API is supposed to update once per day, and our DAG runs once per day, so in theory we shouldn't be getting any duplicates. In practice, we need to plan for the possibility that we have duplicate data getting into our bitcoin_landing.currencies table. That'll be taken care of in the final Task. Now let's load our data: t_load_data_into_bq = BashOperator( task_id=\"load_data_into_bq\", bash_command=f\"\"\" bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type=DAY \\\\ --time_partitioning_field last_updated \\\\ bitcoin_landing.currencies \\\\ \"gs://de-book-bitcoin-web-data/coingecko/dt={{{{ ds_nodash }}}}/*.json\" \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) t_load_data_into_bq.set_upstream(t_save_file_to_gcs) When designating the source location we use *.json to indicate we will be loading all JSON files in that sub-folder in this operation. While we would usually only expect a single file in there, it's possible multiple source files will be generated in a single day. We don't have to worry about loading the data multiple times, because we're already removing duplicate entries as part of our pipeline in the final Task. We can't specify the specific filename in this Task because it was dynamically generated in the previous Task using the time.time() function. If we tried to reference the file name again using the time.time() function the file name will change, and this bq load will be unable to find the source file. Task 4: Create the Destination Table In Task 3 when we loaded the data into BigQuery we didn't create the table first because the bq load operation took care of table creation for us. For the bitcoin_mart.currencies table we are creating the table first, because the final Task to remove duplicate entries will require SQL code that references this table. If the table doesn't exist first then the query will fail, and so will the Task. While this is only an issue on the first time the fifth Task is run, having the table creation Task in the DAG does no harm, even if it's not needed on subsequent runs. The --force flag is used to ignore errors if the table already exists (i causes BigQuery to ignore the bq mk command). Note that we're not using the set_upstream() method for this Task. This Task can be run independently of the other Tasks, so it does not depend on any other Tasks being complete first. t_create_currencies_table = BashOperator( task_id=\"create_currencies_table\", bash_command=\"\"\" bq mk \\\\ --force \\\\ bitcoin_mart.currencies \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) Task 5: Load Data into the Final Table The currencies table that we are exposing to our users shouldn't have duplicate records in it. We can remove the duplicates with a simple SQL query: with numbered as ( select *, row_number() over(partition by id order by last_updated desc) as rn from bitcoin_landing.currencies ) select * except(rn) from numbered where rn=1; Now we just need to create our final table based on that query: t_update_currencies_table = BashOperator( task_id=\"update_currencies_table\", bash_command=\"\"\" cat /home/airflow/gcs/dags/sql/bitcoin_mart/currencies.sql | bq --headless query \\\\ --destination_table=bitcoin_mart.currencies \\\\ --use_legacy_sql=false \\\\ --replace \"\"\", dag=dag ) t_update_currencies_table.set_upstream([t_create_currencies_table, t_load_data_into_bq]) Note that we have set both the third and fourth Tasks as dependencies for this final Task. Organizing the DAG Now that we've got our Tasks straightened out, the last piece of code is the instantiation of the DAG: default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'coin_gecko', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, # only let 1 instance run at a time catchup=False, # if a scheduled run is missed, skip it default_args=default_args ) The schedule_interval=\"0 * * * *\" refers to how often it should run, using cron syntax . Also note that if a Task fails in this DAG, Airflow will re-run the Task three times before marking the Task as failed. Now we have all the pieces. This is what our entire DAG file looks like: import time import datetime from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonVirtualenvOperator default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'coin_gecko', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) filename = 'coin_gecko.json' filepath = f'/home/airflow/gcs/data/{filename}' def download_currency_data(filename: str) -> None: import pandas as pd # importing within the function is required for PythonVirtualenvOperator import requests url = 'https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100&page=1&sparkline=false' result = requests.get(url) result.raise_for_status() # raise an exception if bad response returned data = result.json() df = pd.DataFrame(data) df.to_json(filename, orient='records', lines=True) # saving as JSON lines file t_download_currency_data = PythonVirtualenvOperator( task_id=\"download_currency_data\", python_version=\"3\", requirements=[\"requests==2.7.0\", \"pandas==1.1.3\"], python_callable=download_currency_data, op_kwargs={\"filename\": filepath}, dag=dag ) t_save_file_to_gcs = BashOperator( task_id=\"save_file_to_gcs\", bash_command=f\"gsutil cp {filepath} gs://de-book-bitcoin-web-data/coingecko/dt={{{{ ds_nodash }}}}/coin_gecko_{time.time()}.json\", dag=dag ) t_save_file_to_gcs.set_upstream(t_download_currency_data) t_load_data_into_bq = BashOperator( task_id=\"load_data_into_bq\", bash_command=\"\"\" bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type=DAY \\\\ --time_partitioning_field last_updated \\\\ bitcoin_landing.currencies \\\\ \"gs://de-book-bitcoin-web-data/coingecko/dt={{ ds_nodash }}/*.json\" \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) t_load_data_into_bq.set_upstream(t_save_file_to_gcs) t_create_currencies_table = BashOperator( task_id=\"create_currencies_table\", bash_command=\"\"\" bq mk \\\\ --force \\\\ bitcoin_mart.currencies \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) t_update_currencies_table = BashOperator( task_id=\"update_currencies_table\", bash_command=\"\"\" cat /home/airflow/gcs/dags/sql/bitcoin_mart/currencies.sql | bq --headless query \\\\ --destination_table=bitcoin_mart.currencies \\\\ --use_legacy_sql=false \\\\ --replace \"\"\", dag=dag ) t_update_currencies_table.set_upstream([t_create_currencies_table, t_load_data_into_bq]) Deploying and Running the DAG So now we have our DAG file ( coin_gecko.py ), our schema file ( coin_gecko_schema.json ) and our sql file ( currencies.sql ). Our next step is to get them deployed to Composer. We'll talk about deployment pipelines in Chapter 12 , but for now we can deploy through the command line: > gcloud composer environments storage dags import \\ --environment bitcoin-dev \\ --location us-central1 \\ --source coin_gecko.py To deploy our JSON and SQL files we'll need to identify the name of the Bucket. Once we know that we can copy our files over: > gcloud composer environments describe bitcoin-dev \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-bitcoin-dev-123abc-bucket/dags > gsutil cp coin_gecko_schema.json gs://us-central1-bitcoin-dev-3d3132eb-bucket/dags/schemas/ > gsutil cp currencies.sql gs://us-central1-bitcoin-dev-3d3132eb-bucket/dags/sql/bitcoin_mart/ We can see our Composer Environment running on the GCP Console . Click the \"Airflow\" link to launch the webserver: We can now see our \"coin_gecko\" DAG. It's scheduled to run at midnight UTC, but should have run once automatically after it was deployed. We can also force it to run by clicking the \"Trigger Dag\" button. By clicking on the DAG name we can see a history of the status of the various Tasks. Clicking on the \"Graph View\" allows us to see the most recent DAG run: We can see that our Tasks completed, so let's take a look at BigQuery in the GCP Console to verify our tables are there. We can see and click on our Datasets and tables on the left to get details about them. When a table is selected you can select the \"Preview\" button to see the data. Cleaning Up In this chapter we created a Composer Environment and it's associated Buckets, two BigQuery datasets, and a GCS Bucket for storing our source data. I've explained how to take these down in previous chapters, but here it is again all together: Deleting the composer instance: > gcloud composer environments delete bitcoin-dev --location us-central1 Identifying and deleting the GCS Buckets: > gsutil list gs://de-book-bitcoin-web-data/ gs://us-central1-bitcoin-dev-3d3132eb-bucket/ > gsutil rm -r gs://de-book-bitcoin-web-data/ > gsutil rm -r gs://us-central1-bitcoin-dev-3d3132eb-bucket/ Finally, let's get rid of our datasets: > bq rm -d -r bitcoin_landing > bq rm -d -r bitcoin_mart Next Chapter: Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions","title":"Chapter 5: Setting up DAGs in Composer and Airflow"},{"location":"de-gcp-book/ch_05_dags/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_05_dags/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_05_dags/#chapter-5-setting-up-dags-in-composer-and-airflow","text":"In previous chapters we focused on specific technologies: Composer, GCS, and BigQuery. In this chapter we'll be pulling together everything we've learned to build a realistic data pipeline. We're going to be setting up a Data Warehouse to allow our users to analyze cryptocurrency data. We'll be building a DAG to pull data into our Data Lake from a web API. We'll then be moving our data into BigQuery. Finally, we'll be running transformations on our data so that it is easy for our users wrangle.","title":"Chapter 5: Setting up DAGs in Composer and Airflow"},{"location":"de-gcp-book/ch_05_dags/#initializing-the-gcp-infrastructure","text":"For this chapter we are going to need a Composer Environment, a GCS Bucket, and a couple BigQuery Datasets. In Chapter 11: Deployment Pipelines with Cloud Build I'll show how to manage each of these pieces of infrastructure in your code using Terraform. But for now, we're going to set them up using the command line tools. First, we need to make sure GCP uses our service account: > export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keys/de-book-dev.json\" The Composer Environment can take a good while to spin up after we issue the command, so we'll start that first: > gcloud composer environments create bitcoin-dev \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com For more details on setting up a Composer Environment check out Chapter 2 . Now let's create our GCS Buckets: > gsutil mb gs://de-book-bitcoin-web-data Finally, let's create our BigQuery Datasets: > bq mk --dataset bitcoin_landing > bq mk --dataset bitcoin_mart The \"bitcoin_landing\" Dataset will be where we will load our data from GCS. The \"bitcoin_mart\" Dataset with be where we expose our transformed data to our users. While we're waiting for our Composer Environment to finish building we can start working on our DAGs.","title":"Initializing the GCP Infrastructure"},{"location":"de-gcp-book/ch_05_dags/#coingecko-data-pipeline","text":"CoinGecko provides numerous free web API endpoints providing data on cryptocurrency. We are going to be getting data from their /coins/markets endpoint to get information about the top 100 currencies they track. The response is in standard JSON format, so we'll convert that to JSON Lines format, then save it in GCS. Finally, we'll load the data into BigQuery and remove duplicate records. You've already seen how to do almost every part of this pipeline in Chapters 1-3, this will just be bringing it all together.","title":"CoinGecko Data Pipeline"},{"location":"de-gcp-book/ch_05_dags/#creating-the-tasks","text":"As I described in Chapter 2 , Airflow manages a series of Tasks that do the work in your pipeline. Here I've chosen to break this pipeline into five Tasks: 1. Download and save the source data. 2. Copy the file to GCS. 3. Load the data into BigQuery. 4. Create the table to be exposed to users. 5. Load the data into the final table. There are no strict rules on how to break up the work. One of the main benefits of using Airflow is the ability to break a pipeline down into discrete Tasks, so it is generally a good idea to keep the Tasks small.","title":"Creating the Tasks"},{"location":"de-gcp-book/ch_05_dags/#task-1-download-and-save-the-source-data","text":"In this Task we will make a GET request to the web API, then save the returned JSON data as a JSON Lines file. We're using the PythonVirtualenvOperator, which allows us to execute a Python function from within a virtual environment. We need the virtual environment to import the pandas and requests libraries. We're using requests to make the HTTP request, and we're using pandas to convert the data to a JSON Lines file. In this step we're saving the file locally, which Composer has mapped to a GCS Bucket. filename = 'coin_gecko.json' filepath = f'/home/airflow/gcs/data/{filename}' def download_currency_data(filename: str) -> None: import pandas as pd # importing within the function is required for PythonVirtualenvOperator import requests url = 'https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100&page=1&sparkline=false' result = requests.get(url) result.raise_for_status() # raise an exception if bad response returned data = result.json() df = pd.DataFrame(data) df.to_json(filename, orient='records', lines=True) # saving as JSON Lines file t_download_currency_data = PythonVirtualenvOperator( Task_id=\"download_currency_data\", python_version=\"3\", requirements=[\"requests==2.7.0\", \"pandas==1.1.3\"], python_callable=download_currency_data, op_kwargs={\"filename\": filepath}, dag=dag, )","title":"Task 1: Download and Save the Source Data"},{"location":"de-gcp-book/ch_05_dags/#task-2-copy-the-file-to-gcs","text":"Now that we have the file saved \"locally\", we need to move it to the designated GCS folder that will act as our Data Lake. We are putting the file inside the \"de-book-bitcoin-web-data\" Bucket we created above. We're using the strange syntax {{{{ ds_nodash }}}} in the path name. This refers to an Airflow macro , which is basically string interpolation within the Airflow DAG context. In this case, it is being used to provide the execution date of the DAG in the format YYYMMDD. While using Python's datetime.date.today() can be useful in some circumstances, it can also be problematic as it is evaluated at the time each Task is run. So if there is a date change between one Task and the next your value for today's date will change. Using the DAG execution date can be tricky, too, as the execution date for a manually run DAG is surprisingly different than when the DAG is executed on a schedule. All that is to say: if you need to dynamically generate a date for a Task, be thoughtful and careful. t_save_file_to_gcs = BashOperator( task_id=\"save_file_to_gcs\", bash_command=f\"gsutil cp {filepath} gs://de-book-bitcoin-web-data/coingecko/dt={{{{ ds_nodash }}}}/coin_gecko_{time.time()}.json\", dag=dag ) t_save_file_to_gcs.set_upstream(t_download_currency_data) We're using the set_upstream() method after each Task to define Task dependencies.","title":"Task 2: Copy the File to GCS"},{"location":"de-gcp-book/ch_05_dags/#task-3-load-the-data-into-bigquery","text":"As I mentioned in Chapter 4 , the bq load command can be used to create a table if one does not already exist. So we do not need to create a separate Task for creating the table. However, we'll still need to specify a schema: [ { \"name\": \"id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\" }, { \"name\": \"symbol\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"image\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"current_price\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap\", \"type\": \"INT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap_rank\", \"type\": \"INT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"fully_diluted_valuation\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"total_volume\", \"type\": \"INT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"high_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"low_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"price_change_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"price_change_percentage_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap_change_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"market_cap_change_percentage_24h\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"circulating_supply\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"total_supply\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"max_supply\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"ath\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"ath_change_percentage\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"ath_date\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\" }, { \"name\": \"atl\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"atl_change_percentage\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"atl_date\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\" }, { \"name\": \"roi\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [ { \"name\": \"times\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" }, { \"name\": \"currency\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }, { \"name\": \"percentage\", \"type\": \"FLOAT64\", \"mode\": \"NULLABLE\" } ] }, { \"name\": \"last_updated\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\" } ] We'll set up our table to be partitioned by the last_updated field, so that it will be easy to filter our data by the date it was updated. The web API is supposed to update once per day, and our DAG runs once per day, so in theory we shouldn't be getting any duplicates. In practice, we need to plan for the possibility that we have duplicate data getting into our bitcoin_landing.currencies table. That'll be taken care of in the final Task. Now let's load our data: t_load_data_into_bq = BashOperator( task_id=\"load_data_into_bq\", bash_command=f\"\"\" bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type=DAY \\\\ --time_partitioning_field last_updated \\\\ bitcoin_landing.currencies \\\\ \"gs://de-book-bitcoin-web-data/coingecko/dt={{{{ ds_nodash }}}}/*.json\" \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) t_load_data_into_bq.set_upstream(t_save_file_to_gcs) When designating the source location we use *.json to indicate we will be loading all JSON files in that sub-folder in this operation. While we would usually only expect a single file in there, it's possible multiple source files will be generated in a single day. We don't have to worry about loading the data multiple times, because we're already removing duplicate entries as part of our pipeline in the final Task. We can't specify the specific filename in this Task because it was dynamically generated in the previous Task using the time.time() function. If we tried to reference the file name again using the time.time() function the file name will change, and this bq load will be unable to find the source file.","title":"Task 3: Load the Data into BigQuery"},{"location":"de-gcp-book/ch_05_dags/#task-4-create-the-destination-table","text":"In Task 3 when we loaded the data into BigQuery we didn't create the table first because the bq load operation took care of table creation for us. For the bitcoin_mart.currencies table we are creating the table first, because the final Task to remove duplicate entries will require SQL code that references this table. If the table doesn't exist first then the query will fail, and so will the Task. While this is only an issue on the first time the fifth Task is run, having the table creation Task in the DAG does no harm, even if it's not needed on subsequent runs. The --force flag is used to ignore errors if the table already exists (i causes BigQuery to ignore the bq mk command). Note that we're not using the set_upstream() method for this Task. This Task can be run independently of the other Tasks, so it does not depend on any other Tasks being complete first. t_create_currencies_table = BashOperator( task_id=\"create_currencies_table\", bash_command=\"\"\" bq mk \\\\ --force \\\\ bitcoin_mart.currencies \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag )","title":"Task 4: Create the Destination Table"},{"location":"de-gcp-book/ch_05_dags/#task-5-load-data-into-the-final-table","text":"The currencies table that we are exposing to our users shouldn't have duplicate records in it. We can remove the duplicates with a simple SQL query: with numbered as ( select *, row_number() over(partition by id order by last_updated desc) as rn from bitcoin_landing.currencies ) select * except(rn) from numbered where rn=1; Now we just need to create our final table based on that query: t_update_currencies_table = BashOperator( task_id=\"update_currencies_table\", bash_command=\"\"\" cat /home/airflow/gcs/dags/sql/bitcoin_mart/currencies.sql | bq --headless query \\\\ --destination_table=bitcoin_mart.currencies \\\\ --use_legacy_sql=false \\\\ --replace \"\"\", dag=dag ) t_update_currencies_table.set_upstream([t_create_currencies_table, t_load_data_into_bq]) Note that we have set both the third and fourth Tasks as dependencies for this final Task.","title":"Task 5: Load Data into the Final Table"},{"location":"de-gcp-book/ch_05_dags/#organizing-the-dag","text":"Now that we've got our Tasks straightened out, the last piece of code is the instantiation of the DAG: default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'coin_gecko', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, # only let 1 instance run at a time catchup=False, # if a scheduled run is missed, skip it default_args=default_args ) The schedule_interval=\"0 * * * *\" refers to how often it should run, using cron syntax . Also note that if a Task fails in this DAG, Airflow will re-run the Task three times before marking the Task as failed. Now we have all the pieces. This is what our entire DAG file looks like: import time import datetime from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonVirtualenvOperator default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'coin_gecko', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) filename = 'coin_gecko.json' filepath = f'/home/airflow/gcs/data/{filename}' def download_currency_data(filename: str) -> None: import pandas as pd # importing within the function is required for PythonVirtualenvOperator import requests url = 'https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100&page=1&sparkline=false' result = requests.get(url) result.raise_for_status() # raise an exception if bad response returned data = result.json() df = pd.DataFrame(data) df.to_json(filename, orient='records', lines=True) # saving as JSON lines file t_download_currency_data = PythonVirtualenvOperator( task_id=\"download_currency_data\", python_version=\"3\", requirements=[\"requests==2.7.0\", \"pandas==1.1.3\"], python_callable=download_currency_data, op_kwargs={\"filename\": filepath}, dag=dag ) t_save_file_to_gcs = BashOperator( task_id=\"save_file_to_gcs\", bash_command=f\"gsutil cp {filepath} gs://de-book-bitcoin-web-data/coingecko/dt={{{{ ds_nodash }}}}/coin_gecko_{time.time()}.json\", dag=dag ) t_save_file_to_gcs.set_upstream(t_download_currency_data) t_load_data_into_bq = BashOperator( task_id=\"load_data_into_bq\", bash_command=\"\"\" bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type=DAY \\\\ --time_partitioning_field last_updated \\\\ bitcoin_landing.currencies \\\\ \"gs://de-book-bitcoin-web-data/coingecko/dt={{ ds_nodash }}/*.json\" \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) t_load_data_into_bq.set_upstream(t_save_file_to_gcs) t_create_currencies_table = BashOperator( task_id=\"create_currencies_table\", bash_command=\"\"\" bq mk \\\\ --force \\\\ bitcoin_mart.currencies \\\\ /home/airflow/gcs/dags/schemas/coin_gecko_schema.json \"\"\", dag=dag ) t_update_currencies_table = BashOperator( task_id=\"update_currencies_table\", bash_command=\"\"\" cat /home/airflow/gcs/dags/sql/bitcoin_mart/currencies.sql | bq --headless query \\\\ --destination_table=bitcoin_mart.currencies \\\\ --use_legacy_sql=false \\\\ --replace \"\"\", dag=dag ) t_update_currencies_table.set_upstream([t_create_currencies_table, t_load_data_into_bq])","title":"Organizing the DAG"},{"location":"de-gcp-book/ch_05_dags/#deploying-and-running-the-dag","text":"So now we have our DAG file ( coin_gecko.py ), our schema file ( coin_gecko_schema.json ) and our sql file ( currencies.sql ). Our next step is to get them deployed to Composer. We'll talk about deployment pipelines in Chapter 12 , but for now we can deploy through the command line: > gcloud composer environments storage dags import \\ --environment bitcoin-dev \\ --location us-central1 \\ --source coin_gecko.py To deploy our JSON and SQL files we'll need to identify the name of the Bucket. Once we know that we can copy our files over: > gcloud composer environments describe bitcoin-dev \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-bitcoin-dev-123abc-bucket/dags > gsutil cp coin_gecko_schema.json gs://us-central1-bitcoin-dev-3d3132eb-bucket/dags/schemas/ > gsutil cp currencies.sql gs://us-central1-bitcoin-dev-3d3132eb-bucket/dags/sql/bitcoin_mart/ We can see our Composer Environment running on the GCP Console . Click the \"Airflow\" link to launch the webserver: We can now see our \"coin_gecko\" DAG. It's scheduled to run at midnight UTC, but should have run once automatically after it was deployed. We can also force it to run by clicking the \"Trigger Dag\" button. By clicking on the DAG name we can see a history of the status of the various Tasks. Clicking on the \"Graph View\" allows us to see the most recent DAG run: We can see that our Tasks completed, so let's take a look at BigQuery in the GCP Console to verify our tables are there. We can see and click on our Datasets and tables on the left to get details about them. When a table is selected you can select the \"Preview\" button to see the data.","title":"Deploying and Running the DAG"},{"location":"de-gcp-book/ch_05_dags/#cleaning-up","text":"In this chapter we created a Composer Environment and it's associated Buckets, two BigQuery datasets, and a GCS Bucket for storing our source data. I've explained how to take these down in previous chapters, but here it is again all together: Deleting the composer instance: > gcloud composer environments delete bitcoin-dev --location us-central1 Identifying and deleting the GCS Buckets: > gsutil list gs://de-book-bitcoin-web-data/ gs://us-central1-bitcoin-dev-3d3132eb-bucket/ > gsutil rm -r gs://de-book-bitcoin-web-data/ > gsutil rm -r gs://us-central1-bitcoin-dev-3d3132eb-bucket/ Finally, let's get rid of our datasets: > bq rm -d -r bitcoin_landing > bq rm -d -r bitcoin_mart Next Chapter: Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions","title":"Cleaning Up"},{"location":"de-gcp-book/ch_06_event_triggers/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 6 : Setting up Event-Triggered Pipelines with Cloud Functions In Chapter 5 I demonstrated how to set up a Data Pipeline that ran on a schedule (or by manually triggering through the Web UI). Airflow has a lot of flexibility for when and how often a DAG should run, but sometimes you don't want your DAG to run on a schedule. Suppose your organization's Data Science team periodically generates a CSV file with valuable information, and you want to ingest that file into your Data Warehouse as soon as it is available. You can't ingest on a schedule, because you don't know when the file will be uploaded. What we can do instead is set up a Cloud Function to listen for a new file to be uploaded, and once a new file is detected it can kick off the Data Pipeline for ingesting that file. Overview of Cloud Functions GCP's Cloud Functions is a \"serverless\" code execution service. It allows predefined code to be executed when triggered by an event, such as GCS events, HTTP events, and Pub/Sub events. We'll focus on GCS events in this chapter, as responding to a new file being uploaded to a GCS Bucket is a common task for Data Engineers, but Cloud Functions has much more functionality than what I'll cover. Additionally, I'll talk about Pub/Sub in Chapter 8: Streaming Data with Pub/Sub as I discuss streaming Data Pipelines. Setting Up an Event-Triggered Pipeline In this chapter we'll be setting up a process where a file is uploaded to a GCS Bucket, which triggers a function that loads that file into a BigQuery table. Prep Work We need to do a few things to make sure our Cloud Function works, once we set it up. Let's start by creating a Bucket that will trigger the Cloud Function: > gsutil mb gs://de-book-trigger-function We need somewhere for the data to go, so let's create a BigQuery Dataset and table: > bq mk --dataset food > bq mk --table food.food_ranks food_name:STRING,ranking:INT64 Now let's enable the Cloud Function API on GCP. In Chapter 2 I showed how to enable the Composer API through the console, but we can also enable services through the command line: > gcloud services enable cloudfunctions.googleapis.com Building and Deploying a Cloud Function Now let's create a function that will be executed once a new file is loaded into the GCS Bucket. It's important to note that the file must be called main.py . To differentiate this function file from others GCP recommends you use subdirectories such as /functions/my_function_name/main.py . def load_to_bq(event, context): import os from google.cloud import bigquery table_id = 'de-book-dev.food.food_ranks' uri = os.path.join('gs://', event['bucket'], event['name']) schema = [ bigquery.SchemaField('food_name', 'STRING', mode='NULLABLE'), bigquery.SchemaField('ranking', 'INT64', mode='NULLABLE') ] job_config = bigquery.LoadJobConfig( schema=schema, source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1 ) client = bigquery.Client() load_job = client.load_table_from_uri( uri, table_id, job_config=job_config ) load_job.result() In the above function we used the library google.cloud.bigquery , which means we'll need to install it into our environment. Fortunately, we can simply create a requirements.txt file in the same directory as our main.py file: google-cloud-bigquery==2.2.0 Now let's deploy the function. When we run the below command we should be in the same directory as the main.py file. It may take a few minutes for the function to deploy. > gcloud functions deploy load_to_bq \\ --runtime python37 \\ --trigger-resource gs://de-book-trigger-function \\ --trigger-event google.storage.object.finalize Testing the Cloud Function We've got our Cloud Function up and running, so let's test it out. First let's create a file to upload that will trigger the function: > echo 'food_name, ranking pizza, 1 cheeseburgers, 2 hot dogs, 3 nachos, 4 tacos, 5' > yum.csv Now we can upload the file to our Bucket and trigger our function: > gsutil cp yum.csv gs://de-book-trigger-function If all went as planned we should now see our data in our table: > bq query 'select * from food.food_ranks' +---------------+---------+ | food_name | ranking | +---------------+---------+ | pizza | 1 | | cheeseburgers | 2 | | hot dogs | 3 | | nachos | 4 | | tacos | 5 | +---------------+---------+ Combining Cloud Functions with Composer One architecture design that can work well is having all of your batch pipelines managed by Composer. In the above Cloud Function we loaded the data directly into BigQuery using the google.cloud.bigquery Python library. However, another option would be to have the Cloud Function trigger a DAG, which will then perform the loading and transformation Tasks. You can read more about setting up a Cloud Function to trigger a DAG here . Cleaning Up Cloud Function pricing is done by use, so our work this chapter is not breaking the bank. Nonetheless, it's good to take down what you're not using. Note that the Cloud Functions service created some storage buckets for it to use, in addition to the Bucket we created, that should also be removed. > gcloud functions delete load_to_bq > bq rm --dataset -r food > gsutil ls gs://de-book-trigger-function/ gs://gcf-sources-204024561480-us-central1/ gs://us.artifacts.de-book-dev.appspot.com/ > gsutil rm -r gs://de-book-trigger-function > gsutil rm -r gs://us.artifacts.de-book-dev.appspot.com/ A note about gs://gcf-sources-204024561480-us-central1 : This bucket is used as the location where Cloud Functions are deployed in GCP. It takes the format \"gcf-sources- - . If you delete this bucket, you may find that future deployments of Cloud Functions fail. You can manually create a bucket with this name to resolve the issue. Next Chapter: Chapter 7: Parallel Processing with DataProc and Spark","title":"Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions"},{"location":"de-gcp-book/ch_06_event_triggers/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_06_event_triggers/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_06_event_triggers/#chapter-6-setting-up-event-triggered-pipelines-with-cloud-functions","text":"In Chapter 5 I demonstrated how to set up a Data Pipeline that ran on a schedule (or by manually triggering through the Web UI). Airflow has a lot of flexibility for when and how often a DAG should run, but sometimes you don't want your DAG to run on a schedule. Suppose your organization's Data Science team periodically generates a CSV file with valuable information, and you want to ingest that file into your Data Warehouse as soon as it is available. You can't ingest on a schedule, because you don't know when the file will be uploaded. What we can do instead is set up a Cloud Function to listen for a new file to be uploaded, and once a new file is detected it can kick off the Data Pipeline for ingesting that file.","title":"Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions"},{"location":"de-gcp-book/ch_06_event_triggers/#overview-of-cloud-functions","text":"GCP's Cloud Functions is a \"serverless\" code execution service. It allows predefined code to be executed when triggered by an event, such as GCS events, HTTP events, and Pub/Sub events. We'll focus on GCS events in this chapter, as responding to a new file being uploaded to a GCS Bucket is a common task for Data Engineers, but Cloud Functions has much more functionality than what I'll cover. Additionally, I'll talk about Pub/Sub in Chapter 8: Streaming Data with Pub/Sub as I discuss streaming Data Pipelines.","title":"Overview of Cloud Functions"},{"location":"de-gcp-book/ch_06_event_triggers/#setting-up-an-event-triggered-pipeline","text":"In this chapter we'll be setting up a process where a file is uploaded to a GCS Bucket, which triggers a function that loads that file into a BigQuery table.","title":"Setting Up an Event-Triggered Pipeline"},{"location":"de-gcp-book/ch_06_event_triggers/#prep-work","text":"We need to do a few things to make sure our Cloud Function works, once we set it up. Let's start by creating a Bucket that will trigger the Cloud Function: > gsutil mb gs://de-book-trigger-function We need somewhere for the data to go, so let's create a BigQuery Dataset and table: > bq mk --dataset food > bq mk --table food.food_ranks food_name:STRING,ranking:INT64 Now let's enable the Cloud Function API on GCP. In Chapter 2 I showed how to enable the Composer API through the console, but we can also enable services through the command line: > gcloud services enable cloudfunctions.googleapis.com","title":"Prep Work"},{"location":"de-gcp-book/ch_06_event_triggers/#building-and-deploying-a-cloud-function","text":"Now let's create a function that will be executed once a new file is loaded into the GCS Bucket. It's important to note that the file must be called main.py . To differentiate this function file from others GCP recommends you use subdirectories such as /functions/my_function_name/main.py . def load_to_bq(event, context): import os from google.cloud import bigquery table_id = 'de-book-dev.food.food_ranks' uri = os.path.join('gs://', event['bucket'], event['name']) schema = [ bigquery.SchemaField('food_name', 'STRING', mode='NULLABLE'), bigquery.SchemaField('ranking', 'INT64', mode='NULLABLE') ] job_config = bigquery.LoadJobConfig( schema=schema, source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1 ) client = bigquery.Client() load_job = client.load_table_from_uri( uri, table_id, job_config=job_config ) load_job.result() In the above function we used the library google.cloud.bigquery , which means we'll need to install it into our environment. Fortunately, we can simply create a requirements.txt file in the same directory as our main.py file: google-cloud-bigquery==2.2.0 Now let's deploy the function. When we run the below command we should be in the same directory as the main.py file. It may take a few minutes for the function to deploy. > gcloud functions deploy load_to_bq \\ --runtime python37 \\ --trigger-resource gs://de-book-trigger-function \\ --trigger-event google.storage.object.finalize","title":"Building and Deploying a Cloud Function"},{"location":"de-gcp-book/ch_06_event_triggers/#testing-the-cloud-function","text":"We've got our Cloud Function up and running, so let's test it out. First let's create a file to upload that will trigger the function: > echo 'food_name, ranking pizza, 1 cheeseburgers, 2 hot dogs, 3 nachos, 4 tacos, 5' > yum.csv Now we can upload the file to our Bucket and trigger our function: > gsutil cp yum.csv gs://de-book-trigger-function If all went as planned we should now see our data in our table: > bq query 'select * from food.food_ranks' +---------------+---------+ | food_name | ranking | +---------------+---------+ | pizza | 1 | | cheeseburgers | 2 | | hot dogs | 3 | | nachos | 4 | | tacos | 5 | +---------------+---------+","title":"Testing the Cloud Function"},{"location":"de-gcp-book/ch_06_event_triggers/#combining-cloud-functions-with-composer","text":"One architecture design that can work well is having all of your batch pipelines managed by Composer. In the above Cloud Function we loaded the data directly into BigQuery using the google.cloud.bigquery Python library. However, another option would be to have the Cloud Function trigger a DAG, which will then perform the loading and transformation Tasks. You can read more about setting up a Cloud Function to trigger a DAG here .","title":"Combining Cloud Functions with Composer"},{"location":"de-gcp-book/ch_06_event_triggers/#cleaning-up","text":"Cloud Function pricing is done by use, so our work this chapter is not breaking the bank. Nonetheless, it's good to take down what you're not using. Note that the Cloud Functions service created some storage buckets for it to use, in addition to the Bucket we created, that should also be removed. > gcloud functions delete load_to_bq > bq rm --dataset -r food > gsutil ls gs://de-book-trigger-function/ gs://gcf-sources-204024561480-us-central1/ gs://us.artifacts.de-book-dev.appspot.com/ > gsutil rm -r gs://de-book-trigger-function > gsutil rm -r gs://us.artifacts.de-book-dev.appspot.com/ A note about gs://gcf-sources-204024561480-us-central1 : This bucket is used as the location where Cloud Functions are deployed in GCP. It takes the format \"gcf-sources- - . If you delete this bucket, you may find that future deployments of Cloud Functions fail. You can manually create a bucket with this name to resolve the issue. Next Chapter: Chapter 7: Parallel Processing with DataProc and Spark","title":"Cleaning Up"},{"location":"de-gcp-book/ch_07_parallel_processing/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 7 : Parallel Processing with Dataproc and Spark Dataproc is GCP's fully managed service for running Apache Spark. Spark is an open-source program that has a wide array of capabilities, such as machine learning and data streaming, but I'm going to show you how we can use Spark perform transformations on very large data files. Spark's Python API (called PySpark), will look familiar to you if you have used Python's Pandas library. However, the key difference between processing data in Pandas vs. Spark is that Pandas works entirely in memory on a single machine, whereas Spark is designed to work across multiple machines and can manage the data in memory and on disk. Spark is operating on a Cluster of machines, and each machine is working in coordination to process a transformation job. This behavior works by default and allows Spark to perform massive parallel processing. That means we can quickly perform complex transformations on extremely large amounts of data. A Dataproc Cluster behaves similarly to a Composer Environment, in that we issue commands to create the Cluster and GCP charges us for the amount of time the Cluster is running. Like Composer, Dataproc does not automatically auto-scale (though unlike Composer, you can set an autoscaling policy as described here ). You set the number and type of machines being used when you instantiate your Cluster. In this chapter we'll look at a file that won't load into BigQuery, and we'll use PySpark to transform the file to allow us to load it. We'll need a bucket to store the file we'll be transforming, so let's make that now: > gsutil mb gs://de-book-dataproc Why do we need Dataproc? So far in this book I've been demonstrating an ELT (Extract Load Transform) approach to building a Data Pipeline, as distinguished from ETL. In an ELT approach we get our raw data into our Data Lake and Data Warehouse as quickly as possible, and then we perform our transformations. However, there are good reasons to perform transformations before loading your data into your Warehouse. Perhaps your team is more comfortable working in Python, and it's more convenient to do your processing in Python before you load in your data. Even if you are taking an ELT approach to your Pipelines, there may be instances where you will be unable to load the data into BigQuery without first doing some transformations. For example, while BigQuery can ingest nested data such as an array of integers or an array of objects (in BQ the objects are called \"Structs\"), BigQuery cannot ingest an array of arrays. Additionally, you may run into other problems with source files, such as them being malformed, that prevents BigQuery from loading the files. In Chapter 5 we did some pre-processing before we loaded our JSON data into BigQuery: we converted the JSON file from standard JSON format to Newline-Delimited JSON. We were able to do this using the compute power from our Composer Environment cluster because the amount of data we were dealing with was small. If we're dealing with large amounts of data that needs pre-processing, then we need to spin up additional resources so that we don't overwhelm our Composer Environment. Creating a Dataproc Cluster is an excellent way to bring more compute power to bear on your data before it is loaded into BigQuery. Creating a Dataproc Cluster Before we can start working with Dataproc we need to enable it on GCP: > gcloud services enable dataproc.googleapis.com There are a lot of options you can set for creating a Dataproc Cluster. Here is a typical command: > gcloud dataproc clusters create my-cluster \\ --region=us-central1 \\ --num-workers=2 \\ --worker-machine-type=n2-standard-2 \\ --image-version=1.5-debian10 You can see I've specified the number of workers and the type of machine, which controls how much processing power the Cluster has. The image name defines the environment Spark will be running in. You can see more details about the images you can use here . We could have also included the --metadata and --initialization-actions options to install Python packages into our environment: --metadata='PIP_PACKAGES=requests==2.24.0' \\ --initialization-actions=gs://de-book-dataproc/pip-install.sh Google provides the script to install the requirements in an open GCS location, but recommends you copy the file locally to ensure updates they make to the file don't break your code. We can do that here: > gsutil cp gs://goog-dataproc-initialization-actions-us-central1/python/pip-install.sh gs://de-book-dataproc We won't need to install any Python packages for our job, below. We now have a Dataproc Cluster running Spark. Let's submit a job and have our Cluster do some work. Submitting a PySpark Job Let's suppose we have a data source that provides a JSON file where one of its fields is a list of lists, which is a structure BigQuery doesn't support. We can use Dataproc to transform the data and then save the file back to GCS to be loaded into BigQuery. First, let's create our source file and save it to our bucket: > echo '{\"one_dimension\": [1,2,3], \"two_dimensions\": [[\"a\",\"b\",\"c\"],[1,2,3]]} {\"one_dimension\": [3,4,5], \"two_dimensions\": [[\"d\",\"e\",\"f\"],[3,4,5]]}' > raw_file.json > gsutil cp raw_file.json gs://de-book-dataproc Now, let's build our PySpark file that will perform our transformation: #!/usr/bin/env python from pyspark.sql import SparkSession from pyspark.sql.functions import flatten def main(): spark = SparkSession.builder.appName(\"FileCleaner\").getOrCreate() df = spark.read.json(\"gs://de-book-dataproc/raw_file.json\") df2 = df.withColumn('two_dimensions', flatten(df.two_dimensions)) df2.write.json(\"gs://de-book-dataproc/cleaned_files/\", mode='overwrite') if __name__ == \"__main__\": main() Here we're doing a simple transformation of flattening our two-dimensional array into a one-dimensional array. Spark has a lot of versatility in transforming data, with whole books being written about it. Now that we have our PySpark file let's move it up to GCS: > gsutil cp flattener.py gs://de-book-dataproc Finally, we are ready to submit our dataproc job: > gcloud dataproc jobs submit pyspark \\ gs://de-book-dataproc/flattener.py \\ --cluster=my-cluster \\ --region=us-central1 With our job complete we can see our transformed file: > gsutil ls gs://de-book-dataproc/cleaned_files gs://de-book-dataproc/cleaned_files/ gs://de-book-dataproc/cleaned_files/_SUCCESS gs://de-book-dataproc/cleaned_files/part-00000-a07721fc-76a2-4235-912c-b88df333e4d4-c000.json > gsutil cat gs://de-book-dataproc/cleaned_files/part-00000-a07721fc-76a2-4235-912c-b88df333e4d4-c000.json {\"one_dimension\":[1,2,3],\"two_dimensions\":[\"a\",\"b\",\"c\",\"1\",\"2\",\"3\"]} {\"one_dimension\":[3,4,5],\"two_dimensions\":[\"d\",\"e\",\"f\",\"3\",\"4\",\"5\"]} Unfortunately, Spark does not allow you to name your output files, so if you need to distinguish which files were the output of a particular job you should group your files by timestamped folders (e.g. gs://de-book-dataproc/cleaned_files_20201031/ ). Deleting a PySpark Cluster If you have periodic need for data processing with Spark, then you should delete your Clusters once you are done using them. It only takes a minute or two to build a new Cluster, and you don't want to be paying for a Cluster you're not using. > gcloud dataproc clusters delete my-cluster --region=us-central1 However, if you are continuously using your Dataproc Cluster (e.g. to process data from a pipeline that has batches every 10 minutes), then it may make sense to leave your Cluster up all the time. Dataproc and Composer We've talked about how to use Dataproc, but we haven't really discussed how to integrate it into our Data Pipelines. The answer is simple enough, we can create tasks in Airflow (discussed in Chapter 5 ) to create the Cluster, submit the job, then delete the Cluster: t_create_cluster = BashOperator( task_id='create_cluster', bash_command=\"\"\" gcloud dataproc clusters create my-cluster \\\\ --region=us-central1 \\\\ --num-workers=2 \\\\ --worker-machine-type=n2-standard-2 \\\\ --image-version=1.5-debian10 \"\"\", dag=dag ) t_submit_job = BashOperator( task_id='submit_job', bash_command=\"\"\" gcloud dataproc jobs submit pyspark \\\\ gs://de-book-dataproc/flattener.py \\\\ --cluster=my-cluster \\\\ --region=us-central1 \"\"\", dag=dag ) t_submit_job.set_upstream(t_create_cluster) t_delete_cluster = BashOperator( task_id='delete_cluster', bash_command='gcloud dataproc clusters delete my-cluster --region=us-central1', dag=dag ) t_delete_cluster.set_upstream(t_submit_job) When managing a Dataproc Cluster from your Composer Environment it is important to have good alerting (discussed in Chapter 13 ). It is possible for Airflow to mark a task as failed (e.g. because it has timed out), without the Dataproc job ending or Cluster deleting itself. Cleaning Up We've already deleted our Dataproc Cluster above, so all that's left is for us to delete the bucket we created and the buckets created by Dataproc: > gsutil ls gs://dataproc-staging-us-central1-204024561480-kkb48scf/ gs://dataproc-temp-us-central1-204024561480-3i3d24k7/ gs://de-book-dataproc/ > gsutil rm -r gs://dataproc-staging-us-central1-204024561480-kkb48scf/ > gsutil rm -r gs://dataproc-temp-us-central1-204024561480-3i3d24k7/ > gsutil rm -r gs://de-book-dataproc/ Next Chapter: Chapter 8: Streaming Data with Pub/Sub","title":"Chapter 7: Parallel Processing with Dataproc and Spark"},{"location":"de-gcp-book/ch_07_parallel_processing/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_07_parallel_processing/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_07_parallel_processing/#chapter-7-parallel-processing-with-dataproc-and-spark","text":"Dataproc is GCP's fully managed service for running Apache Spark. Spark is an open-source program that has a wide array of capabilities, such as machine learning and data streaming, but I'm going to show you how we can use Spark perform transformations on very large data files. Spark's Python API (called PySpark), will look familiar to you if you have used Python's Pandas library. However, the key difference between processing data in Pandas vs. Spark is that Pandas works entirely in memory on a single machine, whereas Spark is designed to work across multiple machines and can manage the data in memory and on disk. Spark is operating on a Cluster of machines, and each machine is working in coordination to process a transformation job. This behavior works by default and allows Spark to perform massive parallel processing. That means we can quickly perform complex transformations on extremely large amounts of data. A Dataproc Cluster behaves similarly to a Composer Environment, in that we issue commands to create the Cluster and GCP charges us for the amount of time the Cluster is running. Like Composer, Dataproc does not automatically auto-scale (though unlike Composer, you can set an autoscaling policy as described here ). You set the number and type of machines being used when you instantiate your Cluster. In this chapter we'll look at a file that won't load into BigQuery, and we'll use PySpark to transform the file to allow us to load it. We'll need a bucket to store the file we'll be transforming, so let's make that now: > gsutil mb gs://de-book-dataproc","title":"Chapter 7 : Parallel Processing with Dataproc and Spark"},{"location":"de-gcp-book/ch_07_parallel_processing/#why-do-we-need-dataproc","text":"So far in this book I've been demonstrating an ELT (Extract Load Transform) approach to building a Data Pipeline, as distinguished from ETL. In an ELT approach we get our raw data into our Data Lake and Data Warehouse as quickly as possible, and then we perform our transformations. However, there are good reasons to perform transformations before loading your data into your Warehouse. Perhaps your team is more comfortable working in Python, and it's more convenient to do your processing in Python before you load in your data. Even if you are taking an ELT approach to your Pipelines, there may be instances where you will be unable to load the data into BigQuery without first doing some transformations. For example, while BigQuery can ingest nested data such as an array of integers or an array of objects (in BQ the objects are called \"Structs\"), BigQuery cannot ingest an array of arrays. Additionally, you may run into other problems with source files, such as them being malformed, that prevents BigQuery from loading the files. In Chapter 5 we did some pre-processing before we loaded our JSON data into BigQuery: we converted the JSON file from standard JSON format to Newline-Delimited JSON. We were able to do this using the compute power from our Composer Environment cluster because the amount of data we were dealing with was small. If we're dealing with large amounts of data that needs pre-processing, then we need to spin up additional resources so that we don't overwhelm our Composer Environment. Creating a Dataproc Cluster is an excellent way to bring more compute power to bear on your data before it is loaded into BigQuery.","title":"Why do we need Dataproc?"},{"location":"de-gcp-book/ch_07_parallel_processing/#creating-a-dataproc-cluster","text":"Before we can start working with Dataproc we need to enable it on GCP: > gcloud services enable dataproc.googleapis.com There are a lot of options you can set for creating a Dataproc Cluster. Here is a typical command: > gcloud dataproc clusters create my-cluster \\ --region=us-central1 \\ --num-workers=2 \\ --worker-machine-type=n2-standard-2 \\ --image-version=1.5-debian10 You can see I've specified the number of workers and the type of machine, which controls how much processing power the Cluster has. The image name defines the environment Spark will be running in. You can see more details about the images you can use here . We could have also included the --metadata and --initialization-actions options to install Python packages into our environment: --metadata='PIP_PACKAGES=requests==2.24.0' \\ --initialization-actions=gs://de-book-dataproc/pip-install.sh Google provides the script to install the requirements in an open GCS location, but recommends you copy the file locally to ensure updates they make to the file don't break your code. We can do that here: > gsutil cp gs://goog-dataproc-initialization-actions-us-central1/python/pip-install.sh gs://de-book-dataproc We won't need to install any Python packages for our job, below. We now have a Dataproc Cluster running Spark. Let's submit a job and have our Cluster do some work.","title":"Creating a Dataproc Cluster"},{"location":"de-gcp-book/ch_07_parallel_processing/#submitting-a-pyspark-job","text":"Let's suppose we have a data source that provides a JSON file where one of its fields is a list of lists, which is a structure BigQuery doesn't support. We can use Dataproc to transform the data and then save the file back to GCS to be loaded into BigQuery. First, let's create our source file and save it to our bucket: > echo '{\"one_dimension\": [1,2,3], \"two_dimensions\": [[\"a\",\"b\",\"c\"],[1,2,3]]} {\"one_dimension\": [3,4,5], \"two_dimensions\": [[\"d\",\"e\",\"f\"],[3,4,5]]}' > raw_file.json > gsutil cp raw_file.json gs://de-book-dataproc Now, let's build our PySpark file that will perform our transformation: #!/usr/bin/env python from pyspark.sql import SparkSession from pyspark.sql.functions import flatten def main(): spark = SparkSession.builder.appName(\"FileCleaner\").getOrCreate() df = spark.read.json(\"gs://de-book-dataproc/raw_file.json\") df2 = df.withColumn('two_dimensions', flatten(df.two_dimensions)) df2.write.json(\"gs://de-book-dataproc/cleaned_files/\", mode='overwrite') if __name__ == \"__main__\": main() Here we're doing a simple transformation of flattening our two-dimensional array into a one-dimensional array. Spark has a lot of versatility in transforming data, with whole books being written about it. Now that we have our PySpark file let's move it up to GCS: > gsutil cp flattener.py gs://de-book-dataproc Finally, we are ready to submit our dataproc job: > gcloud dataproc jobs submit pyspark \\ gs://de-book-dataproc/flattener.py \\ --cluster=my-cluster \\ --region=us-central1 With our job complete we can see our transformed file: > gsutil ls gs://de-book-dataproc/cleaned_files gs://de-book-dataproc/cleaned_files/ gs://de-book-dataproc/cleaned_files/_SUCCESS gs://de-book-dataproc/cleaned_files/part-00000-a07721fc-76a2-4235-912c-b88df333e4d4-c000.json > gsutil cat gs://de-book-dataproc/cleaned_files/part-00000-a07721fc-76a2-4235-912c-b88df333e4d4-c000.json {\"one_dimension\":[1,2,3],\"two_dimensions\":[\"a\",\"b\",\"c\",\"1\",\"2\",\"3\"]} {\"one_dimension\":[3,4,5],\"two_dimensions\":[\"d\",\"e\",\"f\",\"3\",\"4\",\"5\"]} Unfortunately, Spark does not allow you to name your output files, so if you need to distinguish which files were the output of a particular job you should group your files by timestamped folders (e.g. gs://de-book-dataproc/cleaned_files_20201031/ ).","title":"Submitting a PySpark Job"},{"location":"de-gcp-book/ch_07_parallel_processing/#deleting-a-pyspark-cluster","text":"If you have periodic need for data processing with Spark, then you should delete your Clusters once you are done using them. It only takes a minute or two to build a new Cluster, and you don't want to be paying for a Cluster you're not using. > gcloud dataproc clusters delete my-cluster --region=us-central1 However, if you are continuously using your Dataproc Cluster (e.g. to process data from a pipeline that has batches every 10 minutes), then it may make sense to leave your Cluster up all the time.","title":"Deleting a PySpark Cluster"},{"location":"de-gcp-book/ch_07_parallel_processing/#dataproc-and-composer","text":"We've talked about how to use Dataproc, but we haven't really discussed how to integrate it into our Data Pipelines. The answer is simple enough, we can create tasks in Airflow (discussed in Chapter 5 ) to create the Cluster, submit the job, then delete the Cluster: t_create_cluster = BashOperator( task_id='create_cluster', bash_command=\"\"\" gcloud dataproc clusters create my-cluster \\\\ --region=us-central1 \\\\ --num-workers=2 \\\\ --worker-machine-type=n2-standard-2 \\\\ --image-version=1.5-debian10 \"\"\", dag=dag ) t_submit_job = BashOperator( task_id='submit_job', bash_command=\"\"\" gcloud dataproc jobs submit pyspark \\\\ gs://de-book-dataproc/flattener.py \\\\ --cluster=my-cluster \\\\ --region=us-central1 \"\"\", dag=dag ) t_submit_job.set_upstream(t_create_cluster) t_delete_cluster = BashOperator( task_id='delete_cluster', bash_command='gcloud dataproc clusters delete my-cluster --region=us-central1', dag=dag ) t_delete_cluster.set_upstream(t_submit_job) When managing a Dataproc Cluster from your Composer Environment it is important to have good alerting (discussed in Chapter 13 ). It is possible for Airflow to mark a task as failed (e.g. because it has timed out), without the Dataproc job ending or Cluster deleting itself.","title":"Dataproc and Composer"},{"location":"de-gcp-book/ch_07_parallel_processing/#cleaning-up","text":"We've already deleted our Dataproc Cluster above, so all that's left is for us to delete the bucket we created and the buckets created by Dataproc: > gsutil ls gs://dataproc-staging-us-central1-204024561480-kkb48scf/ gs://dataproc-temp-us-central1-204024561480-3i3d24k7/ gs://de-book-dataproc/ > gsutil rm -r gs://dataproc-staging-us-central1-204024561480-kkb48scf/ > gsutil rm -r gs://dataproc-temp-us-central1-204024561480-3i3d24k7/ > gsutil rm -r gs://de-book-dataproc/ Next Chapter: Chapter 8: Streaming Data with Pub/Sub","title":"Cleaning Up"},{"location":"de-gcp-book/ch_08_streaming/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 8: Streaming Data with Pub/Sub : Streaming Data with Pub/Sub In previous chapters we've gone over how to set up batch-processing Data Pipelines, where a large number of records are added to our Warehouse at one time. Streaming, by contrast, focuses on adding small amounts of data very quickly, so that our Warehouse is updated constantly from the source system with minimal lag. To accomplish this we will be using GCP's Pub/Sub service. Pub/Sub is a messaging service that uses a \"publication\" and \"subscription\" architecture. For a particular Data Pipeline we will create a \"Topic\" in Pub/Sub. A Publisher service will publish data from the source system to the Topic, and then a Subscriber service will receive the data from the Topic and process it, such as inserting that data into BigQuery. One of the big concerns about streaming data is that new data will be generated faster than it can be ingested (this problem also exists in traditional batch processing, but is more likely to be a concern with streaming pipelines). Pub/Sub overcomes this potential issue by separating the service that gets the data from the source and the service that inserts the data into our warehouse. Because Pub/Sub persists the data until we are able to consume it, we have less concerns about the data being lost in transit. And once the data is published, Pub/Sub ensures the data will not be lost even if a failure occurs while the subscriber is pulling the data. In this chapter we will create a Topic, deploy a Publisher service to generate our source data, and deploy a Subscriber service that pulls the source data. Creating a Pub/Sub Topic Generally, we'll have a single Topic per pipeline. A single Topic can have multiple Publishers and multiple Subscribers, so it offers lots of flexibility in where the data is coming from and where it is sent to. Creating a Topic is simple. Unlike Composer or Dataproc, there are few configuration options , and you'll most likely be setting up your Topic by simply executing: > gcloud pubsub topics create my_topic In Chapter 11: Deployment Pipelines with Cloud Build we'll discuss how to manage your Topics in Terraform, along with your other cloud infrastructure. Now that we have our Topic, we're ready to create our Publisher and Subscriber. Creating a Publisher The Publisher will need to have authorization to publish to your Topic. This means that the Publisher must either be controlled by your team, or you must work with another team to grant them access. Sometimes it'll be convenient to grant credentials to another team, as they will be working with a service that can publish directly. However, your source data may come from a system that is unable or unwilling to publish directly to your Topic. In such an instance you will have to set up your own service to gather the data, then publish to your Topic. An example would be gathering data in small chunks (micro-batches) through a JDBC connection and publishing that data to a Pub/Sub Topic. For our purposes, we are going to upload data to a GCS bucket (covered in Chapter 3 ) and use a Cloud Function (covered in Chapter 6 ) to publish the data to a Pub/Sub Topic. Instead of running in a Cloud Function, you may decide to run the Publisher on Google Kubernetes Engine (GKE) or Cloud Run, which are also good options. First let's create a bucket that will trigger the Cloud Function publisher: > gsutil mb gs://de-book-publisher Now let's create our Cloud Function to publish our data. As discussed in Chapter 6 , this file must be saved as main.py before being uploaded to GCP. def publish_data(event, context): from google.cloud import storage from google.cloud import pubsub_v1 # get the message to publish bucket_name = event['bucket'] file_path = event['name'] storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(file_path) message = blob.download_as_text() # publish the message project_id = 'de-book-dev' topic_id = 'my_topic' publisher = pubsub_v1.PublisherClient() topic_path = publisher.topic_path(project_id, topic_id) response = publisher.publish(topic_path, message.encode('utf-8')) print(response.result()) Our Cloud Function requires two libraries, so we've got to let GCP know we need these in our environment. As discussed in Chapter 6 , we do this by adding a requirements.txt file in the same folder as our Cloud Function: google-cloud-storage=1.35.0 google-cloud-pubsub=2.2.0 Now let's deploy this function and tell it to trigger whenever a new file is uploaded to our bucket. Run the following from the same folder where the main.py file you just created is saved. > gcloud functions deploy publish_data \\ --runtime python37 \\ --trigger-resource gs://de-book-publisher \\ --trigger-event google.storage.object.finalize That's it. We now have a function that will publish messages to our topic. Now we just need a subscriber so we can read them. Creating a Subscriber There are two types of subscriptions for a topic: \"push\" and \"pull\". The push subscription has Pub/Sub send the message to a designated HTTPS endpoint whenever it receives a message. A pull subscription requires a different service to poll the Topic to see if messages are available, and then pull them down. Let's create a pull subscription (the default): > gcloud pubsub subscriptions create my-subscriber --topic=my_topic Now that our publisher and subscriber are set up let's upload a file to the bucket that will publish a message: > echo \"My first message\" | gsutil cp - gs://de-book-publisher/first_message.txt We've created a file in our bucket, which has triggered a Cloud Function that copied the contents of the file into a message that it published to a Pub/Sub Topic. Let's check to see if the message is there: > gcloud pubsub subscriptions pull my-subscriber \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA \u2502 MESSAGE_ID \u2502 ORDERING_KEY \u2502 ATTRIBUTES \u2502 DELIVERY_ATTEMPT \u2502 ACK_ID \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 My first message \u2502 1901834288157567 \u2502 \u2502 \u2502 \u2502 ABC123 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 One of the ways that Pub/Sub allows your streaming service to be resilient is that it will not remove a message from a Topic until the message has been acknowledged. This helps ensure that a failure while your subscriber is pulling data doesn't result in data loss (though this does leave open the possibility of receiving the same message twice). So this time let's pull the message down and acknowledge: > gcloud pubsub subscriptions pull my-subscriber --auto-ack \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA \u2502 MESSAGE_ID \u2502 ORDERING_KEY \u2502 ATTRIBUTES \u2502 DELIVERY_ATTEMPT \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 My first message \u2502 1901834288157567 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 If we try to pull again we find there are no messages: > gcloud pubsub subscriptions pull my-subscriber --auto-ack Listed 0 items. In practice you're more likely to pull messages with Python code like so: from google.cloud import pubsub_v1 project_id = 'de-book-dev' subscription_id = 'my-subscriber' subscriber = pubsub_v1.SubscriberClient() subscription_path = subscriber.subscription_path(project_id, subscription_id) while True: response = subscriber.pull(subscription_path) if response.received_messages: for item in response.received_messages: print(item.message) # this is where you would do something like insert the data into BigQuery subscriber.acknowledge(subscription_path, ack_ids=[item.ack_id]) Cleaning Up We created a GCS bucket, Cloud Function, and Pub/Sub Topic in this chapter, so let's delete them: > gsutil rm -r gs://de-book-publisher > gcloud functions delete publish_data > gcloud pubsub topics delete my_topic Next Chapter: Chapter 9: Managing Credentials with Google Secret Manager","title":"Chapter 8: Streaming Data with Pub/Sub"},{"location":"de-gcp-book/ch_08_streaming/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_08_streaming/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_08_streaming/#chapter-8-streaming-data-with-pubsub-streaming-data-with-pubsub","text":"In previous chapters we've gone over how to set up batch-processing Data Pipelines, where a large number of records are added to our Warehouse at one time. Streaming, by contrast, focuses on adding small amounts of data very quickly, so that our Warehouse is updated constantly from the source system with minimal lag. To accomplish this we will be using GCP's Pub/Sub service. Pub/Sub is a messaging service that uses a \"publication\" and \"subscription\" architecture. For a particular Data Pipeline we will create a \"Topic\" in Pub/Sub. A Publisher service will publish data from the source system to the Topic, and then a Subscriber service will receive the data from the Topic and process it, such as inserting that data into BigQuery. One of the big concerns about streaming data is that new data will be generated faster than it can be ingested (this problem also exists in traditional batch processing, but is more likely to be a concern with streaming pipelines). Pub/Sub overcomes this potential issue by separating the service that gets the data from the source and the service that inserts the data into our warehouse. Because Pub/Sub persists the data until we are able to consume it, we have less concerns about the data being lost in transit. And once the data is published, Pub/Sub ensures the data will not be lost even if a failure occurs while the subscriber is pulling the data. In this chapter we will create a Topic, deploy a Publisher service to generate our source data, and deploy a Subscriber service that pulls the source data.","title":"Chapter 8: Streaming Data with Pub/Sub: Streaming Data with Pub/Sub"},{"location":"de-gcp-book/ch_08_streaming/#creating-a-pubsub-topic","text":"Generally, we'll have a single Topic per pipeline. A single Topic can have multiple Publishers and multiple Subscribers, so it offers lots of flexibility in where the data is coming from and where it is sent to. Creating a Topic is simple. Unlike Composer or Dataproc, there are few configuration options , and you'll most likely be setting up your Topic by simply executing: > gcloud pubsub topics create my_topic In Chapter 11: Deployment Pipelines with Cloud Build we'll discuss how to manage your Topics in Terraform, along with your other cloud infrastructure. Now that we have our Topic, we're ready to create our Publisher and Subscriber.","title":"Creating a Pub/Sub Topic"},{"location":"de-gcp-book/ch_08_streaming/#creating-a-publisher","text":"The Publisher will need to have authorization to publish to your Topic. This means that the Publisher must either be controlled by your team, or you must work with another team to grant them access. Sometimes it'll be convenient to grant credentials to another team, as they will be working with a service that can publish directly. However, your source data may come from a system that is unable or unwilling to publish directly to your Topic. In such an instance you will have to set up your own service to gather the data, then publish to your Topic. An example would be gathering data in small chunks (micro-batches) through a JDBC connection and publishing that data to a Pub/Sub Topic. For our purposes, we are going to upload data to a GCS bucket (covered in Chapter 3 ) and use a Cloud Function (covered in Chapter 6 ) to publish the data to a Pub/Sub Topic. Instead of running in a Cloud Function, you may decide to run the Publisher on Google Kubernetes Engine (GKE) or Cloud Run, which are also good options. First let's create a bucket that will trigger the Cloud Function publisher: > gsutil mb gs://de-book-publisher Now let's create our Cloud Function to publish our data. As discussed in Chapter 6 , this file must be saved as main.py before being uploaded to GCP. def publish_data(event, context): from google.cloud import storage from google.cloud import pubsub_v1 # get the message to publish bucket_name = event['bucket'] file_path = event['name'] storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(file_path) message = blob.download_as_text() # publish the message project_id = 'de-book-dev' topic_id = 'my_topic' publisher = pubsub_v1.PublisherClient() topic_path = publisher.topic_path(project_id, topic_id) response = publisher.publish(topic_path, message.encode('utf-8')) print(response.result()) Our Cloud Function requires two libraries, so we've got to let GCP know we need these in our environment. As discussed in Chapter 6 , we do this by adding a requirements.txt file in the same folder as our Cloud Function: google-cloud-storage=1.35.0 google-cloud-pubsub=2.2.0 Now let's deploy this function and tell it to trigger whenever a new file is uploaded to our bucket. Run the following from the same folder where the main.py file you just created is saved. > gcloud functions deploy publish_data \\ --runtime python37 \\ --trigger-resource gs://de-book-publisher \\ --trigger-event google.storage.object.finalize That's it. We now have a function that will publish messages to our topic. Now we just need a subscriber so we can read them.","title":"Creating a Publisher"},{"location":"de-gcp-book/ch_08_streaming/#creating-a-subscriber","text":"There are two types of subscriptions for a topic: \"push\" and \"pull\". The push subscription has Pub/Sub send the message to a designated HTTPS endpoint whenever it receives a message. A pull subscription requires a different service to poll the Topic to see if messages are available, and then pull them down. Let's create a pull subscription (the default): > gcloud pubsub subscriptions create my-subscriber --topic=my_topic Now that our publisher and subscriber are set up let's upload a file to the bucket that will publish a message: > echo \"My first message\" | gsutil cp - gs://de-book-publisher/first_message.txt We've created a file in our bucket, which has triggered a Cloud Function that copied the contents of the file into a message that it published to a Pub/Sub Topic. Let's check to see if the message is there: > gcloud pubsub subscriptions pull my-subscriber \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA \u2502 MESSAGE_ID \u2502 ORDERING_KEY \u2502 ATTRIBUTES \u2502 DELIVERY_ATTEMPT \u2502 ACK_ID \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 My first message \u2502 1901834288157567 \u2502 \u2502 \u2502 \u2502 ABC123 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 One of the ways that Pub/Sub allows your streaming service to be resilient is that it will not remove a message from a Topic until the message has been acknowledged. This helps ensure that a failure while your subscriber is pulling data doesn't result in data loss (though this does leave open the possibility of receiving the same message twice). So this time let's pull the message down and acknowledge: > gcloud pubsub subscriptions pull my-subscriber --auto-ack \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA \u2502 MESSAGE_ID \u2502 ORDERING_KEY \u2502 ATTRIBUTES \u2502 DELIVERY_ATTEMPT \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 My first message \u2502 1901834288157567 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 If we try to pull again we find there are no messages: > gcloud pubsub subscriptions pull my-subscriber --auto-ack Listed 0 items. In practice you're more likely to pull messages with Python code like so: from google.cloud import pubsub_v1 project_id = 'de-book-dev' subscription_id = 'my-subscriber' subscriber = pubsub_v1.SubscriberClient() subscription_path = subscriber.subscription_path(project_id, subscription_id) while True: response = subscriber.pull(subscription_path) if response.received_messages: for item in response.received_messages: print(item.message) # this is where you would do something like insert the data into BigQuery subscriber.acknowledge(subscription_path, ack_ids=[item.ack_id])","title":"Creating a Subscriber"},{"location":"de-gcp-book/ch_08_streaming/#cleaning-up","text":"We created a GCS bucket, Cloud Function, and Pub/Sub Topic in this chapter, so let's delete them: > gsutil rm -r gs://de-book-publisher > gcloud functions delete publish_data > gcloud pubsub topics delete my_topic Next Chapter: Chapter 9: Managing Credentials with Google Secret Manager","title":"Cleaning Up"},{"location":"de-gcp-book/ch_09_secrets/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 9: Managing Credentials with Google Secret Manager : Managing Credentials with Google Secret Manager There will likely be times where you are going to need to give your data pipelines access to credentials. For GCP resources we can manage access through permissions on our service accounts, but often your pipeline will need to access systems outside of GCP. By using Google Secret Manager we are able to securely store passwords and other secret information. In this chapter I will show you how to create and access secrets using Google Secret Manager. Then we'll create an Airflow DAG that will simulate making an HTTP request to a secured web API, then saving the results to GCS. Creating a Secret The first thing we need to do is enable the Google Secret Manager service: > gcloud services enable secretmanager.googleapis.com Creating a secret is quite simple. Here I'm creating a secret called \"source-api-password\" that contains the value \"abc123\": > echo -n \"abc123\" | gcloud secrets create source-api-password --data-file=- We can also create a secret where the value is the contents of a file: > gcloud secrets create source-api-password-2 --data-file=my-password.txt In Chapter 10: Infrastructure as Code with Terraform we'll discuss how to manage secrets with Terraform. While it is good practice to create secrets with your Infrastructure as Code solution, the values of those secrets still need to be added manually to ensure they are not saved in your code repositories. Accessing a Secret Accessing a secret is just as easy as creating it: > gcloud secrets versions access latest --secret=source-api-password abc123 The values in each secret have a particular \"version\", so you can update the value in a secret and still access older values for that same secret. In the above code we specify we want the value for the \"latest\" version, but we also could have specified an ID for a specific version. While you will likely be setting secret values through the command line, you will most likely be accessing them within your code: from google.cloud import secretmanager def get_secret(project, secret_name, version): client = secretmanager.SecretManagerServiceClient() secret_path = client.secret_version_path(project, secret_name, version) secret = client.access_secret_version(secret_path) return secret.payload.data.decode(\"UTF-8\") project = \"de-book-dev\" secret_name = \"source-api-password\" version = \"latest\" plaintext_secret = get_secret(project, secret_name, version) Using Google Secret Manager in Airflow Below is an example of how you might typically use Google Secret Manager to access secrets within your DAG for getting data from a web API. The web API in this example doesn't exist (so it won't work if you run it unmodified in your own Airflow instance), but the below code shows you a common type of DAG that would require accessing a secret. import requests import json from airflow import DAG from airflow.operators.python_operator import PythonOperator from airflow.operators.bash_operator import BashOperator default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'download_form_web_api', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) def get_secret(project, secret_name, version='latest'): client = secretmanager.SecretManagerServiceClient() secret_path = client.secret_version_path(project, secret_name, version) secret = client.access_secret_version(secret_path) return secret.payload.data.decode('UTF-8') def generate_credentials(): username = 'my_username' password = get_secret('my_project', 'source-api-password') credentials = f'{username}:{password}' return credentials def download_data_to_local(): url = 'https://www.example.com/api/source-endpoint' credentials = generate_credentials() request_headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\", \"Authorization\": credentials} export_json = { \"exportType\": \"Full_Data\", } response = requests.post(url=url, json=export_json, headers=request_headers) data = response.json() # composer automatically maps \"/home/airflow/gcs/data/\" to a bucket so it can be treated as a local directory with open('/home/airflow/gcs/data/data.json', 'w') as f: json.dump(data, f) t_download_data_to_local = PythonOperator( task_id='download_data_to_local', python_callable=get_data, op_kwargs={\"sku_list\": sku_list}, dag=dag ) t_copy_data_to_gcs = BashOperator( task_id='copy_data_to_gcs', bash_command='gsutil cp /home/airflow/gcs/data/data.json gs://my-bucket/web-api-files/' dag=dag ) t_download_data_to_local.set_upstream(t_download_data_to_local) Cleaning Up Google Secret Manager is quite cheap, with each secret version costing 6 cents per month, and an additional 3 cents for every 10,000 times you access your secret. Nonetheless, let's clean up what we're not using. > gcloud secrets list NAME CREATED REPLICATION_POLICY LOCATIONS source-api-password 2021-01-13T04:07:31 automatic - > gcloud secrets delete source-api-password Next Chapter: Chapter 10: Infrastructure as Code with Terraform","title":"Chapter 9: Managing Credentials with Google Secret Manager"},{"location":"de-gcp-book/ch_09_secrets/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_09_secrets/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_09_secrets/#chapter-9-managing-credentials-with-google-secret-manager-managing-credentials-with-google-secret-manager","text":"There will likely be times where you are going to need to give your data pipelines access to credentials. For GCP resources we can manage access through permissions on our service accounts, but often your pipeline will need to access systems outside of GCP. By using Google Secret Manager we are able to securely store passwords and other secret information. In this chapter I will show you how to create and access secrets using Google Secret Manager. Then we'll create an Airflow DAG that will simulate making an HTTP request to a secured web API, then saving the results to GCS.","title":"Chapter 9: Managing Credentials with Google Secret Manager: Managing Credentials with Google Secret Manager"},{"location":"de-gcp-book/ch_09_secrets/#creating-a-secret","text":"The first thing we need to do is enable the Google Secret Manager service: > gcloud services enable secretmanager.googleapis.com Creating a secret is quite simple. Here I'm creating a secret called \"source-api-password\" that contains the value \"abc123\": > echo -n \"abc123\" | gcloud secrets create source-api-password --data-file=- We can also create a secret where the value is the contents of a file: > gcloud secrets create source-api-password-2 --data-file=my-password.txt In Chapter 10: Infrastructure as Code with Terraform we'll discuss how to manage secrets with Terraform. While it is good practice to create secrets with your Infrastructure as Code solution, the values of those secrets still need to be added manually to ensure they are not saved in your code repositories.","title":"Creating a Secret"},{"location":"de-gcp-book/ch_09_secrets/#accessing-a-secret","text":"Accessing a secret is just as easy as creating it: > gcloud secrets versions access latest --secret=source-api-password abc123 The values in each secret have a particular \"version\", so you can update the value in a secret and still access older values for that same secret. In the above code we specify we want the value for the \"latest\" version, but we also could have specified an ID for a specific version. While you will likely be setting secret values through the command line, you will most likely be accessing them within your code: from google.cloud import secretmanager def get_secret(project, secret_name, version): client = secretmanager.SecretManagerServiceClient() secret_path = client.secret_version_path(project, secret_name, version) secret = client.access_secret_version(secret_path) return secret.payload.data.decode(\"UTF-8\") project = \"de-book-dev\" secret_name = \"source-api-password\" version = \"latest\" plaintext_secret = get_secret(project, secret_name, version)","title":"Accessing a Secret"},{"location":"de-gcp-book/ch_09_secrets/#using-google-secret-manager-in-airflow","text":"Below is an example of how you might typically use Google Secret Manager to access secrets within your DAG for getting data from a web API. The web API in this example doesn't exist (so it won't work if you run it unmodified in your own Airflow instance), but the below code shows you a common type of DAG that would require accessing a secret. import requests import json from airflow import DAG from airflow.operators.python_operator import PythonOperator from airflow.operators.bash_operator import BashOperator default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'download_form_web_api', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) def get_secret(project, secret_name, version='latest'): client = secretmanager.SecretManagerServiceClient() secret_path = client.secret_version_path(project, secret_name, version) secret = client.access_secret_version(secret_path) return secret.payload.data.decode('UTF-8') def generate_credentials(): username = 'my_username' password = get_secret('my_project', 'source-api-password') credentials = f'{username}:{password}' return credentials def download_data_to_local(): url = 'https://www.example.com/api/source-endpoint' credentials = generate_credentials() request_headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\", \"Authorization\": credentials} export_json = { \"exportType\": \"Full_Data\", } response = requests.post(url=url, json=export_json, headers=request_headers) data = response.json() # composer automatically maps \"/home/airflow/gcs/data/\" to a bucket so it can be treated as a local directory with open('/home/airflow/gcs/data/data.json', 'w') as f: json.dump(data, f) t_download_data_to_local = PythonOperator( task_id='download_data_to_local', python_callable=get_data, op_kwargs={\"sku_list\": sku_list}, dag=dag ) t_copy_data_to_gcs = BashOperator( task_id='copy_data_to_gcs', bash_command='gsutil cp /home/airflow/gcs/data/data.json gs://my-bucket/web-api-files/' dag=dag ) t_download_data_to_local.set_upstream(t_download_data_to_local)","title":"Using Google Secret Manager in Airflow"},{"location":"de-gcp-book/ch_09_secrets/#cleaning-up","text":"Google Secret Manager is quite cheap, with each secret version costing 6 cents per month, and an additional 3 cents for every 10,000 times you access your secret. Nonetheless, let's clean up what we're not using. > gcloud secrets list NAME CREATED REPLICATION_POLICY LOCATIONS source-api-password 2021-01-13T04:07:31 automatic - > gcloud secrets delete source-api-password Next Chapter: Chapter 10: Infrastructure as Code with Terraform","title":"Cleaning Up"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 10: Infrastructure as Code with Terraform : Infrastructure as Code with Terraform Much of a Data Engineer's responsibility is to manage their tools. So far in this book we've discussed Composer, GCS, BigQuery, Cloud Functions, Dataproc, Pub/Sub, and Secret Manager. We've been managing these resources through command line scripts, which is useful for learning, but not a good way to handle your production environment. We want to define these resources in text files and have those files in source control. This is called Infrastructure as Code. Terraform is a tool that allows you to define your infrastructure, and the various permissions to access that infrastructure, in a series of configuration files. Terraform is not owned or managed by Google, but it does work well with GCP. Terraform is not a GCP service, so we'll be running Terraform on our local machine. Installing Terraform If you're on a Mac, Terraform can easily be installed through Homebrew : > brew tap hashicorp/tap > brew install hashicorp/tap/terraform Terraform also works on Linux and Windows. More installation methods are available here . If you want to enable auto-complete for Terraform on your bash or zsh shell, then execute the following command then restart your terminal: > terraform -install-autocomplete That's it. We now have Terraform on our local machine. Next we'll create some configuration files to define our infrastructure. Creating Your Configuration Files One of the main benefits of using Terraform, or Infrastructure as Code more generally, is the ability to define our infrastructure in a way that allows us to put it into source control. In this section we'll be writing the configuration files that will go into our source control (e.g. git). While Terraform allows you to get sophisticated in your setup, it also allows you to get up and running with minimal code. Let's create our main.tf file: // This \"terraform block\" tells Terraform it will need to look in the google registry when identifying which resources to deploy terraform { required_providers { google = { source = \"hashicorp/google\" version = \"3.5.0\" } } } // This \"provider block\" configures your google account. provider \"google\" { credentials = file(\"../keys/de-book-dev-secret-key.json\") // provide the file path to your GCP secret key file project = \"de-book-dev\" region = \"us-central1\" zone = \"us-central1-c\" } // This \"resource block\" configures the specific infrastructure you wish to deploy resource \"google_storage_bucket\" \"de-book-terraform-test\" { name = \"de-book-terraform-test-1234567654321\" // This is the name of the bucket to be created location = \"US\" force_destroy = false // This setting prevents us from deleting a bucket if there are files within storage_class = \"STANDARD\" } You can view how to create additional types of resource blocks here . This code allows us to create a bucket called \"de-book-terraform-test-1234567654321\". While the above code only creates a single resource (a GCS bucket), we can add as many resource blocks as we need to this file to define all the infrastructure we need. We'll show this in the Updating your Infrastructure section, below. While our infrastructure configuration files, like the one above, should go into source control, Terraform will also generate some configuration files for internal use that are not valuable to add to source control. For those we can create a .gitignore file: .tfstate .terraform .tfplan Deploying Your Infrastructure to GCP Now that we've defined our infrastructure, let's deploy it. In the same directory as our main.tf file, execute the following: > terraform init > terraform apply An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_storage_bucket.de-book-terraform-test will be created + resource \"google_storage_bucket\" \"de-book-terraform-test\" { + bucket_policy_only = (known after apply) + force_destroy = false + id = (known after apply) + location = \"US\" + name = \"de-book-terraform-test-1234567654321\" + project = (known after apply) + self_link = (known after apply) + storage_class = \"STANDARD\" + url = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Before allowing us to deploy our infrastructure, Terraform prints an execution plan with all of the actions it will take. This is especially useful when multiple engineers are working on the same infrastructure (as is often the case). If someone else has modified your infrastructure, but that change is not present in your files, you will see it in this execution plan. So be sure to actually read this plan, otherwise you could end up accidentally rolling back infrastructure your colleagues deployed. And no one wants to be that guy. But in this case, we see just the one change we were expecting: a new bucket is being added. So let's type \"yes\" and deploy our bucket: Enter a value: yes google_storage_bucket.de-book-terraform-test: Creating... google_storage_bucket.de-book-terraform-test: Creation complete after 1s [id=de-book-terraform-test-1234567654321] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Let's confirm that our bucket was created: > gsutil ls gs://de-book-terraform-test-1234567654321/ Obviously we'll be using a lot more infrastructure in production. I provide an example Terraform file with more resources listed in Appendix A . Documentation for the full list of infrastructure you can deploy is here . Updating your Infrastructure We can expect our infrastructure needs to continue to change. Fortunately implementing that change is as simple as updating our Terraform file. Let's create a Pub/Sub Topic, a subscription for that Topic, and set force_destroy to true for our existing bucket. Our main.tf file now looks like: terraform { required_providers { google = { source = \"hashicorp/google\" version = \"3.5.0\" } } } provider \"google\" { credentials = file(\"../keys/de-book-dev-2d2abed79f9f.json\") project = \"de-book-dev\" region = \"us-central1\" zone = \"us-central1-c\" } resource \"google_storage_bucket\" \"de-book-terraform-test\" { name = \"de-book-terraform-test-1234567654321\" location = \"US\" force_destroy = true # Updated this to true storage_class = \"STANDARD\" } // Added the resources below resource \"google_pubsub_topic\" \"terraform-test\" { name = \"terraform-test-topic\" } resource \"google_pubsub_subscription\" \"example\" { name = \"example-subscription\" topic = google_pubsub_topic.terraform-test.name ack_deadline_seconds = 20 } Now we can apply our changes (typing \"yes\" when prompted): > terraform apply google_storage_bucket.de-book-terraform-test: Refreshing state... [id=de-book-terraform-test-1234567654321] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create ~ update in-place Terraform will perform the following actions: # google_pubsub_subscription.example will be created + resource \"google_pubsub_subscription\" \"example\" { + ack_deadline_seconds = 20 + id = (known after apply) + message_retention_duration = \"604800s\" + name = \"example-subscription\" + path = (known after apply) + project = (known after apply) + topic = \"terraform-test-topic\" + expiration_policy { + ttl = (known after apply) } } # google_pubsub_topic.terraform-test will be created + resource \"google_pubsub_topic\" \"terraform-test\" { + id = (known after apply) + name = \"terraform-test-topic\" + project = (known after apply) + message_storage_policy { + allowed_persistence_regions = (known after apply) } } # google_storage_bucket.de-book-terraform-test will be updated in-place ~ resource \"google_storage_bucket\" \"de-book-terraform-test\" { bucket_policy_only = false ~ force_destroy = false -> true id = \"de-book-terraform-test-1234567654321\" labels = {} location = \"US\" name = \"de-book-terraform-test-1234567654321\" project = \"de-book-dev\" requester_pays = false self_link = \"https://www.googleapis.com/storage/v1/b/de-book-terraform-test-1234567654321\" storage_class = \"STANDARD\" url = \"gs://de-book-terraform-test-1234567654321\" } Plan: 2 to add, 1 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes google_pubsub_topic.terraform-test: Creating... google_storage_bucket.de-book-terraform-test: Modifying... [id=de-book-terraform-test-1234567654321] google_storage_bucket.de-book-terraform-test: Modifications complete after 0s [id=de-book-terraform-test-1234567654321] google_pubsub_topic.terraform-test: Creation complete after 1s [id=projects/de-book-dev/topics/terraform-test-topic] google_pubsub_subscription.example: Creating... google_pubsub_subscription.example: Creation complete after 2s [id=projects/de-book-dev/subscriptions/example-subscription] Apply complete! Resources: 2 added, 1 changed, 0 destroyed. We see that Terraform added two resources and changed one resource, as we expected. While it is still possible to provision GCP infrastructure through other means, such as the GCP Console or command line, in practice our Terraform file or files should represent our entire infrastructure on GCP. We can remove a resource by simply deleting the resource block from our Terraform file. Let's delete out bucket: terraform { required_providers { google = { source = \"hashicorp/google\" version = \"3.5.0\" } } } provider \"google\" { credentials = file(\"../keys/de-book-dev-2d2abed79f9f.json\") project = \"de-book-dev\" region = \"us-central1\" zone = \"us-central1-c\" } // Our GCS resource used to be here resource \"google_pubsub_topic\" \"terraform-test\" { name = \"terraform-test-topic\" } resource \"google_pubsub_subscription\" \"example\" { name = \"example-subscription\" topic = google_pubsub_topic.terraform-test.name ack_deadline_seconds = 20 } Now let's apply our changes: > terraform apply google_pubsub_topic.terraform-test: Refreshing state... [id=projects/de-book-dev/topics/terraform-test-topic] google_storage_bucket.de-book-terraform-test: Refreshing state... [id=de-book-terraform-test-1234567654321] google_pubsub_subscription.example: Refreshing state... [id=projects/de-book-dev/subscriptions/example-subscription] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_storage_bucket.de-book-terraform-test will be destroyed - resource \"google_storage_bucket\" \"de-book-terraform-test\" { - bucket_policy_only = false -> null - force_destroy = true -> null - id = \"de-book-terraform-test-1234567654321\" -> null - labels = {} -> null - location = \"US\" -> null - name = \"de-book-terraform-test-1234567654321\" -> null - project = \"de-book-dev\" -> null - requester_pays = false -> null - self_link = \"https://www.googleapis.com/storage/v1/b/de-book-terraform-test-1234567654321\" -> null - storage_class = \"STANDARD\" -> null - url = \"gs://de-book-terraform-test-1234567654321\" -> null } Plan: 0 to add, 0 to change, 1 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes google_storage_bucket.de-book-terraform-test: Destroying... [id=de-book-terraform-test-1234567654321] google_storage_bucket.de-book-terraform-test: Destruction complete after 1s Apply complete! Resources: 0 added, 0 changed, 1 destroyed. Cleaning Up We used Terraform to create three resources, then destroy one of them. So we should have two resources left: > terraform state list google_pubsub_subscription.example google_pubsub_topic.terraform-test We could delete these Pub/Sub resources with the command line, as we did in Chapter 8: Streaming Data with Pub/Sub . However, we can also tell Terraform to destroy all of the resources it manages: > terraform destroy google_pubsub_topic.terraform-test: Refreshing state... [id=projects/de-book-dev/topics/terraform-test-topic] google_pubsub_subscription.example: Refreshing state... [id=projects/de-book-dev/subscriptions/example-subscription] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_pubsub_subscription.example will be destroyed - resource \"google_pubsub_subscription\" \"example\" { - ack_deadline_seconds = 20 -> null - id = \"projects/de-book-dev/subscriptions/example-subscription\" -> null - labels = {} -> null - message_retention_duration = \"604800s\" -> null - name = \"example-subscription\" -> null - path = \"projects/de-book-dev/subscriptions/example-subscription\" -> null - project = \"de-book-dev\" -> null - retain_acked_messages = false -> null - topic = \"projects/de-book-dev/topics/terraform-test-topic\" -> null - expiration_policy { - ttl = \"2678400s\" -> null } } # google_pubsub_topic.terraform-test will be destroyed - resource \"google_pubsub_topic\" \"terraform-test\" { - id = \"projects/de-book-dev/topics/terraform-test-topic\" -> null - labels = {} -> null - name = \"terraform-test-topic\" -> null - project = \"de-book-dev\" -> null } Plan: 0 to add, 0 to change, 2 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes google_pubsub_subscription.example: Destroying... [id=projects/de-book-dev/subscriptions/example-subscription] google_pubsub_subscription.example: Destruction complete after 1s google_pubsub_topic.terraform-test: Destroying... [id=projects/de-book-dev/topics/terraform-test-topic] google_pubsub_topic.terraform-test: Destruction complete after 1s Destroy complete! Resources: 2 destroyed. Next Chapter: Chapter 11: Deployment Pipelines with Cloud Build","title":"Chapter 10: Infrastructure as Code with Terraform"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#chapter-10-infrastructure-as-code-with-terraform-infrastructure-as-code-with-terraform","text":"Much of a Data Engineer's responsibility is to manage their tools. So far in this book we've discussed Composer, GCS, BigQuery, Cloud Functions, Dataproc, Pub/Sub, and Secret Manager. We've been managing these resources through command line scripts, which is useful for learning, but not a good way to handle your production environment. We want to define these resources in text files and have those files in source control. This is called Infrastructure as Code. Terraform is a tool that allows you to define your infrastructure, and the various permissions to access that infrastructure, in a series of configuration files. Terraform is not owned or managed by Google, but it does work well with GCP. Terraform is not a GCP service, so we'll be running Terraform on our local machine.","title":"Chapter 10: Infrastructure as Code with Terraform: Infrastructure as Code with Terraform"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#installing-terraform","text":"If you're on a Mac, Terraform can easily be installed through Homebrew : > brew tap hashicorp/tap > brew install hashicorp/tap/terraform Terraform also works on Linux and Windows. More installation methods are available here . If you want to enable auto-complete for Terraform on your bash or zsh shell, then execute the following command then restart your terminal: > terraform -install-autocomplete That's it. We now have Terraform on our local machine. Next we'll create some configuration files to define our infrastructure.","title":"Installing Terraform"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#creating-your-configuration-files","text":"One of the main benefits of using Terraform, or Infrastructure as Code more generally, is the ability to define our infrastructure in a way that allows us to put it into source control. In this section we'll be writing the configuration files that will go into our source control (e.g. git). While Terraform allows you to get sophisticated in your setup, it also allows you to get up and running with minimal code. Let's create our main.tf file: // This \"terraform block\" tells Terraform it will need to look in the google registry when identifying which resources to deploy terraform { required_providers { google = { source = \"hashicorp/google\" version = \"3.5.0\" } } } // This \"provider block\" configures your google account. provider \"google\" { credentials = file(\"../keys/de-book-dev-secret-key.json\") // provide the file path to your GCP secret key file project = \"de-book-dev\" region = \"us-central1\" zone = \"us-central1-c\" } // This \"resource block\" configures the specific infrastructure you wish to deploy resource \"google_storage_bucket\" \"de-book-terraform-test\" { name = \"de-book-terraform-test-1234567654321\" // This is the name of the bucket to be created location = \"US\" force_destroy = false // This setting prevents us from deleting a bucket if there are files within storage_class = \"STANDARD\" } You can view how to create additional types of resource blocks here . This code allows us to create a bucket called \"de-book-terraform-test-1234567654321\". While the above code only creates a single resource (a GCS bucket), we can add as many resource blocks as we need to this file to define all the infrastructure we need. We'll show this in the Updating your Infrastructure section, below. While our infrastructure configuration files, like the one above, should go into source control, Terraform will also generate some configuration files for internal use that are not valuable to add to source control. For those we can create a .gitignore file: .tfstate .terraform .tfplan","title":"Creating Your Configuration Files"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#deploying-your-infrastructure-to-gcp","text":"Now that we've defined our infrastructure, let's deploy it. In the same directory as our main.tf file, execute the following: > terraform init > terraform apply An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_storage_bucket.de-book-terraform-test will be created + resource \"google_storage_bucket\" \"de-book-terraform-test\" { + bucket_policy_only = (known after apply) + force_destroy = false + id = (known after apply) + location = \"US\" + name = \"de-book-terraform-test-1234567654321\" + project = (known after apply) + self_link = (known after apply) + storage_class = \"STANDARD\" + url = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Before allowing us to deploy our infrastructure, Terraform prints an execution plan with all of the actions it will take. This is especially useful when multiple engineers are working on the same infrastructure (as is often the case). If someone else has modified your infrastructure, but that change is not present in your files, you will see it in this execution plan. So be sure to actually read this plan, otherwise you could end up accidentally rolling back infrastructure your colleagues deployed. And no one wants to be that guy. But in this case, we see just the one change we were expecting: a new bucket is being added. So let's type \"yes\" and deploy our bucket: Enter a value: yes google_storage_bucket.de-book-terraform-test: Creating... google_storage_bucket.de-book-terraform-test: Creation complete after 1s [id=de-book-terraform-test-1234567654321] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Let's confirm that our bucket was created: > gsutil ls gs://de-book-terraform-test-1234567654321/ Obviously we'll be using a lot more infrastructure in production. I provide an example Terraform file with more resources listed in Appendix A . Documentation for the full list of infrastructure you can deploy is here .","title":"Deploying Your Infrastructure to GCP"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#updating-your-infrastructure","text":"We can expect our infrastructure needs to continue to change. Fortunately implementing that change is as simple as updating our Terraform file. Let's create a Pub/Sub Topic, a subscription for that Topic, and set force_destroy to true for our existing bucket. Our main.tf file now looks like: terraform { required_providers { google = { source = \"hashicorp/google\" version = \"3.5.0\" } } } provider \"google\" { credentials = file(\"../keys/de-book-dev-2d2abed79f9f.json\") project = \"de-book-dev\" region = \"us-central1\" zone = \"us-central1-c\" } resource \"google_storage_bucket\" \"de-book-terraform-test\" { name = \"de-book-terraform-test-1234567654321\" location = \"US\" force_destroy = true # Updated this to true storage_class = \"STANDARD\" } // Added the resources below resource \"google_pubsub_topic\" \"terraform-test\" { name = \"terraform-test-topic\" } resource \"google_pubsub_subscription\" \"example\" { name = \"example-subscription\" topic = google_pubsub_topic.terraform-test.name ack_deadline_seconds = 20 } Now we can apply our changes (typing \"yes\" when prompted): > terraform apply google_storage_bucket.de-book-terraform-test: Refreshing state... [id=de-book-terraform-test-1234567654321] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create ~ update in-place Terraform will perform the following actions: # google_pubsub_subscription.example will be created + resource \"google_pubsub_subscription\" \"example\" { + ack_deadline_seconds = 20 + id = (known after apply) + message_retention_duration = \"604800s\" + name = \"example-subscription\" + path = (known after apply) + project = (known after apply) + topic = \"terraform-test-topic\" + expiration_policy { + ttl = (known after apply) } } # google_pubsub_topic.terraform-test will be created + resource \"google_pubsub_topic\" \"terraform-test\" { + id = (known after apply) + name = \"terraform-test-topic\" + project = (known after apply) + message_storage_policy { + allowed_persistence_regions = (known after apply) } } # google_storage_bucket.de-book-terraform-test will be updated in-place ~ resource \"google_storage_bucket\" \"de-book-terraform-test\" { bucket_policy_only = false ~ force_destroy = false -> true id = \"de-book-terraform-test-1234567654321\" labels = {} location = \"US\" name = \"de-book-terraform-test-1234567654321\" project = \"de-book-dev\" requester_pays = false self_link = \"https://www.googleapis.com/storage/v1/b/de-book-terraform-test-1234567654321\" storage_class = \"STANDARD\" url = \"gs://de-book-terraform-test-1234567654321\" } Plan: 2 to add, 1 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes google_pubsub_topic.terraform-test: Creating... google_storage_bucket.de-book-terraform-test: Modifying... [id=de-book-terraform-test-1234567654321] google_storage_bucket.de-book-terraform-test: Modifications complete after 0s [id=de-book-terraform-test-1234567654321] google_pubsub_topic.terraform-test: Creation complete after 1s [id=projects/de-book-dev/topics/terraform-test-topic] google_pubsub_subscription.example: Creating... google_pubsub_subscription.example: Creation complete after 2s [id=projects/de-book-dev/subscriptions/example-subscription] Apply complete! Resources: 2 added, 1 changed, 0 destroyed. We see that Terraform added two resources and changed one resource, as we expected. While it is still possible to provision GCP infrastructure through other means, such as the GCP Console or command line, in practice our Terraform file or files should represent our entire infrastructure on GCP. We can remove a resource by simply deleting the resource block from our Terraform file. Let's delete out bucket: terraform { required_providers { google = { source = \"hashicorp/google\" version = \"3.5.0\" } } } provider \"google\" { credentials = file(\"../keys/de-book-dev-2d2abed79f9f.json\") project = \"de-book-dev\" region = \"us-central1\" zone = \"us-central1-c\" } // Our GCS resource used to be here resource \"google_pubsub_topic\" \"terraform-test\" { name = \"terraform-test-topic\" } resource \"google_pubsub_subscription\" \"example\" { name = \"example-subscription\" topic = google_pubsub_topic.terraform-test.name ack_deadline_seconds = 20 } Now let's apply our changes: > terraform apply google_pubsub_topic.terraform-test: Refreshing state... [id=projects/de-book-dev/topics/terraform-test-topic] google_storage_bucket.de-book-terraform-test: Refreshing state... [id=de-book-terraform-test-1234567654321] google_pubsub_subscription.example: Refreshing state... [id=projects/de-book-dev/subscriptions/example-subscription] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_storage_bucket.de-book-terraform-test will be destroyed - resource \"google_storage_bucket\" \"de-book-terraform-test\" { - bucket_policy_only = false -> null - force_destroy = true -> null - id = \"de-book-terraform-test-1234567654321\" -> null - labels = {} -> null - location = \"US\" -> null - name = \"de-book-terraform-test-1234567654321\" -> null - project = \"de-book-dev\" -> null - requester_pays = false -> null - self_link = \"https://www.googleapis.com/storage/v1/b/de-book-terraform-test-1234567654321\" -> null - storage_class = \"STANDARD\" -> null - url = \"gs://de-book-terraform-test-1234567654321\" -> null } Plan: 0 to add, 0 to change, 1 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes google_storage_bucket.de-book-terraform-test: Destroying... [id=de-book-terraform-test-1234567654321] google_storage_bucket.de-book-terraform-test: Destruction complete after 1s Apply complete! Resources: 0 added, 0 changed, 1 destroyed.","title":"Updating your Infrastructure"},{"location":"de-gcp-book/ch_10_infrastructure_as_code/#cleaning-up","text":"We used Terraform to create three resources, then destroy one of them. So we should have two resources left: > terraform state list google_pubsub_subscription.example google_pubsub_topic.terraform-test We could delete these Pub/Sub resources with the command line, as we did in Chapter 8: Streaming Data with Pub/Sub . However, we can also tell Terraform to destroy all of the resources it manages: > terraform destroy google_pubsub_topic.terraform-test: Refreshing state... [id=projects/de-book-dev/topics/terraform-test-topic] google_pubsub_subscription.example: Refreshing state... [id=projects/de-book-dev/subscriptions/example-subscription] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_pubsub_subscription.example will be destroyed - resource \"google_pubsub_subscription\" \"example\" { - ack_deadline_seconds = 20 -> null - id = \"projects/de-book-dev/subscriptions/example-subscription\" -> null - labels = {} -> null - message_retention_duration = \"604800s\" -> null - name = \"example-subscription\" -> null - path = \"projects/de-book-dev/subscriptions/example-subscription\" -> null - project = \"de-book-dev\" -> null - retain_acked_messages = false -> null - topic = \"projects/de-book-dev/topics/terraform-test-topic\" -> null - expiration_policy { - ttl = \"2678400s\" -> null } } # google_pubsub_topic.terraform-test will be destroyed - resource \"google_pubsub_topic\" \"terraform-test\" { - id = \"projects/de-book-dev/topics/terraform-test-topic\" -> null - labels = {} -> null - name = \"terraform-test-topic\" -> null - project = \"de-book-dev\" -> null } Plan: 0 to add, 0 to change, 2 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes google_pubsub_subscription.example: Destroying... [id=projects/de-book-dev/subscriptions/example-subscription] google_pubsub_subscription.example: Destruction complete after 1s google_pubsub_topic.terraform-test: Destroying... [id=projects/de-book-dev/topics/terraform-test-topic] google_pubsub_topic.terraform-test: Destruction complete after 1s Destroy complete! Resources: 2 destroyed. Next Chapter: Chapter 11: Deployment Pipelines with Cloud Build","title":"Cleaning Up"},{"location":"de-gcp-book/ch_11_deployment_pipelines/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 11: Deployment Pipelines with Cloud Build : Deployment Pipelines with Cloud Build We've been talking a lot about data pipelines in this book, but in this chapter we'll be building another type of pipeline: a deployment pipeline. A deployment pipeline is the series of steps that occur between when a developer wants to put their code in production and when it actually gets there. This is also sometimes called a \"build pipeline\" or a \"CI/CD pipeline\". The concept of continuous integration/continuous delivery (\"CI/CD\"), is often intertwined with a deployment pipeline, but they are distinct. CI/CD is a strategy that allows developers to integrate their code into production whenever it is ready, whereas a deployment pipeline is the tool used to actually get the code into production. The deployment pipeline we are building in this chapter is compatible with a CI/CD strategy. More information on CI/CD is available here . In general terms, we are going to create a deployment pipeline where anytime code is pushed into our master branch a pipeline will run automated tests then deploy the code to GCP. In this chapter we are going to: 1. Instantiate a Composer (Airflow) instance. 2. Create a GitHub repo and connect it to GCP. 3. Create some DAGs with unit tests. 4. Create a GCP trigger that will test and deploy our DAGs when they our pushed into our repo. 5. Push our code to our repo and watch Cloud Build deploy it on our behalf. Instantiate a Composer Instance As we discussed way back in Chapter 2 , it can take quite awhile for GCP to build a new Composer instance (called an \"environment\"), so it's best we start that process now for it to run while we do everything else: > gcloud composer environments create my-environment \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com Create a GitHub Repo and Connect it to GCP We'll be using GitHub in this chapter because it is the most popular (and free) code hosting platform. Cloud Build also integrates with BitBucket (still in beta as of 2021-01-20) and GCP's own Cloud Source Repositories. First, if you don't have a GitHub account you should create one . Next, let's create a repository. I'm calling mine \"cloud-build-test\". If you're not familiar with GitHub, instructions are here for creating a repository. So now that we have a GitHub repository, let's connect it to GCP. While we generally try to interact with GCP through the command line, we'll need to use the GCP console to connect our GitHub repo. First, we'll got to the triggers page and select our project. Then we'll select \"connect repository\". On the next page select \"GitHub (Cloud Build GitHub App)\" and click \"Continue\". You will be brought to a GitHub page requesting you confirm you want to grant Google Cloud Build access to your GitHub account. Click the \"Authorize Google Cloud Build\" button. Now back in the GCP console you should select the button to install the Cloud Build GitHub app. Select the repository you wish to connect and click the \"Connect repository\" button. The next page asks if you want to create a trigger. You can click to skip this step. We'll need to get our Composer/Airflow DAGs set up before we can make our trigger. That's it. Our GitHub repository is now connected to our GCP project. Writing our DAGs If we're going to demonstrate a deployment pipeline we should probably have some code to deploy, so let's write that. We're going to write some DAGs to be deployed to Composer (covered in more detail in Chapter 5 ). Because we'll be deploying to our GitHub repo, we should start by cloning the repo down to our local machine. > git clone https://github.com/Nunie123/cloud-build-test.git Cloning into 'cloud-build-test'... warning: You appear to have cloned an empty repository. > cd cloud-build-test/ Now let's build our file structure and (empty) files: > touch cloudbuild.yaml > mkdir dags > cd dags > mkdir python_scripts > mkdir tests > touch first_dag.py > touch second_dag.py > cd python_scripts > touch first_script.py > touch second_script.py > cd ../tests > touch test_python_scripts.py We now have two DAG files ( first_dag.py , second_dag.py ), two Python script files that we'll use in our DAGs ( first_script.py , second_script.py ), and one file to run unit tests on our Python scripts ( test_python_scripts.py ). Let's start adding our code to these files. Here's first_dag.py: # first_dag.py import datetime from airflow import DAG from airflow.operators.python_operator import PythonOperator from python_scripts import first_script default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'first_dag', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) t_run_first_script = PythonOperator( task_id=\"run_first_script\", python_callable=first_script.count_to_five, dag=dag ) And now second_dag.py: # second_dag.py import datetime from airflow import DAG from airflow.operators.python_operator import PythonOperator from python_scripts import second_script default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'second_dag', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) t_run_second_script = PythonOperator( task_id=\"run_second_script\", python_callable=second_script.say_hello, dag=dag ) Let's fill in our first Python script: # first_script.py def count_to_five(): count_list = [] for i in range(1, 6): count_list.append(i) return count_list And now our second script: # second_script.py def say_hello(): message = 'AHOY!' return message And finally, we'll create our test file. In this book so far I've decided that for brevity's sake we'll be skipping writing tests for our code. But your production code should absolutely be well tested. I'm including a test file here to demonstrate the good practice of having your deployment pipeline test your code before pushing it into production. # test_python_scripts.py # run from top level of repo import unittest from ..python_scripts import first_script, second_script class TestScripts(unittest.TestCase): def test_first_script(self): returned_list = first_script.count_to_five() length = len(returned_list) last_value = returned_list[-1] self.assertEqual(length, 5) self.assertEqual(last_value, 5) def test_second_script(self): returned_value = second_script.say_hello() self.assertEqual(returned_value, 'AHOY!') if __name__ == '__main__': unittest.main() Now we can run our unit tests by navigating to the top level of our repo (the cloud-build-test folder) and executing: > python -m unittest tests/test_python_scripts.py .. ---------------------------------------------------------------------- Ran 2 tests in 0.000s OK We now have a code repository with two DAGs that we want deployed to GCP and two unit tests we want to pass before we allow our code to deploy. Create a GCP Trigger Now let's create a Trigger. This Trigger will kick off a series of Bash commands to test our code, then deploy it to GCP any time our GitHub repo's master branch has been updated. To accomplish this we'll first make our Cloud Build configuration file defining the commands we want to run when a change is pushed to the master branch. As you can probably guess just by looking at the file below, we're having Cloud Build take two actions. First, we're having it run our tests, just like we did in the command line above. If that succeeds without raising an error, then we will copy (using rsync , discussed in Chapter 3 ) the contents of our dag/ folder into the GCS bucket where our Composer instance looks for DAGs. To find the bucket used for our Composer instance execute: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-environment-c4c96098-bucket/dags Now that we know the bucket we need to copy our DAGs to, we can write our Cloud Build file in the top level of our repo: # cloudbuild.yaml steps: - name: 'docker.io/library/python:3.7' id: Test entrypoint: /bin/sh args: [-c, 'python -m unittest dags/tests/test_python_scripts.py'] - name: gcr.io/google.com/cloudsdktool/cloud-sdk id: Deploy entrypoint: bash args: [ '-c', 'gsutil -m rsync -d -r ./dags gs://${_COMPOSER_BUCKET}/dags'] substitutions: _COMPOSER_BUCKET: us-central1-my-environment-c4c96098-bucket More information on creating Cloud Build files is available here . Now we need to deploy our trigger to GCP. We can do that through the command line like so: > gcloud beta builds triggers create github \\ --repo-name=cloud-build-test \\ --repo-owner=Nunie123 \\ --branch-pattern=\"master\" \\ --build-config=cloudbuild.yaml You may have noticed the beta in our gcloud command (if you are prompted to install \"beta\" components after executing the above command, do so). The beta signifies that this gcloud builds triggers command utility is still being tested by GCP developers, so should be used with caution (and best avoided in production code). Fortunately, as we discussed in Chapter 10: Infrastructure as Code with Terraform , we shouldn't be deploying our infrastructure through the command line, anyway. If we we were deploying through Terraform we would just add a resource block to our Terraform file like this: resource \"google_cloudbuild_trigger\" \"filename-trigger\" { trigger_template { branch_name = \"master\" repo_name = \"my-repo\" } substitutions = { _FOO = \"bar\" _BAZ = \"qux\" } filename = \"cloudbuild.yaml\" } Instead of having one file for our Cloud Build configuration and another file for Terraform, it's possible to combine them into a single Terraform file. More details on deploying Triggers with Terraform is available here . Pushing to GitHub Now that our Trigger is set up, all we have to do is push our code to the master branch of our GitHub repo, and it will run our deployment pipeline: > git add --all > git commit -m 'Initial commit of Cloud Build test DAGs' > git push In a production environment we would not be pushing our code from our local machine into the master branch. Particularly if you are practicing CI/CD, you'll be creating branches, doing pull requests, and merging the branches back into your master branch. This is often referred to as Git Flow . If we travel to our repo on GitHub (for me it's https://github.com/Nunie123/cloud-build-test), we can see the status of our build process: The green check mark indicates the deployment was successful (An orange dot means it is in process, and a red \"x\" means it failed). We can click on the icon to be taken to the GCP Console, where we can view more details about our build. As a final check, let's take a look at our Airflow UI and make sure our DAGs are there. Head to the GCP Composer Console , select your project, then select the link for the Airflow webserver. We can see that our DAGs are deployed and successfully run: Cleaning up For this chapter we created a Composer Environment, a bucket (created by Composer) for our DAGs, a Cloud Build Trigger, and a GitHub repository. Let's start taking them down. In this chapter we built our infrastructure using the command line for expedience. If you used Terraform to build your infrastructure (as discussed in Chapter 10: Infrastructure as Code with Terraform ), the you can run the terraform destroy command to take your GCP resources down. As I've shown in Chapters 2 and 5, we can delete the Composer Environment by running: > gcloud composer environments delete my-environment --location us-central1 It may take some time to delete the Composer environment, so you may want to open a new terminal session to clean up the rest while that runs. We already found the bucket name so we could add it to our cloudbuild.yaml file. Now let's delete it. > gsutil rm -r gs://us-central1-my-environment-c4c96098-bucket Now for something new in this book, let's find our Trigger's name, then delete it: > gcloud beta builds triggers list --- createTime: '2021-01-23T14:15:19.273898053Z' filename: cloudbuild.yaml github: name: cloud-build-test owner: Nunie123 push: branch: master id: 53042473-7323-471d-817d-a8e59939d84a name: trigger > gcloud beta builds triggers delete trigger Finally, we have our GitHub repository. We're going to need a GitHub repository for the next chapter, so feel free to leave this repo up if you'd like to reuse it. If you decide to delete it, you'll need to go to your repo's URL (e.g. https://github.com/Nunie123/cloud-build-test) and click on the \"Settings\" tab. When you scroll to the bottom of the settings page you'll see the button to delete the repository. You'll see all sorts of warnings, because deleting code by accident is a very bad thing. But if you're confident you won't need this repository anymore then go ahead and click the \"Delete this repository\" button and follow the prompts. Next Chapter: Chapter 12: Monitoring and Alerting","title":"Chapter 11: Deployment Pipelines with Cloud Build"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#chapter-11-deployment-pipelines-with-cloud-build-deployment-pipelines-with-cloud-build","text":"We've been talking a lot about data pipelines in this book, but in this chapter we'll be building another type of pipeline: a deployment pipeline. A deployment pipeline is the series of steps that occur between when a developer wants to put their code in production and when it actually gets there. This is also sometimes called a \"build pipeline\" or a \"CI/CD pipeline\". The concept of continuous integration/continuous delivery (\"CI/CD\"), is often intertwined with a deployment pipeline, but they are distinct. CI/CD is a strategy that allows developers to integrate their code into production whenever it is ready, whereas a deployment pipeline is the tool used to actually get the code into production. The deployment pipeline we are building in this chapter is compatible with a CI/CD strategy. More information on CI/CD is available here . In general terms, we are going to create a deployment pipeline where anytime code is pushed into our master branch a pipeline will run automated tests then deploy the code to GCP. In this chapter we are going to: 1. Instantiate a Composer (Airflow) instance. 2. Create a GitHub repo and connect it to GCP. 3. Create some DAGs with unit tests. 4. Create a GCP trigger that will test and deploy our DAGs when they our pushed into our repo. 5. Push our code to our repo and watch Cloud Build deploy it on our behalf.","title":"Chapter 11: Deployment Pipelines with Cloud Build: Deployment Pipelines with Cloud Build"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#instantiate-a-composer-instance","text":"As we discussed way back in Chapter 2 , it can take quite awhile for GCP to build a new Composer instance (called an \"environment\"), so it's best we start that process now for it to run while we do everything else: > gcloud composer environments create my-environment \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com","title":"Instantiate a Composer Instance"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#create-a-github-repo-and-connect-it-to-gcp","text":"We'll be using GitHub in this chapter because it is the most popular (and free) code hosting platform. Cloud Build also integrates with BitBucket (still in beta as of 2021-01-20) and GCP's own Cloud Source Repositories. First, if you don't have a GitHub account you should create one . Next, let's create a repository. I'm calling mine \"cloud-build-test\". If you're not familiar with GitHub, instructions are here for creating a repository. So now that we have a GitHub repository, let's connect it to GCP. While we generally try to interact with GCP through the command line, we'll need to use the GCP console to connect our GitHub repo. First, we'll got to the triggers page and select our project. Then we'll select \"connect repository\". On the next page select \"GitHub (Cloud Build GitHub App)\" and click \"Continue\". You will be brought to a GitHub page requesting you confirm you want to grant Google Cloud Build access to your GitHub account. Click the \"Authorize Google Cloud Build\" button. Now back in the GCP console you should select the button to install the Cloud Build GitHub app. Select the repository you wish to connect and click the \"Connect repository\" button. The next page asks if you want to create a trigger. You can click to skip this step. We'll need to get our Composer/Airflow DAGs set up before we can make our trigger. That's it. Our GitHub repository is now connected to our GCP project.","title":"Create a GitHub Repo and Connect it to GCP"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#writing-our-dags","text":"If we're going to demonstrate a deployment pipeline we should probably have some code to deploy, so let's write that. We're going to write some DAGs to be deployed to Composer (covered in more detail in Chapter 5 ). Because we'll be deploying to our GitHub repo, we should start by cloning the repo down to our local machine. > git clone https://github.com/Nunie123/cloud-build-test.git Cloning into 'cloud-build-test'... warning: You appear to have cloned an empty repository. > cd cloud-build-test/ Now let's build our file structure and (empty) files: > touch cloudbuild.yaml > mkdir dags > cd dags > mkdir python_scripts > mkdir tests > touch first_dag.py > touch second_dag.py > cd python_scripts > touch first_script.py > touch second_script.py > cd ../tests > touch test_python_scripts.py We now have two DAG files ( first_dag.py , second_dag.py ), two Python script files that we'll use in our DAGs ( first_script.py , second_script.py ), and one file to run unit tests on our Python scripts ( test_python_scripts.py ). Let's start adding our code to these files. Here's first_dag.py: # first_dag.py import datetime from airflow import DAG from airflow.operators.python_operator import PythonOperator from python_scripts import first_script default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'first_dag', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) t_run_first_script = PythonOperator( task_id=\"run_first_script\", python_callable=first_script.count_to_five, dag=dag ) And now second_dag.py: # second_dag.py import datetime from airflow import DAG from airflow.operators.python_operator import PythonOperator from python_scripts import second_script default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } dag = DAG( 'second_dag', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) t_run_second_script = PythonOperator( task_id=\"run_second_script\", python_callable=second_script.say_hello, dag=dag ) Let's fill in our first Python script: # first_script.py def count_to_five(): count_list = [] for i in range(1, 6): count_list.append(i) return count_list And now our second script: # second_script.py def say_hello(): message = 'AHOY!' return message And finally, we'll create our test file. In this book so far I've decided that for brevity's sake we'll be skipping writing tests for our code. But your production code should absolutely be well tested. I'm including a test file here to demonstrate the good practice of having your deployment pipeline test your code before pushing it into production. # test_python_scripts.py # run from top level of repo import unittest from ..python_scripts import first_script, second_script class TestScripts(unittest.TestCase): def test_first_script(self): returned_list = first_script.count_to_five() length = len(returned_list) last_value = returned_list[-1] self.assertEqual(length, 5) self.assertEqual(last_value, 5) def test_second_script(self): returned_value = second_script.say_hello() self.assertEqual(returned_value, 'AHOY!') if __name__ == '__main__': unittest.main() Now we can run our unit tests by navigating to the top level of our repo (the cloud-build-test folder) and executing: > python -m unittest tests/test_python_scripts.py .. ---------------------------------------------------------------------- Ran 2 tests in 0.000s OK We now have a code repository with two DAGs that we want deployed to GCP and two unit tests we want to pass before we allow our code to deploy.","title":"Writing our DAGs"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#create-a-gcp-trigger","text":"Now let's create a Trigger. This Trigger will kick off a series of Bash commands to test our code, then deploy it to GCP any time our GitHub repo's master branch has been updated. To accomplish this we'll first make our Cloud Build configuration file defining the commands we want to run when a change is pushed to the master branch. As you can probably guess just by looking at the file below, we're having Cloud Build take two actions. First, we're having it run our tests, just like we did in the command line above. If that succeeds without raising an error, then we will copy (using rsync , discussed in Chapter 3 ) the contents of our dag/ folder into the GCS bucket where our Composer instance looks for DAGs. To find the bucket used for our Composer instance execute: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-environment-c4c96098-bucket/dags Now that we know the bucket we need to copy our DAGs to, we can write our Cloud Build file in the top level of our repo: # cloudbuild.yaml steps: - name: 'docker.io/library/python:3.7' id: Test entrypoint: /bin/sh args: [-c, 'python -m unittest dags/tests/test_python_scripts.py'] - name: gcr.io/google.com/cloudsdktool/cloud-sdk id: Deploy entrypoint: bash args: [ '-c', 'gsutil -m rsync -d -r ./dags gs://${_COMPOSER_BUCKET}/dags'] substitutions: _COMPOSER_BUCKET: us-central1-my-environment-c4c96098-bucket More information on creating Cloud Build files is available here . Now we need to deploy our trigger to GCP. We can do that through the command line like so: > gcloud beta builds triggers create github \\ --repo-name=cloud-build-test \\ --repo-owner=Nunie123 \\ --branch-pattern=\"master\" \\ --build-config=cloudbuild.yaml You may have noticed the beta in our gcloud command (if you are prompted to install \"beta\" components after executing the above command, do so). The beta signifies that this gcloud builds triggers command utility is still being tested by GCP developers, so should be used with caution (and best avoided in production code). Fortunately, as we discussed in Chapter 10: Infrastructure as Code with Terraform , we shouldn't be deploying our infrastructure through the command line, anyway. If we we were deploying through Terraform we would just add a resource block to our Terraform file like this: resource \"google_cloudbuild_trigger\" \"filename-trigger\" { trigger_template { branch_name = \"master\" repo_name = \"my-repo\" } substitutions = { _FOO = \"bar\" _BAZ = \"qux\" } filename = \"cloudbuild.yaml\" } Instead of having one file for our Cloud Build configuration and another file for Terraform, it's possible to combine them into a single Terraform file. More details on deploying Triggers with Terraform is available here .","title":"Create a GCP Trigger"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#pushing-to-github","text":"Now that our Trigger is set up, all we have to do is push our code to the master branch of our GitHub repo, and it will run our deployment pipeline: > git add --all > git commit -m 'Initial commit of Cloud Build test DAGs' > git push In a production environment we would not be pushing our code from our local machine into the master branch. Particularly if you are practicing CI/CD, you'll be creating branches, doing pull requests, and merging the branches back into your master branch. This is often referred to as Git Flow . If we travel to our repo on GitHub (for me it's https://github.com/Nunie123/cloud-build-test), we can see the status of our build process: The green check mark indicates the deployment was successful (An orange dot means it is in process, and a red \"x\" means it failed). We can click on the icon to be taken to the GCP Console, where we can view more details about our build. As a final check, let's take a look at our Airflow UI and make sure our DAGs are there. Head to the GCP Composer Console , select your project, then select the link for the Airflow webserver. We can see that our DAGs are deployed and successfully run:","title":"Pushing to GitHub"},{"location":"de-gcp-book/ch_11_deployment_pipelines/#cleaning-up","text":"For this chapter we created a Composer Environment, a bucket (created by Composer) for our DAGs, a Cloud Build Trigger, and a GitHub repository. Let's start taking them down. In this chapter we built our infrastructure using the command line for expedience. If you used Terraform to build your infrastructure (as discussed in Chapter 10: Infrastructure as Code with Terraform ), the you can run the terraform destroy command to take your GCP resources down. As I've shown in Chapters 2 and 5, we can delete the Composer Environment by running: > gcloud composer environments delete my-environment --location us-central1 It may take some time to delete the Composer environment, so you may want to open a new terminal session to clean up the rest while that runs. We already found the bucket name so we could add it to our cloudbuild.yaml file. Now let's delete it. > gsutil rm -r gs://us-central1-my-environment-c4c96098-bucket Now for something new in this book, let's find our Trigger's name, then delete it: > gcloud beta builds triggers list --- createTime: '2021-01-23T14:15:19.273898053Z' filename: cloudbuild.yaml github: name: cloud-build-test owner: Nunie123 push: branch: master id: 53042473-7323-471d-817d-a8e59939d84a name: trigger > gcloud beta builds triggers delete trigger Finally, we have our GitHub repository. We're going to need a GitHub repository for the next chapter, so feel free to leave this repo up if you'd like to reuse it. If you decide to delete it, you'll need to go to your repo's URL (e.g. https://github.com/Nunie123/cloud-build-test) and click on the \"Settings\" tab. When you scroll to the bottom of the settings page you'll see the button to delete the repository. You'll see all sorts of warnings, because deleting code by accident is a very bad thing. But if you're confident you won't need this repository anymore then go ahead and click the \"Delete this repository\" button and follow the prompts. Next Chapter: Chapter 12: Monitoring and Alerting","title":"Cleaning up"},{"location":"de-gcp-book/ch_12_monitoring/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 12 : Monitoring and Alerting We've spent 11 chapters in this book going over how to set up your infrastructure. But that doesn't do us much good if everything breaks and you don't fix it. There will be problems, whether with your code, or a GCP outage, or a malformed file. It's our responsibility as data engineers to quickly identify when there's a problem and give ourselves as much information as we can to fix the issue. Conveniently, GCP provides dashboard for each of its services, so we can easily monitor the health of our infrastructure and make adjustments as needed. While the monitoring dashboards are automatically set up for you and can be easily customized through the console, you'll be responsible for setting up your own alerts. Consequently, this Chapter is going to be focused on setting Alerts, but you can check out GCP's Monitoring Service for setting up monitoring dashboards. In this chapter we're going to set up alerting on a Composer (Airflow) instance, and on a deployment pipeline. We'll be using Airflow's built-in alerting functionality for failed Tasks, and rolling our own alerting functionality for the Cloud Build deployment pipeline. There's lots of places you can send alerts, such as BigQuery, Pub/Sub, or GCS, but in this chapter we'll be sending our alerts to a Slack channel. While we're only covering alerting for two pieces of infrastructure in this chapter, you should set up alerting for all of your critical infrastructure in a production environment. In Chapter 13 we'll be setting up a complete infrastructure, including alerting for all our critical systems, so check that out if you want to see how alerting is done on other tools, such as DataProc. Specifically what we're going to do in this chapter is: 1. Instantiate a Composer Environment. 2. Create a Slack workspace. 3. Create a deployment pipeline. 4. Create DAGs with custom alerting. 5. Create alerting on our deployment pipeline. 6. Deploy a DAG and see how the alerts work. Many of these steps were just covered in the last chapter (Chapter 11), so I'll be brief on the details. Please refer to Chapter 11: Deployment Pipelines with Cloud Build if you want a more detailed explanation of how to set up a deployment pipeline, and Chapters 2 and 5 for more details on setting up a Composer Environment. Starting a Composer Environment This command should be pretty familiar to you by now. We do this first because it'll take awhile for GCP to set up our instance. > gcloud composer environments create my-environment \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com In a production setting we would create this Composer Environment using Terraform, discussed in Chapter 10: Infrastructure as Code with Terraform . Create a Slack Workspace We need to send the alerts somewhere that will catch the eye of the data engineering team, so Slack is a good choice. Slack has a pretty intuitive setup process, and creating a workspace is free. So go here to create your workspace. Once you've got your new slack workspace set up go here to set up incoming webhooks, which will allow you to post messages to Slack through an HTTP POST request. The URL you've just generated should be kept secret (so that random strangers can't post messages to your Slack channels). Consequently, we're going to add it to Google Secret Manager, discussed in more detail in Chapter 9: Managing Credentials with Google Secret Manager . > echo -n \"https://hooks.slack.com/services/QWERTY/ASDFG/123456\" | gcloud secrets create slack_webhook --data-file=- Replace the above string with the incoming webhook URL indicated by Slack for your account. Create a Deployment Pipeline Just like last chapter, we'll need to create a GitHub repository and link it to our GCP account. Rather than just repeating that whole section in this chapter, you should go read that section from [Chapter 11: Deployment Pipelines with Cloud Build](https://github.com/Nunie123/data_engineering_on_gcp_book/blob/master/ch_11_deployment_pipelines.md) . Now that we have our GitHub Repo connected to GCP, we need to clone the repo to our local machine: > git clone https://github.com/Nunie123/gcp_alerting_test.git Cloning into 'gcp_alerting_test'... warning: You appear to have cloned an empty repository. > cd gcp_alerting_test/ Let's build our file structure and (empty) files: > touch cloudbuild.yaml > touch requirements.txt > mkdir dags > cd dags > mkdir python_scripts > mkdir tests > touch my_dag.py > touch python_scripts/my_script.py > touch tests/test_python_scripts.py Before we can fill in our cloudbuild.yaml file we'll need the bucket name Composer will use to look for our DAGs: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-environment-9567c0a7-bucket/dags Note that you may need to wait for your Composer Environment to finish building before the above command will work. Now we can write our cloudbuild.yaml file: # cloudbuild.yaml steps: - name: 'docker.io/library/python:3.7' id: Test entrypoint: /bin/sh args: [-c, 'python -m unittest dags/tests/test_python_scripts.py'] - name: gcr.io/google.com/cloudsdktool/cloud-sdk id: Deploy entrypoint: bash args: [ '-c', 'gsutil -m rsync -d -r ./dags gs://${_COMPOSER_BUCKET}/dags'] substitutions: _COMPOSER_BUCKET: us-central1-my-environment-9567c0a7-bucket Finally, let's deploy our Cloud Build Trigger to GCP: > gcloud beta builds triggers create github \\ --repo-name=gcp_alerting_test \\ --repo-owner=Nunie123 \\ --branch-pattern=\"master\" \\ --build-config=cloudbuild.yaml Everything we did in this section was described in more detail in the previous chapter, so check out Chapter 11: Deployment Pipelines with Cloud Build if you want to know a bit more about what we just did. Create DAGs with Custom Alerting We're going to need to use the requests and google-cloud-secret-manager libraries to send messages to our Slack workspace. Let's add these packages to our Composer Environment. First, let's fill in the requirements.txt file in the top level of our repo: requests google-cloud-secret-manager Now let's update our Composer Environment: > gcloud composer environments update my-environment \\ --update-pypi-packages-from-file requirements.txt \\ --location us-central1 While you're waiting for this to complete we can carry on filling out our files. We just need this to complete before we push our code to our repo. Now that we've taken care of our Python dependencies, let's make a DAG that will run a custom alert whenever a task fails. We do this with the on_error_callback option, which can be set at the DAG or Task level and indicates a function that will be executed when a task fails. This function can do anything, but we're going to make the callback function send a message to Slack. # my_dag.py import datetime import textwrap import requests from google.cloud import secretmanager from airflow import DAG from airflow.operators.python_operator import PythonOperator from python_scripts import my_script def get_secret(project, secret_name, version): client = secretmanager.SecretManagerServiceClient() secret_path = client.secret_version_path(project, secret_name, version) secret = client.access_secret_version(secret_path) return secret.payload.data.decode(\"UTF-8\") def send_error_to_slack(context): webhook_url = get_secret('de-book-dev', 'slack_webhook', 'latest') message = textwrap.dedent(f\"\"\"\\ :red_circle: Task Failed. *Dag*: {context.get('task_instance').dag_id} *Task*: <{context.get('task_instance').log_url}|*{context.get('task_instance').task_id}*>\"\"\") message_json = dict(text=message) requests.post(webhook_url, json=message_json) default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), 'on_error_callback': send_error_to_slack, } dag = DAG( 'my_dag', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) t_run_my_script = PythonOperator( task_id=\"run_my_script\", python_callable=my_script.fail_sometimes, on_failure_callback=send_error_to_slack, dag=dag ) Let's create our Python function for our Task. We'll make it work for now, and will edit it in the Testing Our Alerts section to see what happens when an Exception is raised. # my_script.py def fail_sometimes(): # div_by_zero = 1/0 return 'No Exception!' Finally, let's create our test. Like above, we'll make the test pass for now, but will edit it to fail in the Testing Our Alerts section. # test_python_scripts.py import unittest class TestScripts(unittest.TestCase): def test_script(self): self.assertTrue(1==1) # self.assertTrue(1==0) if __name__ == '__main__': unittest.main() In this section we created alerts directly in Airflow. In the next section we'll use GCP-specific tools. Add Alerts to the Deployment Pipeline To get deployment notifications to Slack we are going to rely on Cloud Build, Pub/Sub, and Cloud Functions. The process looks like this: 1. We push code changes to our GitHub repository. 2. Cloud Build executes our tests and deploys our code to GCP. 3. Cloud Build then publishes the outcome of its actions (e.g. success or failure of the deployment) to a Pub/Sub Topic. 4. This triggers a Cloud Function which sends our alerts to Slack. The only magic in this process is Cloud Build automatically publishing to a Pub/Sub topic. We have to build all the other steps, but we've already covered all these tools previously in this book. We already set up Cloud Build, above, so let's create our Pub/Sub Topic. I cover Pub/Sub topics in more detail in Chapter 8: Streaming Data with Pub/Sub . > gcloud pubsub topics create cloud-builds The name of the Topic must be cloud-builds so that Cloud Build knows where to publish. Now for the last piece, our Cloud Function. I covered Cloud Functions in more detail in Chapter 6 . At the top level of the repository we created for our Airflow DAGs, lets create our function files: > mkdir functions > mkdir functions/send-to-slack > touch functions/send-to-slack/main.py > touch functions/send-to-slack/requirements.txt Now we are basically re-implementing the alerting function we used for our Airflow Task, above. We'll start with our requirements file, as we'll need Secrets Manager to get the Slack endpoint, and we'll need the requests library to send our HTTP Post request. requirements.txt: requests google-cloud-secret-manager main.py: def send_to_slack(event, context): import requests from google.cloud import secretmanager client = secretmanager.SecretManagerServiceClient() name = \"projects/204024561480/secrets/slack_webhook/versions/latest\" response = client.access_secret_version(request={\"name\": name}) webhook_url = response.payload.data.decode(\"UTF-8\") build_status = event.get('attributes').get('status') build_id = event.get('attributes').get('buildId') if build_status == 'SUCCESS': slack_text = f':large_green_circle: Cloud Build Success for {build_id}' elif build_status == 'FAILURE':: slack_text = f':red_circle: ATTENTION! Cloud Build {build_id} did not succeed. Status is {build_status}.' slack_message = dict(text=slack_text) requests.post(webhook_url, json=slack_message) To get the fully specified name of your secret ( projects/204024561480/secrets/slack_webhook/versions/latest above) execute: > gcloud secrets versions describe latest --secret=slack_webhook createTime: '2021-02-01T01:07:29.214017Z' name: projects/204024561480/secrets/slack_webhook/versions/1 replicationStatus: automatic: {} state: ENABLED Now let's deploy our function. In a production environment we would just add another step to our cloudbuild.yaml file we created above, but we want to see our alerts on our first push to our repo, so we'll deploy them manually right now. Navigate to the send-to-slack folder and execute: > gcloud functions deploy send_to_slack \\ --runtime python37 \\ --trigger-topic cloud-builds \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com I used my composer-dev@de-book-dev.iam.gserviceaccount.com service account I created back in Chapter 1. You can find yours by executing: > gcloud iam service-accounts list DISPLAY NAME EMAIL DISABLED composer-dev composer-dev@de-book-dev.iam.gserviceaccount.com False Compute Engine default service account 204024561480-compute@developer.gserviceaccount.com False Because our Cloud Function will be accessing Secret Manager we'll need to make sure out service account can also access secrets. > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/secretmanager.admin' That's it. Deployment messages will now be sent to our Slack workspace. In the next section we'll see it in action. Testing our Alerts Now we have two alerts set up. One will send an alert if a Task fails. The other will send an alert whenever there's a deployment. So let's start by deploying our code and seeing a Slack alert notifying us whether the deploy was successful. As we talked about in Chapter 11: Deployment Pipelines with Cloud Build , we kick off our deploy process by pushing our code to GitHub: > git status > git add --all > git commit -m 'Initial commit of alerting test code' > git push It'll take a moment for your deployment process to finish. You can monitor your deployment process from your repo in GitHub, or from the GCP Cloud Build console. But pretty soon you'll be getting a message letting you know your deployment succeeded. Let's update our unit test so that our deployment fails: # test_python_scripts.py import unittest class TestScripts(unittest.TestCase): def test_script(self): self.assertTrue(1==1) self.assertTrue(1==0) # This will cause an exception if __name__ == '__main__': unittest.main() Now when we deploy our code we will see a failure notification in Slack: > git status > git add --all > git commit -m 'Making unit test fail' > git push We are now alerted when our deployments succeed or fail. Let's test our Airflow alerting for when a Task fails. First we'll fix our unit test: # test_python_scripts.py import unittest class TestScripts(unittest.TestCase): def test_script(self): self.assertTrue(1==1) # self.assertTrue(1==0) if __name__ == '__main__': unittest.main() Now lets update the script our DAG runs so that it raises an exception: # my_script.py def fail_sometimes(): div_by_zero = 1/0 # This will raise an exception return 'No Exception!' Let's deploy our changes: > git status > git add --all > git commit -m 'Making Airflow Task fail' > git push Now let's visit our Airflow Web UI to trigger our DAG. We can find the web address by executing: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.airflowUri)\" https://ic1434f8836d84236p-tp.appspot.com Let's trigger the DAG (check out Chapter 5 for more details on interacting with the Airflow UI). Our task will retry once. When it fails on it's second try it will send us an alert in Slack. Wrapping Up We did a lot in this chapter. We set up a Composer Environment, a DAG, a Cloud Function, a Cloud Build Trigger, a Secret, and a Pub/Sub Topic. Hopefully you feel pretty good about how your knowledge has grown over this book such that you're able to tie multiple GCP services together to build your infrastructure. This is the last chapter in this book where we will be introducing new tools. While there is a lot more we could have talked about on GCP, you should now have a good understanding of the major components of a typical data engineering stack, and how to stand those components up using GCP. The next chapter, Chapter 13 , will go over how to get a complete Data Engineering infrastructure up and running. Cleaning Up We used a lot of different GCP services in this chapter, so let's start taking them down. We'll begin with finding the bucket for our Composer Environment and deleting it: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-environment-9567c0a7-bucket/dags > gsutil rm -r gs://us-central1-my-environment-9567c0a7-bucket Now we can delete the Composer Environment: > gcloud composer environments delete my-environment --location us-central1 That's going to take awhile, so let's go to a new terminal session and delete our Cloud Function: > gcloud functions delete send_to_slack We can delete our Pub/Sub Topic: > gcloud pubsub topics delete cloud-builds And our Secret: > gcloud secrets delete slack_webhook Finally, let's delete our Cloud Build Trigger: > gcloud beta builds triggers list --- createTime: '2021-01-23T14:15:19.273898053Z' filename: cloudbuild.yaml github: name: cloud-build-test owner: Nunie123 push: branch: master id: 53042473-7323-471d-817d-a8e59939d84a name: trigger > gcloud beta builds triggers delete trigger In Chapter 11: Deployment Pipelines with Cloud Build I showed you how to delete your GitHub repository. See the instructions there if you want to delete the repo we made in this chapter. Next Chapter: Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure","title":"Chapter 12: Monitoring and Alerting"},{"location":"de-gcp-book/ch_12_monitoring/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_12_monitoring/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_12_monitoring/#chapter-12-monitoring-and-alerting","text":"We've spent 11 chapters in this book going over how to set up your infrastructure. But that doesn't do us much good if everything breaks and you don't fix it. There will be problems, whether with your code, or a GCP outage, or a malformed file. It's our responsibility as data engineers to quickly identify when there's a problem and give ourselves as much information as we can to fix the issue. Conveniently, GCP provides dashboard for each of its services, so we can easily monitor the health of our infrastructure and make adjustments as needed. While the monitoring dashboards are automatically set up for you and can be easily customized through the console, you'll be responsible for setting up your own alerts. Consequently, this Chapter is going to be focused on setting Alerts, but you can check out GCP's Monitoring Service for setting up monitoring dashboards. In this chapter we're going to set up alerting on a Composer (Airflow) instance, and on a deployment pipeline. We'll be using Airflow's built-in alerting functionality for failed Tasks, and rolling our own alerting functionality for the Cloud Build deployment pipeline. There's lots of places you can send alerts, such as BigQuery, Pub/Sub, or GCS, but in this chapter we'll be sending our alerts to a Slack channel. While we're only covering alerting for two pieces of infrastructure in this chapter, you should set up alerting for all of your critical infrastructure in a production environment. In Chapter 13 we'll be setting up a complete infrastructure, including alerting for all our critical systems, so check that out if you want to see how alerting is done on other tools, such as DataProc. Specifically what we're going to do in this chapter is: 1. Instantiate a Composer Environment. 2. Create a Slack workspace. 3. Create a deployment pipeline. 4. Create DAGs with custom alerting. 5. Create alerting on our deployment pipeline. 6. Deploy a DAG and see how the alerts work. Many of these steps were just covered in the last chapter (Chapter 11), so I'll be brief on the details. Please refer to Chapter 11: Deployment Pipelines with Cloud Build if you want a more detailed explanation of how to set up a deployment pipeline, and Chapters 2 and 5 for more details on setting up a Composer Environment.","title":"Chapter 12: Monitoring and Alerting"},{"location":"de-gcp-book/ch_12_monitoring/#starting-a-composer-environment","text":"This command should be pretty familiar to you by now. We do this first because it'll take awhile for GCP to set up our instance. > gcloud composer environments create my-environment \\ --location us-central1 \\ --zone us-central1-f \\ --machine-type n1-standard-1 \\ --image-version composer-1.12.2-airflow-1.10.10 \\ --python-version 3 \\ --node-count 3 \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com In a production setting we would create this Composer Environment using Terraform, discussed in Chapter 10: Infrastructure as Code with Terraform .","title":"Starting a Composer Environment"},{"location":"de-gcp-book/ch_12_monitoring/#create-a-slack-workspace","text":"We need to send the alerts somewhere that will catch the eye of the data engineering team, so Slack is a good choice. Slack has a pretty intuitive setup process, and creating a workspace is free. So go here to create your workspace. Once you've got your new slack workspace set up go here to set up incoming webhooks, which will allow you to post messages to Slack through an HTTP POST request. The URL you've just generated should be kept secret (so that random strangers can't post messages to your Slack channels). Consequently, we're going to add it to Google Secret Manager, discussed in more detail in Chapter 9: Managing Credentials with Google Secret Manager . > echo -n \"https://hooks.slack.com/services/QWERTY/ASDFG/123456\" | gcloud secrets create slack_webhook --data-file=- Replace the above string with the incoming webhook URL indicated by Slack for your account.","title":"Create a Slack Workspace"},{"location":"de-gcp-book/ch_12_monitoring/#create-a-deployment-pipeline","text":"Just like last chapter, we'll need to create a GitHub repository and link it to our GCP account. Rather than just repeating that whole section in this chapter, you should go read that section from [Chapter 11: Deployment Pipelines with Cloud Build](https://github.com/Nunie123/data_engineering_on_gcp_book/blob/master/ch_11_deployment_pipelines.md) . Now that we have our GitHub Repo connected to GCP, we need to clone the repo to our local machine: > git clone https://github.com/Nunie123/gcp_alerting_test.git Cloning into 'gcp_alerting_test'... warning: You appear to have cloned an empty repository. > cd gcp_alerting_test/ Let's build our file structure and (empty) files: > touch cloudbuild.yaml > touch requirements.txt > mkdir dags > cd dags > mkdir python_scripts > mkdir tests > touch my_dag.py > touch python_scripts/my_script.py > touch tests/test_python_scripts.py Before we can fill in our cloudbuild.yaml file we'll need the bucket name Composer will use to look for our DAGs: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-environment-9567c0a7-bucket/dags Note that you may need to wait for your Composer Environment to finish building before the above command will work. Now we can write our cloudbuild.yaml file: # cloudbuild.yaml steps: - name: 'docker.io/library/python:3.7' id: Test entrypoint: /bin/sh args: [-c, 'python -m unittest dags/tests/test_python_scripts.py'] - name: gcr.io/google.com/cloudsdktool/cloud-sdk id: Deploy entrypoint: bash args: [ '-c', 'gsutil -m rsync -d -r ./dags gs://${_COMPOSER_BUCKET}/dags'] substitutions: _COMPOSER_BUCKET: us-central1-my-environment-9567c0a7-bucket Finally, let's deploy our Cloud Build Trigger to GCP: > gcloud beta builds triggers create github \\ --repo-name=gcp_alerting_test \\ --repo-owner=Nunie123 \\ --branch-pattern=\"master\" \\ --build-config=cloudbuild.yaml Everything we did in this section was described in more detail in the previous chapter, so check out Chapter 11: Deployment Pipelines with Cloud Build if you want to know a bit more about what we just did.","title":"Create a Deployment Pipeline"},{"location":"de-gcp-book/ch_12_monitoring/#create-dags-with-custom-alerting","text":"We're going to need to use the requests and google-cloud-secret-manager libraries to send messages to our Slack workspace. Let's add these packages to our Composer Environment. First, let's fill in the requirements.txt file in the top level of our repo: requests google-cloud-secret-manager Now let's update our Composer Environment: > gcloud composer environments update my-environment \\ --update-pypi-packages-from-file requirements.txt \\ --location us-central1 While you're waiting for this to complete we can carry on filling out our files. We just need this to complete before we push our code to our repo. Now that we've taken care of our Python dependencies, let's make a DAG that will run a custom alert whenever a task fails. We do this with the on_error_callback option, which can be set at the DAG or Task level and indicates a function that will be executed when a task fails. This function can do anything, but we're going to make the callback function send a message to Slack. # my_dag.py import datetime import textwrap import requests from google.cloud import secretmanager from airflow import DAG from airflow.operators.python_operator import PythonOperator from python_scripts import my_script def get_secret(project, secret_name, version): client = secretmanager.SecretManagerServiceClient() secret_path = client.secret_version_path(project, secret_name, version) secret = client.access_secret_version(secret_path) return secret.payload.data.decode(\"UTF-8\") def send_error_to_slack(context): webhook_url = get_secret('de-book-dev', 'slack_webhook', 'latest') message = textwrap.dedent(f\"\"\"\\ :red_circle: Task Failed. *Dag*: {context.get('task_instance').dag_id} *Task*: <{context.get('task_instance').log_url}|*{context.get('task_instance').task_id}*>\"\"\") message_json = dict(text=message) requests.post(webhook_url, json=message_json) default_args = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), 'on_error_callback': send_error_to_slack, } dag = DAG( 'my_dag', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=default_args ) t_run_my_script = PythonOperator( task_id=\"run_my_script\", python_callable=my_script.fail_sometimes, on_failure_callback=send_error_to_slack, dag=dag ) Let's create our Python function for our Task. We'll make it work for now, and will edit it in the Testing Our Alerts section to see what happens when an Exception is raised. # my_script.py def fail_sometimes(): # div_by_zero = 1/0 return 'No Exception!' Finally, let's create our test. Like above, we'll make the test pass for now, but will edit it to fail in the Testing Our Alerts section. # test_python_scripts.py import unittest class TestScripts(unittest.TestCase): def test_script(self): self.assertTrue(1==1) # self.assertTrue(1==0) if __name__ == '__main__': unittest.main() In this section we created alerts directly in Airflow. In the next section we'll use GCP-specific tools.","title":"Create DAGs with Custom Alerting"},{"location":"de-gcp-book/ch_12_monitoring/#add-alerts-to-the-deployment-pipeline","text":"To get deployment notifications to Slack we are going to rely on Cloud Build, Pub/Sub, and Cloud Functions. The process looks like this: 1. We push code changes to our GitHub repository. 2. Cloud Build executes our tests and deploys our code to GCP. 3. Cloud Build then publishes the outcome of its actions (e.g. success or failure of the deployment) to a Pub/Sub Topic. 4. This triggers a Cloud Function which sends our alerts to Slack. The only magic in this process is Cloud Build automatically publishing to a Pub/Sub topic. We have to build all the other steps, but we've already covered all these tools previously in this book. We already set up Cloud Build, above, so let's create our Pub/Sub Topic. I cover Pub/Sub topics in more detail in Chapter 8: Streaming Data with Pub/Sub . > gcloud pubsub topics create cloud-builds The name of the Topic must be cloud-builds so that Cloud Build knows where to publish. Now for the last piece, our Cloud Function. I covered Cloud Functions in more detail in Chapter 6 . At the top level of the repository we created for our Airflow DAGs, lets create our function files: > mkdir functions > mkdir functions/send-to-slack > touch functions/send-to-slack/main.py > touch functions/send-to-slack/requirements.txt Now we are basically re-implementing the alerting function we used for our Airflow Task, above. We'll start with our requirements file, as we'll need Secrets Manager to get the Slack endpoint, and we'll need the requests library to send our HTTP Post request. requirements.txt: requests google-cloud-secret-manager main.py: def send_to_slack(event, context): import requests from google.cloud import secretmanager client = secretmanager.SecretManagerServiceClient() name = \"projects/204024561480/secrets/slack_webhook/versions/latest\" response = client.access_secret_version(request={\"name\": name}) webhook_url = response.payload.data.decode(\"UTF-8\") build_status = event.get('attributes').get('status') build_id = event.get('attributes').get('buildId') if build_status == 'SUCCESS': slack_text = f':large_green_circle: Cloud Build Success for {build_id}' elif build_status == 'FAILURE':: slack_text = f':red_circle: ATTENTION! Cloud Build {build_id} did not succeed. Status is {build_status}.' slack_message = dict(text=slack_text) requests.post(webhook_url, json=slack_message) To get the fully specified name of your secret ( projects/204024561480/secrets/slack_webhook/versions/latest above) execute: > gcloud secrets versions describe latest --secret=slack_webhook createTime: '2021-02-01T01:07:29.214017Z' name: projects/204024561480/secrets/slack_webhook/versions/1 replicationStatus: automatic: {} state: ENABLED Now let's deploy our function. In a production environment we would just add another step to our cloudbuild.yaml file we created above, but we want to see our alerts on our first push to our repo, so we'll deploy them manually right now. Navigate to the send-to-slack folder and execute: > gcloud functions deploy send_to_slack \\ --runtime python37 \\ --trigger-topic cloud-builds \\ --service-account composer-dev@de-book-dev.iam.gserviceaccount.com I used my composer-dev@de-book-dev.iam.gserviceaccount.com service account I created back in Chapter 1. You can find yours by executing: > gcloud iam service-accounts list DISPLAY NAME EMAIL DISABLED composer-dev composer-dev@de-book-dev.iam.gserviceaccount.com False Compute Engine default service account 204024561480-compute@developer.gserviceaccount.com False Because our Cloud Function will be accessing Secret Manager we'll need to make sure out service account can also access secrets. > gcloud projects add-iam-policy-binding 'de-book-dev' \\ --member='serviceAccount:composer-dev@de-book-dev.iam.gserviceaccount.com' \\ --role='roles/secretmanager.admin' That's it. Deployment messages will now be sent to our Slack workspace. In the next section we'll see it in action.","title":"Add Alerts to the Deployment Pipeline"},{"location":"de-gcp-book/ch_12_monitoring/#testing-our-alerts","text":"Now we have two alerts set up. One will send an alert if a Task fails. The other will send an alert whenever there's a deployment. So let's start by deploying our code and seeing a Slack alert notifying us whether the deploy was successful. As we talked about in Chapter 11: Deployment Pipelines with Cloud Build , we kick off our deploy process by pushing our code to GitHub: > git status > git add --all > git commit -m 'Initial commit of alerting test code' > git push It'll take a moment for your deployment process to finish. You can monitor your deployment process from your repo in GitHub, or from the GCP Cloud Build console. But pretty soon you'll be getting a message letting you know your deployment succeeded. Let's update our unit test so that our deployment fails: # test_python_scripts.py import unittest class TestScripts(unittest.TestCase): def test_script(self): self.assertTrue(1==1) self.assertTrue(1==0) # This will cause an exception if __name__ == '__main__': unittest.main() Now when we deploy our code we will see a failure notification in Slack: > git status > git add --all > git commit -m 'Making unit test fail' > git push We are now alerted when our deployments succeed or fail. Let's test our Airflow alerting for when a Task fails. First we'll fix our unit test: # test_python_scripts.py import unittest class TestScripts(unittest.TestCase): def test_script(self): self.assertTrue(1==1) # self.assertTrue(1==0) if __name__ == '__main__': unittest.main() Now lets update the script our DAG runs so that it raises an exception: # my_script.py def fail_sometimes(): div_by_zero = 1/0 # This will raise an exception return 'No Exception!' Let's deploy our changes: > git status > git add --all > git commit -m 'Making Airflow Task fail' > git push Now let's visit our Airflow Web UI to trigger our DAG. We can find the web address by executing: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.airflowUri)\" https://ic1434f8836d84236p-tp.appspot.com Let's trigger the DAG (check out Chapter 5 for more details on interacting with the Airflow UI). Our task will retry once. When it fails on it's second try it will send us an alert in Slack.","title":"Testing our Alerts"},{"location":"de-gcp-book/ch_12_monitoring/#wrapping-up","text":"We did a lot in this chapter. We set up a Composer Environment, a DAG, a Cloud Function, a Cloud Build Trigger, a Secret, and a Pub/Sub Topic. Hopefully you feel pretty good about how your knowledge has grown over this book such that you're able to tie multiple GCP services together to build your infrastructure. This is the last chapter in this book where we will be introducing new tools. While there is a lot more we could have talked about on GCP, you should now have a good understanding of the major components of a typical data engineering stack, and how to stand those components up using GCP. The next chapter, Chapter 13 , will go over how to get a complete Data Engineering infrastructure up and running.","title":"Wrapping Up"},{"location":"de-gcp-book/ch_12_monitoring/#cleaning-up","text":"We used a lot of different GCP services in this chapter, so let's start taking them down. We'll begin with finding the bucket for our Composer Environment and deleting it: > gcloud composer environments describe my-environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-my-environment-9567c0a7-bucket/dags > gsutil rm -r gs://us-central1-my-environment-9567c0a7-bucket Now we can delete the Composer Environment: > gcloud composer environments delete my-environment --location us-central1 That's going to take awhile, so let's go to a new terminal session and delete our Cloud Function: > gcloud functions delete send_to_slack We can delete our Pub/Sub Topic: > gcloud pubsub topics delete cloud-builds And our Secret: > gcloud secrets delete slack_webhook Finally, let's delete our Cloud Build Trigger: > gcloud beta builds triggers list --- createTime: '2021-01-23T14:15:19.273898053Z' filename: cloudbuild.yaml github: name: cloud-build-test owner: Nunie123 push: branch: master id: 53042473-7323-471d-817d-a8e59939d84a name: trigger > gcloud beta builds triggers delete trigger In Chapter 11: Deployment Pipelines with Cloud Build I showed you how to delete your GitHub repository. See the instructions there if you want to delete the repo we made in this chapter. Next Chapter: Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure","title":"Cleaning Up"},{"location":"de-gcp-book/ch_13_up_and_running/","text":"Up and Running: Data Engineering on the Google Cloud Platform The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section . Table of Contents Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository Chapter 13 : Up and Running - Building a Complete Data Engineering Infrastructure In the previous 12 chapters you learned: * how to set up batch and streaming data pipelines * how to orchestrate your pipelines * how to transform your data * how to set up your data warehouse * how to deploy your code * how to set up infrastructure as code * how to monitor your infrastructure In this chapter we are going to cover all of that in one go: setting up a complete data engineering infrastructure for a fictional company. I'm not going to go into as much detail for each piece we are going to set up, as we already covered the details in previous chapters. Instead, this chapter will walk you through the steps you would take if you needed to get a complete infrastructure up and running on GCP. I mentioned this in the introduction, but I'll repeat it here: what is covered in this book is not THE data engineering stack, it is A data engineering stack. Even within GCP there are lots of ways to accomplish similar tasks. For example, we could have used Dataflow (based on Apache Beam) for our streaming solution. Or we could have gone with a completely different paradigm for how we store and query our data, such as storing our data as Parquet files in GCS and querying with Spark. I mention this here to make sure you understand that this book is not the complete guide to being a data engineer. Rather, it is an introduction to the types of problems a data engineer solves, and a sampling common tools in GCP used to solve those problems. You can view the code we set up in this chapter in its final form in Appendix A , which is a sample code repository. This is going to be quite a long chapter, so here are the links for each section: * Your Requirements * Setting Up a New Project * Mocking Hamsterwheel's Source Systems * Instantiating Your Infrastructure with Terraform * Writing Your Batch Pipeline with Composer/Airflow * Writing Your Event-Driven Pipelines with Cloud Functions and Dataproc/Spark * Writing your Streaming Pipeline with Pub/Sub, Cloud Functions, and GKE * Deploying Your Code with Cloud Build * Final Thoughts * Cleaning Up Your Requirements Welcome Hamsterwheel Batteries Inc., an up-and-coming battery retailer, where you are the newest (and only) data engineer for the company. Our analysts have decided that emailing Excel files to each other is no longer meeting their needs, so it's your job to set up a data warehouse and the various pipelines to feed that data. You'll need to bring in data from a variety of data sources, including: * Web APIs * CSV and JSON files periodically updated in GCS * Batch data from the database supporting Hamsterwheel's website * Streaming data from the database supporting Hamsterwheel's website You'll need to store that data in the data lake and data warehouse you'll set up. You'll then need to transform that data into a format that is easy for our analysts to use. Setting Up a New Project So far in this book I've been working in de-book-dev project. In this chapter I'm going to build a production environment, so I'll make a new de-book-prod project. Keeping your environments in different projects is a good idea. For this chapter it's not strictly necessary, but it might be a good idea in case you have an lingering infrastructure on an existing project that may make it harder to track what you're building. Setting up a new project was discussed in Chapter 1. We're also going to create a new GitHub repository for our code. I went over that in Chapter 11: Deployment Pipelines with Cloud Build . You can see mine here: https://github.com/Nunie123/hamsterwheel, though the same code is available in Appendix A . Now that we have a new project and a new repo, let's create a new service account and save our key in a top-level keys/ folder in our repo. I discussed how to do this in Chapter 1. Because this code is going to be published to a remote repository we need to make sure our key file doesn't get made public. let's add a .gitignore file at the top level of our repo: keys __pycache__ .vscode .DS_Store The three entries below keys we aren't excluding because they are private, but rather because they contain cached data and settings that are not valuable to have in version control. Finally, let's make sure we have the right account initialized with our new project set to default: > gcloud init If you don't have gcloud or the other GCP command line tools installed, check out Chapter 1. Mocking Hamsterwheel's Source Systems Before we can dive into building our data pipelines we're going to need some source data to ingest. Because we don't have access to an actual company's data sources we are going to mock up some of our own. Yes, it does seem a bit silly to generate data in one place just so we can move it to another place. But by doing this we can actually see our infrastructure working, rather than building it and just taking my word that it would work. In this section we will set up the systems and data we need to mock real data sources. Web API For your Web API there's no fake data needed, we'll use ComEd's energy pricing API available here: https://hourlypricing.comed.com/api?type=5minutefeed CSV File For our CSV files, let's suppose those are generated by the marketing department as potential leads for new customers. Hamsterwheel's marketing department puts a new file into a GCS bucket every morning. Let's start by making the CSV file using this Python script: # sale_leads.py import csv from faker import Faker def generate_file() -> None: fake = Faker() with open('sale_leads.csv', 'w') as f: writer = csv.writer(f, delimiter=',') writer.writerow(['name', 'phone_number']) for _ in range(500): writer.writerow([fake.name(), fake.phone_number()]) if __name__ == '__main__': generate_file() faker is a Python library for generating fake data. It is not part of the standard library, so you'll need to install it ( pip install Faker ). We're also going to need the Spanner Python library, so lets install that now as well. These should be installed inside a Python virtual environment, using the tool of your choice (e.g. venv . > pip install Faker > pip install google-cloud-spanner Now let's generate the CSV file by executing the script: > python sale_leads.py We can verify the output looks right: > head -n 5 sale_leads.csv name,phone_number Brittany Walker,+1-152-651-8997x12442 Felicia Coleman,245.551.7982 Crystal Bell,803-316-1054x535 James Morris,354-636-3746 We need this file in a bucket (as if put there by Hamsterwheel's marketing department), so lets create a bucket and copy over the file: > gsutil mb gs://de-book-source > gsutil cp sale_leads.csv gs://de-book-source/marketing/ Note that the bucket I created I called de-book-source , but that name will likely not be available to you since all bucket names are unique on GCS. Choose whatever name you like. JSON File For our JSON files, let's suppose Hamsterwheel runs a web scraper to get information on competitor products, and deposits a new file in a GCS bucket every evening after the script finishes running. We'll again use a python script to generate this file: # competitor_products.py import json import random from faker import Faker def generate_file() -> None: fake = Faker() data = [] for _ in range(500): product = dict( company=fake.company(), product_name=' '.join(fake.words(2)), in_stock=fake.boolean(), sku=fake.ean(), price=random.randrange(500,20000)/100, product_groups=[[random.randrange(1,500), random.randrange(1,500)], [random.randrange(1,500), random.randrange(1,500)]] ) data.append(product) with open('competitor_products.json', 'w') as f: json.dump(data, f) if __name__ == '__main__': generate_file() We can execute the script to generate the file, verify the contents, then copy the file to our GCS bucket: > python competitor_products.py > head -c 500 competitor_products.json > gsutil cp competitor_products.json gs://de-book-source/scraper/ Company Database We have a Web API and a couple flat files, now for the hard part. We're going to set up a relational database to simulate Hamsterwheel's transactional database that tracks customer data and sales. We are going to use Spanner , a fully managed relational database service on GCP to create a customers table and a sales table. We'll do a one-time load into our customer table, and set up a script to periodically add sales data to the sales table, which we'll use for our streaming pipeline. We'll start by specifying our tables in Data Definition Language (DDL) : -- hamsterwheel_db.sql CREATE TABLE customers ( customer_id STRING(36) NOT NULL, customer_name STRING(100) NOT NULL, address STRING(1000) NULL, phone_number STRING(20) NULL, is_active BOOL NOT NULL AS TRUE ) PRIMARY KEY (customer_id) ; CREATE TABLE sales( sale_id STRING(36) NOT NULL, sale_price FLOAT64 NOT NULL, sale_timestamp TIMESTAMP NOT NULL, customer_id STRING(36) NOT NULL ) PRIMARY KEY (sale_id) , FOREIGN KEY (customer_id) REFERENCES customers (customer_id) ; Now we can instantiate the database: > gcloud spanner databases create hamsterwheel-db \\ --instance=hamsterwheel-instance \\ --ddl-file=hamsterwheel_db.sql We have our tables set up, so now we can insert some data. We're going to use some Python scripts to generate and insert our data. # customer_data.py from faker import Faker from google.cloud import spanner fake = Faker() def generate_insert_statement() -> str: sql_list = ['insert into customers(customer_id, customer_name, address, phone_number, is_active)'] sql_list.append('values') for i in range(1, 501): row = f'({i}, {fake.name()}, {fake.address()}, {fake.phone_number()}, {fake.boolean})' sql_list.append(row) sql_statement = '\\n'.join(sql_list) return sql_statement def insert_customer_records_callback(transaction) -> None: sql_statement = generate_insert_statement() row_ct = transaction.execute_update(sql_statement) print(f\"{row_ct} records inserted.\".) def insert_custom_records(): spanner_client = spanner.Client() instance = spanner_client.instance('hamsterwheel-instance') database = instance.database('hamsterwheel-db') database.run_in_transaction(insert_customer_records_callback) if __name__ == '__main__': insert_customer_records() Let's execute it: > python customer_data.py For sales data, we want to set up a long-running Python script that will periodically be adding new records: # sales_data.py import uuid import random import datetime import time from google.cloud import spanner def generate_insert_statement() -> str: sql_list = ['insert into sales(sale_id, sale_price, sale_timestamp, customer_id)'] sql_list.append('values') row = f'({uuid.uuid4()}, {random.randrange(500,20000)/100}, {datetime.datetime.now()}, {random.randint(1,500)})' sql_list.append(row) sql_statement = '\\n'.join(sql_list) return sql_statement def insert_product_record_callback(transaction) -> None: sql_statement = generate_insert_statement() row_ct = transaction.execute_update(sql_statement) print(f\"{row_ct} record inserted.\".) def insert_product_records_over_time(): spanner_client = spanner.Client() instance = spanner_client.instance('hamsterwheel-instance') database = instance.database('hamsterwheel-db') while i < 240: database.run_in_transaction(insert_product_record_callback) i = i + 1 sleep_time = random.randint(20,40) time.sleep(sleep_time) if __name__ == '__main__': insert_product_records_over_time() > python sales_data.py This script is set up to run for about two hours. We can just leave it alone and open a new terminal session to use for the rest of the chapter. Finally, let's make sure our data is in our tables: > gcloud spanner databases execute-sql hamsterwheel-db \\ --instance=hamsterwheel-instance \\ --sql=\"select * from customers limit 20\" > gcloud spanner databases execute-sql hamsterwheel-db \\ --instance=hamsterwheel-instance \\ --sql=\"select * from sales limit 20\" We now have all the data sources we need to set up realistic data engineering infrastructure. So let's get started. Instantiating Your Infrastructure with Terraform As the sole data engineer at Hamsterwheel Batteries Inc. it's your job to maintain all of the data engineering infrastructure. That includes debugging infrastructure failures and recovering from disasters (e.g. a backhoe severs the internet connection of the GCP location where your infrastructure is running). Both of those tasks (debugging, disaster recovery) get a lot easier if you know exactly what infrastructure you are running at any given time. As we talked about in Chapter 10: Infrastructure as Code with Terraform , we can solve those problems by adopting Infrastructure as Code with Terraform. We installed Terraform in Chapter 10: Infrastructure as Code with Terraform . If you don't have it installed refer to Chapter 10: Infrastructure as Code with Terraform or follow the online instructions . Now we need to plan out what infrastructure we are going to need: * You are presenting your data to users in a data warehouse, so you'll need BigQuery. * You need a data lake to store your raw data, so you'll need Google Cloud Storage (GCS). * You need to ingest data files once they are loaded into a GCS bucket, so you'll need Cloud Functions. * You need to transform your data before loading it into BigQuery, so you'll need Dataproc/Spark. * You need to ingest data at regular intervals from a web API and a database, so you'll need Composer/Airflow. * You need to stream data from a database into BigQuery, so you'll need a GKE cluster for querying the database, Pub/Sub for queueing the data, and a Cloud Function to insert it into BigQuery. * You need a deployment pipeline to get your code into GCP, so you'll need Cloud Build. * You need to make sure the services you are using have permission to do the things they need and no more, so you'll need to set up service accounts. This is a good list to get you started. It's easy enough to change your infrastructure as you go (another benefit of Terraform). Now let's get down to writing our Terraform file. You'll start by creating a terraform folder at the top level of your repo, and while you're at ti you can create your main.tf file. > mkdir terraform > touch terraform/main.tf And because you don't want any unhelpful terraform cache files cluttering up your repo, you'll update your .gitignore file: keys __pycache__ .vscode .DS_Store .tfstate .terraform .tfplan Now we are ready to fill in main.tf: That was a lot. As your infrastructure grows you can split this Terraform file into multiple files (e.g. a single file for BigQuery). You can also take advantage of Terraform's modules to reduce boilerplate code. Another optimization is to take advantage of Terraform variables to have the same Terraform code instantiate multiple environments (e.g. Dev and Prod). Now that you have your infrastructure defined, you need to deploy it. Deploying with Terraform should be done manually on the command line, so that you can review any changes before applying your configuration. In the same directory as your main.tf file execute: > terraform init > terraform apply After reviewing your plan, enter \"yes\" when prompted. Writing Your Batch Pipelines with Composer/Airflow Your Composer Environment is running now, but it's not going to do anything until you write some DAGs. A good way to organize your DAGs is to make one DAG per pipeline. You have four data sources that you are going to batch load, and you need a separate pipeline for each: 1. Energy pricing data from ComEd's web API 2. Customer data from Hamsterwheel's Spanner DB 3. Marketing leads data from GCS 4. Competitor products data from GCS At the top level of your repo create your dags folder and helpers subfolder: > mkdir dags > mkdir dags/helpers In your text editor of choice create a settings.py file in the helpers folder: # settings.py # These are the default settings to be used for every DAG. They can be overwritten in the DAG file if needed. DEFAULT_DAG_ARGS = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } # Composer maps this file path to a GCS bucket so it can be treated like local storage. LOCAL_STORAGE = '/home/airflow/gcs/data/' DAGS_FOLDER = '/home/airflow/gcs/dags/' Now add these four files in your dags folder. comed.py : #! /usr/bin/env python3 \"\"\" comed.py This DAG pulls energy pricing data from https://hourlypricing.comed.com/api?type=5minutefeed. That data will be saved to GCS, then uploaded to BQ. \"\"\" import datetime import json import os import textwrap from airflow import DAG from airflow.operators.python_operator import PythonVirtualenvOperator from airflow.operators.bash_operator import BashOperator from helpers import settings dag = DAG( 'comed', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) LOCAL_FILE = os.path.join(settings.LOCAL_STORAGE, 'energy_prices.json') COMED_URL = 'https://hourlypricing.comed.com/api?type=5minutefeed' TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') GCS_LOCATION = os.path.join('gs://de-book-prod/comed', TODAY_NODASHES, 'energy_prices.json') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'comed_pricing_lnd.json') def download_data_to_local(url: str, destination: str) -> None: \"\"\" This function sends an HTTP GET request to the provided URL, and saves the returned data to the provided destination as a newline delimited JSON file. ## PARAMETERS ## url: String. The URL to which and HTTP GET request will be sent. e.g. \"http://www.example.com/api/\" destination: String. The file location where the data will be saved. e.g. \"/path/to/local/file.txt\" ## returns: None \"\"\" import requests import pandas as pd response = requests.get(url) response.raise_for_status() data = response.json() df = pd.DataFrame(data) df.to_json(destination, orient='records', lines=True) t_download_data_to_local = PythonVirtualenvOperator( task_id=\"download_data_to_local\", python_version=\"3\", python_callable=download_data_to_local, requirements=[\"requests==2.7.0\", \"pandas==1.1.3\"], op_kwargs={ url=COMED_URL, destination=LOCAL_FILE } dag=dag ) t_upload_data_to_gcs = BashOperator( task_id=\"upload_data_to_gcs\", bash_command=f\"gsutil cp {LOCAL_FILE} {GCS_LOCATION}\", dag=dag ) t_upload_data_to_bq.set_upstream(t_download_data_to_local) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ 'competitors.comed_pricing_lnd${TODAY_NODASHES}' \\\\ '{GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_upload_data_to_gcs) hamsterwheel_db.py : #! /usr/bin/env python3 \"\"\" hamsterwheel_db.py This DAG pulls customer data from a replica of Hamsterwheel's application DB. That data will be saved to GCS, then uploaded to BQ. \"\"\" import datetime import json import os from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonVirtualenvOperator from helpers import settings dag = DAG( 'comed', schedule_interval=\"0 * * * *\", # run every hour, on the hour max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) LOCAL_FILE = os.path.join(settings.LOCAL_STORAGE, 'customers.json') CUSTOMERS_SQL = \"SELECT customer_id, customer_name, address, phone_number, is_active FROM customers\" TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') GCS_LOCATION = os.path.join('gs://de-book-prod/comed', f'{TODAY_NODASHES}/') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'customer_lnd.json') def save_data_to_local(sql: str, destination: str) -> None: \"\"\" This function takes the provided SQL, executes it against the Hamsterwheel Spanner DB, then saves the results at the provided location as newline-delimited JSON. ## PARAMETERS ## sql: String. The SQL query to be executed. e.g. \"select * from my_table\" destination: String. The file location where the data will be saved. e.g. \"/path/to/local/file.txt\" ## returns: None \"\"\" from google.cloud import spanner import pandas as pd spanner_client = spanner.Client() instance = spanner_client.instance('hamsterwheel-instance') database = instance.database('hamsterwheel-db') customers = [] with database.snapshot() as snapshot: results = snapshot.execute_sql(sql) for row in results: customer = dict( customer_id = row[0] customer_name = row[1] address = row[2] phone_number = row[3] is_active = row[4] ) customers.append(customer) df = pd.DataFrame(customers) df.to_json(destination, orient='records', lines=True) t_save_data_to_local = PythonVirtualenvOperator( task_id=\"save_data_to_local\", python_version=\"3\", python_callable=save_data_to_local, requirements=[\"google-cloud-spanner==3.0.0\"], op_kwargs={ sql=CUSTOMERS_SQL, destination=LOCAL_FILE } dag=dag ) t_copy_data_to_lake = BashOperator( task_id=\"copy_data_to_lake\", bash_command=f\"gsutil cp {LOCAL_FILE} gs://de-book-prod/comed/{TODAY_NODASHES}/\", dag=dag ) t_copy_data_to_lake.set_upstream(t_save_data_to_local) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ 'competitors.comed_pricing_lnd${TODAY_NODASHES}' \\\\ '{GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_copy_data_to_lake) competitor_products.py : #! /usr/bin/env python3 \"\"\" competitor_products.py This DAG pulls competitor product data from a source GCS bucket, moves it to a GCS bucket in our data lake, process the file in Dataproc to remove the array of arrays that is incompatible with BigQuery, save the cleaned file to GCS, then load to BigQuery. \"\"\" import datetime import json import os import textwrap from airflow import DAG from airflow.operators.bash_operator import BashOperator from helpers import settings dag = DAG( 'competitor_products', schedule_interval=None, # Not scheduled. Will be triggered by Cloud Function max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) SOURCE_GCS_DESTINATION = 'gs://de-book-source/scraper/competitor_products.json' TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') DESTINATION_GCS_LOCATION = os.path.join('gs://de-book-prod/competitors', TODAY_NODASHES, 'raw', 'competitor_products.json') CLEAN_GCS_LOCATION = os.path.join('gs://de-book-prod/competitors', TODAY_NODASHES, 'clean', 'competitor_products.json') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'competitor_products_lnd.json') t_copy_raw_data_to_gcs = BashOperator( task_id=\"copy_raw_data_to_gcs\", bash_command=f\"gsutil cp {SOURCE_GCS_DESTINATION} {DESTINATION_GCS_LOCATION}\", dag=dag ) # using Spark because we expect these files to be quite large t_start_dataproc_cluster = BashOperator( task_id=\"start_dataproc_cluster\", bash_command=textwrap.dedent(\"\"\" gcloud dataproc clusters create competitor-products-cluster \\\\ --region=us-central1 \\\\ --num-workers=2 \\\\ --worker-machine-type=n2-standard-2 \\\\ --image-version=1.5-debian10 \"\"\"), dag=dag ) t_submit_pyspark_job = BashOperator( task_id=\"submit_pyspark_job\", bash_command=textwrap.dedent(\"\"\" gcloud dataproc jobs submit pyspark \\\\ gs://de-book-prod/pyspark_jobs/competitor_products.py \\\\ --cluster=my-cluster \\\\ --region=us-central1 \"\"\"), dag=dag ) t_submit_pyspark_job.set_upstream(t_start_dataproc_cluster) t_submit_pyspark_job.set_upstream(t_copy_raw_data_to_gcs) t_delete_dataproc_cluster = BashOperator( task_id=\"submit_pyspark_job\", bash_command=\"gcloud dataproc clusters delete competitor-products-cluster --region=us-central1\", dag=dag ) t_delete_dataproc_cluster.set_upstream(t_submit_pyspark_job) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=CSV \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ --skip_leading_rows 1 \\\\ 'marketing.competitor_products_lnd${TODAY_NODASHES}' \\\\ '{CLEAN_GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_submit_pyspark_job) sales_leads.py : #! /usr/bin/env python3 \"\"\" sales_leads.py This DAG pulls sales lead data from the marketing GCS bucket, brings it into the data lake bucket, then moves it to BigQuery. \"\"\" import datetime import json import os import textwrap from airflow import DAG from airflow.operators.bash_operator import BashOperator from helpers import settings dag = DAG( 'sales_leads', schedule_interval=None, # Not scheduled. Will be triggered by Cloud Function max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) SOURCE_GCS_DESTINATION = 'gs://de-book-source/marketing/sales_leads.csv' TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') DESTINATION_GCS_LOCATION = os.path.join('gs://de-book-prod/marketing', TODAY_NODASHES, 'sales_leads.csv') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'sales_leads_lnd.json') t_copy_data_to_gcs = BashOperator( task_id=\"copy_data_to_gcs\", bash_command=f\"gsutil cp {SOURCE_GCS_DESTINATION} {DESTINATION_GCS_LOCATION}\", dag=dag ) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=CSV \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ --skip_leading_rows 1 \\\\ 'marketing.sales_leads_lnd${TODAY_NODASHES}' \\\\ '{DESTINATION_GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_upload_data_to_gcs) Those are the DAGs for your four batch pipelines. There are a couple a python functions inside those DAGs, so you should create some unit tests. Start by adding a tests folder inside your dags folder, then create a dag_tests.py file: # dag_tests.py # run from top level of repo import unittest from unittest import mock import json from ..comed import download_data_to_local from ..hamsterwheel_db import save_data_to_local class TestComedDag(unittest.TestCase): @mock.patch('requests.get') def download_data_to_local(self, mock_request): mock_data = [ {'millisUTC': '1612738500000', 'price': '3.1'}, {'millisUTC': '1612738600000', 'price': '3.2'}, {'millisUTC': '1612738700000', 'price': '3.3'} ] mock_request.return_value.json.return_value = mock_data mock_request.return_value.ok = True local_file = 'test_output.json' url = 'http://www.example.com/api/' download_data_to_local(url, local_file) data_list = [] with open(local_file, r) as f: for line in f: data_dict = json.load(line) data_list.append(data_dict) self.assertEqual(len(data_list), 3) self.assertEqual(data_list[0][\"price\"], '3.1') self.assertEqual(data_list[0][\"millisUTC\"], '1612738500000') def tearDown(self): os.remove('test_output.json') class TestHamsterwheelDbDag(unittest.TestCase): @mock.patch('snapshot.execute_sql') def save_data_to_local(self, mock_query): mock_data = [ [1, 'Sam', '1600 Pennsylvania Ave.', '555-555-1111', False], [2, 'Josh', '5 Sesame St.', '555-555-2222', True], [3, 'Toby', '1 Main St.', '555-555-3333', True] ] mock_query.return_value = mock_data sql = 'select foo from bar' local_file = 'test_output.json' save_data_to_local(sql, local_file) data_list = [] with open(local_file, r) as f: for line in f: data_dict = json.load(line) data_list.append(data_dict) self.assertEqual(len(data_list), 3) self.assertEqual(len(data_list[0]), 5) self.assertEqual(data_list[1][\"customer_name\"], 'Josh') self.assertTrue(data_list[2][\"is_acrtive\"]) def tearDown(self): os.remove('test_output.json') if __name__ == '__main__': unittest.main() Those DAGs will control your batch pipelines. But two of those DAGs rely on Cloud Functions to trigger, and one of those also needs to run a Spark job. You'll need to write these before all your batch pipelines will work. Writing Your Event-Driven Pipelines with Cloud Functions and Dataproc/Spark To finish your batch pipelines you'll need the following: 1. A Cloud Function to trigger the Sales Leads DAG. 2. A Cloud Function to trigger the Competitor Products DAG. 3. A Spark job to process the competitor products data. At the top level of your repo create your file structure by executing the following: > mkdir functions > mkdir functions/sales_leads > touch functions/sales_leads/main.py > touch functions/sales_leads/requirements.txt > mkdir functions/competitor_products > touch functions/competitor_products/main.py > touch functions/competitor_products/requirements.txt To trigger a DAG from your Cloud Function, you need to get your Airflow Client ID. To do that, create and execute this Python script: # client_id.py import google.auth import google.auth.transport.requests import requests import six.moves.urllib.parse # Authenticate with Google Cloud. # See: https://cloud.google.com/docs/authentication/getting-started credentials, _ = google.auth.default( scopes=['https://www.googleapis.com/auth/cloud-platform']) authed_session = google.auth.transport.requests.AuthorizedSession(credentials) project_id = 'de-book-prod' location = 'us-central1' composer_environment = 'batch_jobs' environment_url = ( 'https://composer.googleapis.com/v1beta1/projects/{}/locations/{}' '/environments/{}').format(project_id, location, composer_environment) composer_response = authed_session.request('GET', environment_url) environment_data = composer_response.json() airflow_uri = environment_data['config']['airflowUri'] # The Composer environment response does not include the IAP client ID. # Make a second, unauthenticated HTTP request to the web server to get the # redirect URI. redirect_response = requests.get(airflow_uri, allow_redirects=False) redirect_location = redirect_response.headers['location'] # Extract the client_id query parameter from the redirect. parsed = six.moves.urllib.parse.urlparse(redirect_location) query_string = six.moves.urllib.parse.parse_qs(parsed.query) print(query_string['client_id'][0]) > python client_id.py abc123 In addition to your Client ID, you'll need your Webserver ID: > gcloud composer environments describe batch_jobs \\ --location us-central1 \\ --format=\"get(config.airflowUri)\" Your Webserver ID is the first part of your webserver URL. e.g. <Webserver ID>.appspot.com In the main.py file for Sales Leads you'll need: # This code is lifted from: https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf from google.auth.transport.requests import Request from google.oauth2 import id_token import requests IAM_SCOPE = 'https://www.googleapis.com/auth/iam' OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token' def trigger_dag(data, context=None): \"\"\"Makes a POST request to the Composer DAG Trigger API When called via Google Cloud Functions (GCF), data and context are Background function parameters. For more info, refer to https://cloud.google.com/functions/docs/writing/background#functions_background_parameters-python To call this function from a Python script, omit the ``context`` argument and pass in a non-null value for the ``data`` argument. \"\"\" # Fill in with your Composer info here # Navigate to your webserver's login page and get this from the URL # Or use the script found at # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/composer/rest/get_client_id.py client_id = 'YOUR-CLIENT-ID' # This should be part of your webserver's URL: # {tenant-project-id}.appspot.com webserver_id = 'YOUR-TENANT-PROJECT' # The name of the DAG you wish to trigger dag_name = 'sales_leads' webserver_url = ( 'https://' + webserver_id + '.appspot.com/api/experimental/dags/' + dag_name + '/dag_runs' ) # Make a POST request to IAP which then Triggers the DAG make_iap_request( webserver_url, client_id, method='POST', json={\"conf\": data, \"replace_microseconds\": 'false'}) # This code is copied from # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py # START COPIED IAP CODE def make_iap_request(url, client_id, method='GET', **kwargs): \"\"\"Makes a request to an application protected by Identity-Aware Proxy. Args: url: The Identity-Aware Proxy-protected URL to fetch. client_id: The client ID used by Identity-Aware Proxy. method: The request method to use ('GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE') **kwargs: Any of the parameters defined for the request function: https://github.com/requests/requests/blob/master/requests/api.py If no timeout is provided, it is set to 90 by default. Returns: The page body, or raises an exception if the page couldn't be retrieved. \"\"\" # Set the default timeout, if missing if 'timeout' not in kwargs: kwargs['timeout'] = 90 # Obtain an OpenID Connect (OIDC) token from metadata server or using service # account. google_open_id_connect_token = id_token.fetch_id_token(Request(), client_id) # Fetch the Identity-Aware Proxy-protected URL, including an # Authorization header containing \"Bearer \" followed by a # Google-issued OpenID Connect token for the service account. resp = requests.request( method, url, headers={'Authorization': 'Bearer {}'.format( google_open_id_connect_token)}, **kwargs) if resp.status_code == 403: raise Exception('Service account does not have permission to ' 'access the IAP-protected application.') elif resp.status_code != 200: raise Exception( 'Bad response from application: {!r} / {!r} / {!r}'.format( resp.status_code, resp.headers, resp.text)) else: return resp.text # END COPIED IAP CODE Be sure to fill in your Client ID and Webserver ID in the code above. In the same folder, create requirements.txt : requests_toolbelt==0.9.1 google-auth==1.24.0 In the main.py file for Competitor Products you'll need: # This code is lifted from: https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf from google.auth.transport.requests import Request from google.oauth2 import id_token import requests IAM_SCOPE = 'https://www.googleapis.com/auth/iam' OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token' def trigger_dag(data, context=None): \"\"\"Makes a POST request to the Composer DAG Trigger API When called via Google Cloud Functions (GCF), data and context are Background function parameters. For more info, refer to https://cloud.google.com/functions/docs/writing/background#functions_background_parameters-python To call this function from a Python script, omit the ``context`` argument and pass in a non-null value for the ``data`` argument. \"\"\" # Fill in with your Composer info here # Navigate to your webserver's login page and get this from the URL # Or use the script found at # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/composer/rest/get_client_id.py client_id = 'YOUR-CLIENT-ID' # This should be part of your webserver's URL: # {tenant-project-id}.appspot.com webserver_id = 'YOUR-TENANT-PROJECT' # The name of the DAG you wish to trigger dag_name = 'competitor_products' webserver_url = ( 'https://' + webserver_id + '.appspot.com/api/experimental/dags/' + dag_name + '/dag_runs' ) # Make a POST request to IAP which then Triggers the DAG make_iap_request( webserver_url, client_id, method='POST', json={\"conf\": data, \"replace_microseconds\": 'false'}) # This code is copied from # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py # START COPIED IAP CODE def make_iap_request(url, client_id, method='GET', **kwargs): \"\"\"Makes a request to an application protected by Identity-Aware Proxy. Args: url: The Identity-Aware Proxy-protected URL to fetch. client_id: The client ID used by Identity-Aware Proxy. method: The request method to use ('GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE') **kwargs: Any of the parameters defined for the request function: https://github.com/requests/requests/blob/master/requests/api.py If no timeout is provided, it is set to 90 by default. Returns: The page body, or raises an exception if the page couldn't be retrieved. \"\"\" # Set the default timeout, if missing if 'timeout' not in kwargs: kwargs['timeout'] = 90 # Obtain an OpenID Connect (OIDC) token from metadata server or using service # account. google_open_id_connect_token = id_token.fetch_id_token(Request(), client_id) # Fetch the Identity-Aware Proxy-protected URL, including an # Authorization header containing \"Bearer \" followed by a # Google-issued OpenID Connect token for the service account. resp = requests.request( method, url, headers={'Authorization': 'Bearer {}'.format( google_open_id_connect_token)}, **kwargs) if resp.status_code == 403: raise Exception('Service account does not have permission to ' 'access the IAP-protected application.') elif resp.status_code != 200: raise Exception( 'Bad response from application: {!r} / {!r} / {!r}'.format( resp.status_code, resp.headers, resp.text)) else: return resp.text # END COPIED IAP CODE You can use the same Client ID and Webserver ID you used above. Again, you'll need to create requirements.txt in the same folder: requests_toolbelt==0.9.1 google-auth==1.24.0 These two cloud functions are largely identical. However, because these functions are being deployed independently you cannot DRY out this code by abstracting the common code into a shared module. A potential enhancement is to create a single Cloud Function to trigger DAGs, and infer the DAG name from the event data, such as the name of the bucket a file was loaded into. The last piece of your batch pipelines is your Spark job for the competitor products data. This is necessary because the JSON schema contains an array-of-arrays, which is a structure that cannot be loaded into BigQuery. You are using Spark for this (as opposed to e.g. Pandas) because you suspect that this file will periodically be too large to fit into memory. Start by adding the pyspark_jobs folder to the top level of your repo. Inside pyspark_jobs create competitor_products.py : #!/usr/bin/env python \"\"\" This pyspark job takes a JSON file of competitor products and transforms it. Each record in that file contains the \"product_groups\" field, which is an array-of-arrays. This script will change that field to an array-of-strings by concatenating the inner array. The other fields will be unchanged. \"\"\" import argparse import os from pyspark.sql import SparkSession from pyspark.sql.functions import col, concat_ws def generate_clean_blob_path(bucket_name: str, blob_prefix: str) -> str: \"\"\" This function is used to generate the path that the clean file will be saved to. For example, the raw file may be read from \"gs://my-bucket/competitor_products/20211225/raw/competitor_products.json\", and the transformed file will be saved to \"gs://my-bucket/competitor_products/20211225/clean/\". ## PARAMETERS ## bucket_name: String. Name of GCS bucket where the data exists. blob_prefix: String. Name of the blob, including folders within the bucket, if any. ## returns: String. The GCS path to save the transformed data. \"\"\" new_blob_prefix = os.path.dirname(os.path.dirname(blob_prefix)) destination = os.path.join('gs://', new_blob_prefix, \"clean\") return destination def main(bucket_name: str, blob_prefix: str) -> str: \"\"\" This function reads a competitor_products.json file into a dataframe, transform the \"product_groups\" field, then saves the data back to GCS. ## PARAMETERS ## bucket_name: String. Name of GCS bucket where the data exists. blob_prefix: String. Name of the blob, including folders within the bucket, if any. ## returns: None \"\"\" spark = SparkSession.builder.appName(\"competitor_products\").getOrCreate() df = spark.read.json(f\"gs://{bucket_name}/{blob_prefix}\") df2 = df.withColumn(\"product_groups\", concat_ws(\"; \", col(\"product_groups\"))) destination = generate_clean_blob_path(bucket_name, blob_prefix) df2.write.json(destination, mode=\"overwrite\") if __name__ == \"__main__\": parser = argparse.ArgumentParser() parser.add_argument( \"--bucket_name\", dest=\"bucket_name\", required=True, help=\"Name of GCS bucket where the data exists.\" ) parser.add_argument( \"--blob_prefix\", dest=\"blob_prefix\", required=True, help=\"Name of the blob, including folders within the bucket, if any.\" ) args = parser.parse_args() main(bucket_name=args.bucket_name, blob_prefix=args.blob_prefix) Writing your Streaming Pipeline with Pub/Sub, Cloud Functions, and GKE Deploying Your Code with Cloud Build Now you need to get your code onto GCP so your data will start flowing. You need to give it code to execute so it can start moving and storing data. In particular you need your deployment process to do the following: 1. Execute your automated tests 2. Deploy your Cloud Functions 3. Deploy your Composer DAGs 4. Deploy your Dataproc job 5. Deploy your GKE image A note about GKE: You may have noticed that I did not cover Google Kubernetes Engine (GKE) previously in this book. GKE is a powerful tool, but it's also a big topic to cover, so I left it out for the sake of brevity. We'll be using it here, and this code should be a good starting point if you want to learn more. I mention this only so you don't waste your time looking for where I covered GKE in more detail in previous chapters, because I didn't. Before you can complete your Cloud Build file you'll need a few things. First, you'll need the bucket name where Composer will look for new DAGs: > gcloud composer environments describe prod_environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-prod-environment-abc123456-bucket/dags In the top level of your repo create your cloudbuild.yaml file: # cloudbuild.yaml steps: # run automated tests - name: 'docker.io/library/python:3.7' id: Test entrypoint: /bin/sh args: [-c, 'python -m unittest dags/tests/'] # deploy DAGs - name: gcr.io/cloud-builders/gcloud id: Deploy entrypoint: bash args: [ '-c', 'gsutil -m rsync -d -r ./dags gs://${_COMPOSER_BUCKET}/dags'] substitutions: _COMPOSER_BUCKET: us-central1-prod-environment-abc123456-bucket # You retrieved this value above # build the GKE container image - name: \"gcr.io/cloud-builders/docker\" args: [\"build\", \"-t\", \"gcr.io/project-id/image:tag\", \".\"] # push the GKE container image - name: \"gcr.io/cloud-builders/docker\" args: [\"push\", \"gcr.io/project-id/image:tag\"] # deploy container image to GKE - name: \"gcr.io/cloud-builders/gke-deploy\" args: - run - --filename=kubernetes-resource-file - --image=gcr.io/project-id/image:tag - --location=location - --cluster=cluster Final Thoughts Cleaning Up","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_13_up_and_running/#up-and-running-data-engineering-on-the-google-cloud-platform","text":"The completely free E-Book for setting up and running a Data Engineering stack on Google Cloud Platform. NOTE: This book is currently incomplete. If you find errors or would like to fill in the gaps, read the Contributions section .","title":"Up and Running: Data Engineering on the Google Cloud Platform"},{"location":"de-gcp-book/ch_13_up_and_running/#table-of-contents","text":"Preface Chapter 1: Setting up a GCP Account Chapter 2: Setting up Batch Processing Orchestration with Composer and Airflow Chapter 3: Building a Data Lake with Google Cloud Storage (GCS) Chapter 4: Building a Data Warehouse with BigQuery Chapter 5: Setting up DAGs in Composer and Airflow Chapter 6: Setting up Event-Triggered Pipelines with Cloud Functions Chapter 7: Parallel Processing with Dataproc and Spark Chapter 8: Streaming Data with Pub/Sub Chapter 9: Managing Credentials with Google Secret Manager Chapter 10: Infrastructure as Code with Terraform Chapter 11: Deployment Pipelines with Cloud Build Chapter 12: Monitoring and Alerting Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure Appendix A: Example Code Repository","title":"Table of Contents"},{"location":"de-gcp-book/ch_13_up_and_running/#chapter-13-up-and-running-building-a-complete-data-engineering-infrastructure","text":"In the previous 12 chapters you learned: * how to set up batch and streaming data pipelines * how to orchestrate your pipelines * how to transform your data * how to set up your data warehouse * how to deploy your code * how to set up infrastructure as code * how to monitor your infrastructure In this chapter we are going to cover all of that in one go: setting up a complete data engineering infrastructure for a fictional company. I'm not going to go into as much detail for each piece we are going to set up, as we already covered the details in previous chapters. Instead, this chapter will walk you through the steps you would take if you needed to get a complete infrastructure up and running on GCP. I mentioned this in the introduction, but I'll repeat it here: what is covered in this book is not THE data engineering stack, it is A data engineering stack. Even within GCP there are lots of ways to accomplish similar tasks. For example, we could have used Dataflow (based on Apache Beam) for our streaming solution. Or we could have gone with a completely different paradigm for how we store and query our data, such as storing our data as Parquet files in GCS and querying with Spark. I mention this here to make sure you understand that this book is not the complete guide to being a data engineer. Rather, it is an introduction to the types of problems a data engineer solves, and a sampling common tools in GCP used to solve those problems. You can view the code we set up in this chapter in its final form in Appendix A , which is a sample code repository. This is going to be quite a long chapter, so here are the links for each section: * Your Requirements * Setting Up a New Project * Mocking Hamsterwheel's Source Systems * Instantiating Your Infrastructure with Terraform * Writing Your Batch Pipeline with Composer/Airflow * Writing Your Event-Driven Pipelines with Cloud Functions and Dataproc/Spark * Writing your Streaming Pipeline with Pub/Sub, Cloud Functions, and GKE * Deploying Your Code with Cloud Build * Final Thoughts * Cleaning Up","title":"Chapter 13: Up and Running - Building a Complete Data Engineering Infrastructure"},{"location":"de-gcp-book/ch_13_up_and_running/#your-requirements","text":"Welcome Hamsterwheel Batteries Inc., an up-and-coming battery retailer, where you are the newest (and only) data engineer for the company. Our analysts have decided that emailing Excel files to each other is no longer meeting their needs, so it's your job to set up a data warehouse and the various pipelines to feed that data. You'll need to bring in data from a variety of data sources, including: * Web APIs * CSV and JSON files periodically updated in GCS * Batch data from the database supporting Hamsterwheel's website * Streaming data from the database supporting Hamsterwheel's website You'll need to store that data in the data lake and data warehouse you'll set up. You'll then need to transform that data into a format that is easy for our analysts to use.","title":"Your Requirements"},{"location":"de-gcp-book/ch_13_up_and_running/#setting-up-a-new-project","text":"So far in this book I've been working in de-book-dev project. In this chapter I'm going to build a production environment, so I'll make a new de-book-prod project. Keeping your environments in different projects is a good idea. For this chapter it's not strictly necessary, but it might be a good idea in case you have an lingering infrastructure on an existing project that may make it harder to track what you're building. Setting up a new project was discussed in Chapter 1. We're also going to create a new GitHub repository for our code. I went over that in Chapter 11: Deployment Pipelines with Cloud Build . You can see mine here: https://github.com/Nunie123/hamsterwheel, though the same code is available in Appendix A . Now that we have a new project and a new repo, let's create a new service account and save our key in a top-level keys/ folder in our repo. I discussed how to do this in Chapter 1. Because this code is going to be published to a remote repository we need to make sure our key file doesn't get made public. let's add a .gitignore file at the top level of our repo: keys __pycache__ .vscode .DS_Store The three entries below keys we aren't excluding because they are private, but rather because they contain cached data and settings that are not valuable to have in version control. Finally, let's make sure we have the right account initialized with our new project set to default: > gcloud init If you don't have gcloud or the other GCP command line tools installed, check out Chapter 1.","title":"Setting Up a New Project"},{"location":"de-gcp-book/ch_13_up_and_running/#mocking-hamsterwheels-source-systems","text":"Before we can dive into building our data pipelines we're going to need some source data to ingest. Because we don't have access to an actual company's data sources we are going to mock up some of our own. Yes, it does seem a bit silly to generate data in one place just so we can move it to another place. But by doing this we can actually see our infrastructure working, rather than building it and just taking my word that it would work. In this section we will set up the systems and data we need to mock real data sources.","title":"Mocking Hamsterwheel's Source Systems"},{"location":"de-gcp-book/ch_13_up_and_running/#web-api","text":"For your Web API there's no fake data needed, we'll use ComEd's energy pricing API available here: https://hourlypricing.comed.com/api?type=5minutefeed","title":"Web API"},{"location":"de-gcp-book/ch_13_up_and_running/#csv-file","text":"For our CSV files, let's suppose those are generated by the marketing department as potential leads for new customers. Hamsterwheel's marketing department puts a new file into a GCS bucket every morning. Let's start by making the CSV file using this Python script: # sale_leads.py import csv from faker import Faker def generate_file() -> None: fake = Faker() with open('sale_leads.csv', 'w') as f: writer = csv.writer(f, delimiter=',') writer.writerow(['name', 'phone_number']) for _ in range(500): writer.writerow([fake.name(), fake.phone_number()]) if __name__ == '__main__': generate_file() faker is a Python library for generating fake data. It is not part of the standard library, so you'll need to install it ( pip install Faker ). We're also going to need the Spanner Python library, so lets install that now as well. These should be installed inside a Python virtual environment, using the tool of your choice (e.g. venv . > pip install Faker > pip install google-cloud-spanner Now let's generate the CSV file by executing the script: > python sale_leads.py We can verify the output looks right: > head -n 5 sale_leads.csv name,phone_number Brittany Walker,+1-152-651-8997x12442 Felicia Coleman,245.551.7982 Crystal Bell,803-316-1054x535 James Morris,354-636-3746 We need this file in a bucket (as if put there by Hamsterwheel's marketing department), so lets create a bucket and copy over the file: > gsutil mb gs://de-book-source > gsutil cp sale_leads.csv gs://de-book-source/marketing/ Note that the bucket I created I called de-book-source , but that name will likely not be available to you since all bucket names are unique on GCS. Choose whatever name you like.","title":"CSV File"},{"location":"de-gcp-book/ch_13_up_and_running/#json-file","text":"For our JSON files, let's suppose Hamsterwheel runs a web scraper to get information on competitor products, and deposits a new file in a GCS bucket every evening after the script finishes running. We'll again use a python script to generate this file: # competitor_products.py import json import random from faker import Faker def generate_file() -> None: fake = Faker() data = [] for _ in range(500): product = dict( company=fake.company(), product_name=' '.join(fake.words(2)), in_stock=fake.boolean(), sku=fake.ean(), price=random.randrange(500,20000)/100, product_groups=[[random.randrange(1,500), random.randrange(1,500)], [random.randrange(1,500), random.randrange(1,500)]] ) data.append(product) with open('competitor_products.json', 'w') as f: json.dump(data, f) if __name__ == '__main__': generate_file() We can execute the script to generate the file, verify the contents, then copy the file to our GCS bucket: > python competitor_products.py > head -c 500 competitor_products.json > gsutil cp competitor_products.json gs://de-book-source/scraper/","title":"JSON File"},{"location":"de-gcp-book/ch_13_up_and_running/#company-database","text":"We have a Web API and a couple flat files, now for the hard part. We're going to set up a relational database to simulate Hamsterwheel's transactional database that tracks customer data and sales. We are going to use Spanner , a fully managed relational database service on GCP to create a customers table and a sales table. We'll do a one-time load into our customer table, and set up a script to periodically add sales data to the sales table, which we'll use for our streaming pipeline. We'll start by specifying our tables in Data Definition Language (DDL) : -- hamsterwheel_db.sql CREATE TABLE customers ( customer_id STRING(36) NOT NULL, customer_name STRING(100) NOT NULL, address STRING(1000) NULL, phone_number STRING(20) NULL, is_active BOOL NOT NULL AS TRUE ) PRIMARY KEY (customer_id) ; CREATE TABLE sales( sale_id STRING(36) NOT NULL, sale_price FLOAT64 NOT NULL, sale_timestamp TIMESTAMP NOT NULL, customer_id STRING(36) NOT NULL ) PRIMARY KEY (sale_id) , FOREIGN KEY (customer_id) REFERENCES customers (customer_id) ; Now we can instantiate the database: > gcloud spanner databases create hamsterwheel-db \\ --instance=hamsterwheel-instance \\ --ddl-file=hamsterwheel_db.sql We have our tables set up, so now we can insert some data. We're going to use some Python scripts to generate and insert our data. # customer_data.py from faker import Faker from google.cloud import spanner fake = Faker() def generate_insert_statement() -> str: sql_list = ['insert into customers(customer_id, customer_name, address, phone_number, is_active)'] sql_list.append('values') for i in range(1, 501): row = f'({i}, {fake.name()}, {fake.address()}, {fake.phone_number()}, {fake.boolean})' sql_list.append(row) sql_statement = '\\n'.join(sql_list) return sql_statement def insert_customer_records_callback(transaction) -> None: sql_statement = generate_insert_statement() row_ct = transaction.execute_update(sql_statement) print(f\"{row_ct} records inserted.\".) def insert_custom_records(): spanner_client = spanner.Client() instance = spanner_client.instance('hamsterwheel-instance') database = instance.database('hamsterwheel-db') database.run_in_transaction(insert_customer_records_callback) if __name__ == '__main__': insert_customer_records() Let's execute it: > python customer_data.py For sales data, we want to set up a long-running Python script that will periodically be adding new records: # sales_data.py import uuid import random import datetime import time from google.cloud import spanner def generate_insert_statement() -> str: sql_list = ['insert into sales(sale_id, sale_price, sale_timestamp, customer_id)'] sql_list.append('values') row = f'({uuid.uuid4()}, {random.randrange(500,20000)/100}, {datetime.datetime.now()}, {random.randint(1,500)})' sql_list.append(row) sql_statement = '\\n'.join(sql_list) return sql_statement def insert_product_record_callback(transaction) -> None: sql_statement = generate_insert_statement() row_ct = transaction.execute_update(sql_statement) print(f\"{row_ct} record inserted.\".) def insert_product_records_over_time(): spanner_client = spanner.Client() instance = spanner_client.instance('hamsterwheel-instance') database = instance.database('hamsterwheel-db') while i < 240: database.run_in_transaction(insert_product_record_callback) i = i + 1 sleep_time = random.randint(20,40) time.sleep(sleep_time) if __name__ == '__main__': insert_product_records_over_time() > python sales_data.py This script is set up to run for about two hours. We can just leave it alone and open a new terminal session to use for the rest of the chapter. Finally, let's make sure our data is in our tables: > gcloud spanner databases execute-sql hamsterwheel-db \\ --instance=hamsterwheel-instance \\ --sql=\"select * from customers limit 20\" > gcloud spanner databases execute-sql hamsterwheel-db \\ --instance=hamsterwheel-instance \\ --sql=\"select * from sales limit 20\" We now have all the data sources we need to set up realistic data engineering infrastructure. So let's get started.","title":"Company Database"},{"location":"de-gcp-book/ch_13_up_and_running/#instantiating-your-infrastructure-with-terraform","text":"As the sole data engineer at Hamsterwheel Batteries Inc. it's your job to maintain all of the data engineering infrastructure. That includes debugging infrastructure failures and recovering from disasters (e.g. a backhoe severs the internet connection of the GCP location where your infrastructure is running). Both of those tasks (debugging, disaster recovery) get a lot easier if you know exactly what infrastructure you are running at any given time. As we talked about in Chapter 10: Infrastructure as Code with Terraform , we can solve those problems by adopting Infrastructure as Code with Terraform. We installed Terraform in Chapter 10: Infrastructure as Code with Terraform . If you don't have it installed refer to Chapter 10: Infrastructure as Code with Terraform or follow the online instructions . Now we need to plan out what infrastructure we are going to need: * You are presenting your data to users in a data warehouse, so you'll need BigQuery. * You need a data lake to store your raw data, so you'll need Google Cloud Storage (GCS). * You need to ingest data files once they are loaded into a GCS bucket, so you'll need Cloud Functions. * You need to transform your data before loading it into BigQuery, so you'll need Dataproc/Spark. * You need to ingest data at regular intervals from a web API and a database, so you'll need Composer/Airflow. * You need to stream data from a database into BigQuery, so you'll need a GKE cluster for querying the database, Pub/Sub for queueing the data, and a Cloud Function to insert it into BigQuery. * You need a deployment pipeline to get your code into GCP, so you'll need Cloud Build. * You need to make sure the services you are using have permission to do the things they need and no more, so you'll need to set up service accounts. This is a good list to get you started. It's easy enough to change your infrastructure as you go (another benefit of Terraform). Now let's get down to writing our Terraform file. You'll start by creating a terraform folder at the top level of your repo, and while you're at ti you can create your main.tf file. > mkdir terraform > touch terraform/main.tf And because you don't want any unhelpful terraform cache files cluttering up your repo, you'll update your .gitignore file: keys __pycache__ .vscode .DS_Store .tfstate .terraform .tfplan Now we are ready to fill in main.tf: That was a lot. As your infrastructure grows you can split this Terraform file into multiple files (e.g. a single file for BigQuery). You can also take advantage of Terraform's modules to reduce boilerplate code. Another optimization is to take advantage of Terraform variables to have the same Terraform code instantiate multiple environments (e.g. Dev and Prod). Now that you have your infrastructure defined, you need to deploy it. Deploying with Terraform should be done manually on the command line, so that you can review any changes before applying your configuration. In the same directory as your main.tf file execute: > terraform init > terraform apply After reviewing your plan, enter \"yes\" when prompted.","title":"Instantiating Your Infrastructure with Terraform"},{"location":"de-gcp-book/ch_13_up_and_running/#writing-your-batch-pipelines-with-composerairflow","text":"Your Composer Environment is running now, but it's not going to do anything until you write some DAGs. A good way to organize your DAGs is to make one DAG per pipeline. You have four data sources that you are going to batch load, and you need a separate pipeline for each: 1. Energy pricing data from ComEd's web API 2. Customer data from Hamsterwheel's Spanner DB 3. Marketing leads data from GCS 4. Competitor products data from GCS At the top level of your repo create your dags folder and helpers subfolder: > mkdir dags > mkdir dags/helpers In your text editor of choice create a settings.py file in the helpers folder: # settings.py # These are the default settings to be used for every DAG. They can be overwritten in the DAG file if needed. DEFAULT_DAG_ARGS = { 'owner': 'DE Book', 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 3, 'retry_delay': datetime.timedelta(seconds=30), 'start_date': datetime.datetime(2020, 10, 17), } # Composer maps this file path to a GCS bucket so it can be treated like local storage. LOCAL_STORAGE = '/home/airflow/gcs/data/' DAGS_FOLDER = '/home/airflow/gcs/dags/' Now add these four files in your dags folder. comed.py : #! /usr/bin/env python3 \"\"\" comed.py This DAG pulls energy pricing data from https://hourlypricing.comed.com/api?type=5minutefeed. That data will be saved to GCS, then uploaded to BQ. \"\"\" import datetime import json import os import textwrap from airflow import DAG from airflow.operators.python_operator import PythonVirtualenvOperator from airflow.operators.bash_operator import BashOperator from helpers import settings dag = DAG( 'comed', schedule_interval=\"0 0 * * *\", # run every day at midnight UTC max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) LOCAL_FILE = os.path.join(settings.LOCAL_STORAGE, 'energy_prices.json') COMED_URL = 'https://hourlypricing.comed.com/api?type=5minutefeed' TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') GCS_LOCATION = os.path.join('gs://de-book-prod/comed', TODAY_NODASHES, 'energy_prices.json') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'comed_pricing_lnd.json') def download_data_to_local(url: str, destination: str) -> None: \"\"\" This function sends an HTTP GET request to the provided URL, and saves the returned data to the provided destination as a newline delimited JSON file. ## PARAMETERS ## url: String. The URL to which and HTTP GET request will be sent. e.g. \"http://www.example.com/api/\" destination: String. The file location where the data will be saved. e.g. \"/path/to/local/file.txt\" ## returns: None \"\"\" import requests import pandas as pd response = requests.get(url) response.raise_for_status() data = response.json() df = pd.DataFrame(data) df.to_json(destination, orient='records', lines=True) t_download_data_to_local = PythonVirtualenvOperator( task_id=\"download_data_to_local\", python_version=\"3\", python_callable=download_data_to_local, requirements=[\"requests==2.7.0\", \"pandas==1.1.3\"], op_kwargs={ url=COMED_URL, destination=LOCAL_FILE } dag=dag ) t_upload_data_to_gcs = BashOperator( task_id=\"upload_data_to_gcs\", bash_command=f\"gsutil cp {LOCAL_FILE} {GCS_LOCATION}\", dag=dag ) t_upload_data_to_bq.set_upstream(t_download_data_to_local) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ 'competitors.comed_pricing_lnd${TODAY_NODASHES}' \\\\ '{GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_upload_data_to_gcs) hamsterwheel_db.py : #! /usr/bin/env python3 \"\"\" hamsterwheel_db.py This DAG pulls customer data from a replica of Hamsterwheel's application DB. That data will be saved to GCS, then uploaded to BQ. \"\"\" import datetime import json import os from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonVirtualenvOperator from helpers import settings dag = DAG( 'comed', schedule_interval=\"0 * * * *\", # run every hour, on the hour max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) LOCAL_FILE = os.path.join(settings.LOCAL_STORAGE, 'customers.json') CUSTOMERS_SQL = \"SELECT customer_id, customer_name, address, phone_number, is_active FROM customers\" TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') GCS_LOCATION = os.path.join('gs://de-book-prod/comed', f'{TODAY_NODASHES}/') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'customer_lnd.json') def save_data_to_local(sql: str, destination: str) -> None: \"\"\" This function takes the provided SQL, executes it against the Hamsterwheel Spanner DB, then saves the results at the provided location as newline-delimited JSON. ## PARAMETERS ## sql: String. The SQL query to be executed. e.g. \"select * from my_table\" destination: String. The file location where the data will be saved. e.g. \"/path/to/local/file.txt\" ## returns: None \"\"\" from google.cloud import spanner import pandas as pd spanner_client = spanner.Client() instance = spanner_client.instance('hamsterwheel-instance') database = instance.database('hamsterwheel-db') customers = [] with database.snapshot() as snapshot: results = snapshot.execute_sql(sql) for row in results: customer = dict( customer_id = row[0] customer_name = row[1] address = row[2] phone_number = row[3] is_active = row[4] ) customers.append(customer) df = pd.DataFrame(customers) df.to_json(destination, orient='records', lines=True) t_save_data_to_local = PythonVirtualenvOperator( task_id=\"save_data_to_local\", python_version=\"3\", python_callable=save_data_to_local, requirements=[\"google-cloud-spanner==3.0.0\"], op_kwargs={ sql=CUSTOMERS_SQL, destination=LOCAL_FILE } dag=dag ) t_copy_data_to_lake = BashOperator( task_id=\"copy_data_to_lake\", bash_command=f\"gsutil cp {LOCAL_FILE} gs://de-book-prod/comed/{TODAY_NODASHES}/\", dag=dag ) t_copy_data_to_lake.set_upstream(t_save_data_to_local) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=NEWLINE_DELIMITED_JSON \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ 'competitors.comed_pricing_lnd${TODAY_NODASHES}' \\\\ '{GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_copy_data_to_lake) competitor_products.py : #! /usr/bin/env python3 \"\"\" competitor_products.py This DAG pulls competitor product data from a source GCS bucket, moves it to a GCS bucket in our data lake, process the file in Dataproc to remove the array of arrays that is incompatible with BigQuery, save the cleaned file to GCS, then load to BigQuery. \"\"\" import datetime import json import os import textwrap from airflow import DAG from airflow.operators.bash_operator import BashOperator from helpers import settings dag = DAG( 'competitor_products', schedule_interval=None, # Not scheduled. Will be triggered by Cloud Function max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) SOURCE_GCS_DESTINATION = 'gs://de-book-source/scraper/competitor_products.json' TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') DESTINATION_GCS_LOCATION = os.path.join('gs://de-book-prod/competitors', TODAY_NODASHES, 'raw', 'competitor_products.json') CLEAN_GCS_LOCATION = os.path.join('gs://de-book-prod/competitors', TODAY_NODASHES, 'clean', 'competitor_products.json') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'competitor_products_lnd.json') t_copy_raw_data_to_gcs = BashOperator( task_id=\"copy_raw_data_to_gcs\", bash_command=f\"gsutil cp {SOURCE_GCS_DESTINATION} {DESTINATION_GCS_LOCATION}\", dag=dag ) # using Spark because we expect these files to be quite large t_start_dataproc_cluster = BashOperator( task_id=\"start_dataproc_cluster\", bash_command=textwrap.dedent(\"\"\" gcloud dataproc clusters create competitor-products-cluster \\\\ --region=us-central1 \\\\ --num-workers=2 \\\\ --worker-machine-type=n2-standard-2 \\\\ --image-version=1.5-debian10 \"\"\"), dag=dag ) t_submit_pyspark_job = BashOperator( task_id=\"submit_pyspark_job\", bash_command=textwrap.dedent(\"\"\" gcloud dataproc jobs submit pyspark \\\\ gs://de-book-prod/pyspark_jobs/competitor_products.py \\\\ --cluster=my-cluster \\\\ --region=us-central1 \"\"\"), dag=dag ) t_submit_pyspark_job.set_upstream(t_start_dataproc_cluster) t_submit_pyspark_job.set_upstream(t_copy_raw_data_to_gcs) t_delete_dataproc_cluster = BashOperator( task_id=\"submit_pyspark_job\", bash_command=\"gcloud dataproc clusters delete competitor-products-cluster --region=us-central1\", dag=dag ) t_delete_dataproc_cluster.set_upstream(t_submit_pyspark_job) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=CSV \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ --skip_leading_rows 1 \\\\ 'marketing.competitor_products_lnd${TODAY_NODASHES}' \\\\ '{CLEAN_GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_submit_pyspark_job) sales_leads.py : #! /usr/bin/env python3 \"\"\" sales_leads.py This DAG pulls sales lead data from the marketing GCS bucket, brings it into the data lake bucket, then moves it to BigQuery. \"\"\" import datetime import json import os import textwrap from airflow import DAG from airflow.operators.bash_operator import BashOperator from helpers import settings dag = DAG( 'sales_leads', schedule_interval=None, # Not scheduled. Will be triggered by Cloud Function max_active_runs=1, catchup=False, default_args=settings.DEFAULT_DAG_ARGS ) SOURCE_GCS_DESTINATION = 'gs://de-book-source/marketing/sales_leads.csv' TODAY_NODASHES = datetime.date.today().strftime('%Y%m%d') DESTINATION_GCS_LOCATION = os.path.join('gs://de-book-prod/marketing', TODAY_NODASHES, 'sales_leads.csv') SCHEMA_FILE = os.path.join(settings.DAGS_FOLDER, 'schemas', 'sales_leads_lnd.json') t_copy_data_to_gcs = BashOperator( task_id=\"copy_data_to_gcs\", bash_command=f\"gsutil cp {SOURCE_GCS_DESTINATION} {DESTINATION_GCS_LOCATION}\", dag=dag ) t_upload_data_to_bq = BashOperator( task_id=\"upload_data_to_bq\", bash_command=textwrap.dedent(f\"\"\"\\ bq load \\\\ --source_format=CSV \\\\ --time_partitioning_type DAY \\\\ --replace \\\\ --skip_leading_rows 1 \\\\ 'marketing.sales_leads_lnd${TODAY_NODASHES}' \\\\ '{DESTINATION_GCS_LOCATION}' \\\\ '{SCHEMA_FILE}' \"\"\"), dag=dag ) t_upload_data_to_bq.set_upstream(t_upload_data_to_gcs) Those are the DAGs for your four batch pipelines. There are a couple a python functions inside those DAGs, so you should create some unit tests. Start by adding a tests folder inside your dags folder, then create a dag_tests.py file: # dag_tests.py # run from top level of repo import unittest from unittest import mock import json from ..comed import download_data_to_local from ..hamsterwheel_db import save_data_to_local class TestComedDag(unittest.TestCase): @mock.patch('requests.get') def download_data_to_local(self, mock_request): mock_data = [ {'millisUTC': '1612738500000', 'price': '3.1'}, {'millisUTC': '1612738600000', 'price': '3.2'}, {'millisUTC': '1612738700000', 'price': '3.3'} ] mock_request.return_value.json.return_value = mock_data mock_request.return_value.ok = True local_file = 'test_output.json' url = 'http://www.example.com/api/' download_data_to_local(url, local_file) data_list = [] with open(local_file, r) as f: for line in f: data_dict = json.load(line) data_list.append(data_dict) self.assertEqual(len(data_list), 3) self.assertEqual(data_list[0][\"price\"], '3.1') self.assertEqual(data_list[0][\"millisUTC\"], '1612738500000') def tearDown(self): os.remove('test_output.json') class TestHamsterwheelDbDag(unittest.TestCase): @mock.patch('snapshot.execute_sql') def save_data_to_local(self, mock_query): mock_data = [ [1, 'Sam', '1600 Pennsylvania Ave.', '555-555-1111', False], [2, 'Josh', '5 Sesame St.', '555-555-2222', True], [3, 'Toby', '1 Main St.', '555-555-3333', True] ] mock_query.return_value = mock_data sql = 'select foo from bar' local_file = 'test_output.json' save_data_to_local(sql, local_file) data_list = [] with open(local_file, r) as f: for line in f: data_dict = json.load(line) data_list.append(data_dict) self.assertEqual(len(data_list), 3) self.assertEqual(len(data_list[0]), 5) self.assertEqual(data_list[1][\"customer_name\"], 'Josh') self.assertTrue(data_list[2][\"is_acrtive\"]) def tearDown(self): os.remove('test_output.json') if __name__ == '__main__': unittest.main() Those DAGs will control your batch pipelines. But two of those DAGs rely on Cloud Functions to trigger, and one of those also needs to run a Spark job. You'll need to write these before all your batch pipelines will work.","title":"Writing Your Batch Pipelines with Composer/Airflow"},{"location":"de-gcp-book/ch_13_up_and_running/#writing-your-event-driven-pipelines-with-cloud-functions-and-dataprocspark","text":"To finish your batch pipelines you'll need the following: 1. A Cloud Function to trigger the Sales Leads DAG. 2. A Cloud Function to trigger the Competitor Products DAG. 3. A Spark job to process the competitor products data. At the top level of your repo create your file structure by executing the following: > mkdir functions > mkdir functions/sales_leads > touch functions/sales_leads/main.py > touch functions/sales_leads/requirements.txt > mkdir functions/competitor_products > touch functions/competitor_products/main.py > touch functions/competitor_products/requirements.txt To trigger a DAG from your Cloud Function, you need to get your Airflow Client ID. To do that, create and execute this Python script: # client_id.py import google.auth import google.auth.transport.requests import requests import six.moves.urllib.parse # Authenticate with Google Cloud. # See: https://cloud.google.com/docs/authentication/getting-started credentials, _ = google.auth.default( scopes=['https://www.googleapis.com/auth/cloud-platform']) authed_session = google.auth.transport.requests.AuthorizedSession(credentials) project_id = 'de-book-prod' location = 'us-central1' composer_environment = 'batch_jobs' environment_url = ( 'https://composer.googleapis.com/v1beta1/projects/{}/locations/{}' '/environments/{}').format(project_id, location, composer_environment) composer_response = authed_session.request('GET', environment_url) environment_data = composer_response.json() airflow_uri = environment_data['config']['airflowUri'] # The Composer environment response does not include the IAP client ID. # Make a second, unauthenticated HTTP request to the web server to get the # redirect URI. redirect_response = requests.get(airflow_uri, allow_redirects=False) redirect_location = redirect_response.headers['location'] # Extract the client_id query parameter from the redirect. parsed = six.moves.urllib.parse.urlparse(redirect_location) query_string = six.moves.urllib.parse.parse_qs(parsed.query) print(query_string['client_id'][0]) > python client_id.py abc123 In addition to your Client ID, you'll need your Webserver ID: > gcloud composer environments describe batch_jobs \\ --location us-central1 \\ --format=\"get(config.airflowUri)\" Your Webserver ID is the first part of your webserver URL. e.g. <Webserver ID>.appspot.com In the main.py file for Sales Leads you'll need: # This code is lifted from: https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf from google.auth.transport.requests import Request from google.oauth2 import id_token import requests IAM_SCOPE = 'https://www.googleapis.com/auth/iam' OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token' def trigger_dag(data, context=None): \"\"\"Makes a POST request to the Composer DAG Trigger API When called via Google Cloud Functions (GCF), data and context are Background function parameters. For more info, refer to https://cloud.google.com/functions/docs/writing/background#functions_background_parameters-python To call this function from a Python script, omit the ``context`` argument and pass in a non-null value for the ``data`` argument. \"\"\" # Fill in with your Composer info here # Navigate to your webserver's login page and get this from the URL # Or use the script found at # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/composer/rest/get_client_id.py client_id = 'YOUR-CLIENT-ID' # This should be part of your webserver's URL: # {tenant-project-id}.appspot.com webserver_id = 'YOUR-TENANT-PROJECT' # The name of the DAG you wish to trigger dag_name = 'sales_leads' webserver_url = ( 'https://' + webserver_id + '.appspot.com/api/experimental/dags/' + dag_name + '/dag_runs' ) # Make a POST request to IAP which then Triggers the DAG make_iap_request( webserver_url, client_id, method='POST', json={\"conf\": data, \"replace_microseconds\": 'false'}) # This code is copied from # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py # START COPIED IAP CODE def make_iap_request(url, client_id, method='GET', **kwargs): \"\"\"Makes a request to an application protected by Identity-Aware Proxy. Args: url: The Identity-Aware Proxy-protected URL to fetch. client_id: The client ID used by Identity-Aware Proxy. method: The request method to use ('GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE') **kwargs: Any of the parameters defined for the request function: https://github.com/requests/requests/blob/master/requests/api.py If no timeout is provided, it is set to 90 by default. Returns: The page body, or raises an exception if the page couldn't be retrieved. \"\"\" # Set the default timeout, if missing if 'timeout' not in kwargs: kwargs['timeout'] = 90 # Obtain an OpenID Connect (OIDC) token from metadata server or using service # account. google_open_id_connect_token = id_token.fetch_id_token(Request(), client_id) # Fetch the Identity-Aware Proxy-protected URL, including an # Authorization header containing \"Bearer \" followed by a # Google-issued OpenID Connect token for the service account. resp = requests.request( method, url, headers={'Authorization': 'Bearer {}'.format( google_open_id_connect_token)}, **kwargs) if resp.status_code == 403: raise Exception('Service account does not have permission to ' 'access the IAP-protected application.') elif resp.status_code != 200: raise Exception( 'Bad response from application: {!r} / {!r} / {!r}'.format( resp.status_code, resp.headers, resp.text)) else: return resp.text # END COPIED IAP CODE Be sure to fill in your Client ID and Webserver ID in the code above. In the same folder, create requirements.txt : requests_toolbelt==0.9.1 google-auth==1.24.0 In the main.py file for Competitor Products you'll need: # This code is lifted from: https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf from google.auth.transport.requests import Request from google.oauth2 import id_token import requests IAM_SCOPE = 'https://www.googleapis.com/auth/iam' OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token' def trigger_dag(data, context=None): \"\"\"Makes a POST request to the Composer DAG Trigger API When called via Google Cloud Functions (GCF), data and context are Background function parameters. For more info, refer to https://cloud.google.com/functions/docs/writing/background#functions_background_parameters-python To call this function from a Python script, omit the ``context`` argument and pass in a non-null value for the ``data`` argument. \"\"\" # Fill in with your Composer info here # Navigate to your webserver's login page and get this from the URL # Or use the script found at # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/composer/rest/get_client_id.py client_id = 'YOUR-CLIENT-ID' # This should be part of your webserver's URL: # {tenant-project-id}.appspot.com webserver_id = 'YOUR-TENANT-PROJECT' # The name of the DAG you wish to trigger dag_name = 'competitor_products' webserver_url = ( 'https://' + webserver_id + '.appspot.com/api/experimental/dags/' + dag_name + '/dag_runs' ) # Make a POST request to IAP which then Triggers the DAG make_iap_request( webserver_url, client_id, method='POST', json={\"conf\": data, \"replace_microseconds\": 'false'}) # This code is copied from # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py # START COPIED IAP CODE def make_iap_request(url, client_id, method='GET', **kwargs): \"\"\"Makes a request to an application protected by Identity-Aware Proxy. Args: url: The Identity-Aware Proxy-protected URL to fetch. client_id: The client ID used by Identity-Aware Proxy. method: The request method to use ('GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE') **kwargs: Any of the parameters defined for the request function: https://github.com/requests/requests/blob/master/requests/api.py If no timeout is provided, it is set to 90 by default. Returns: The page body, or raises an exception if the page couldn't be retrieved. \"\"\" # Set the default timeout, if missing if 'timeout' not in kwargs: kwargs['timeout'] = 90 # Obtain an OpenID Connect (OIDC) token from metadata server or using service # account. google_open_id_connect_token = id_token.fetch_id_token(Request(), client_id) # Fetch the Identity-Aware Proxy-protected URL, including an # Authorization header containing \"Bearer \" followed by a # Google-issued OpenID Connect token for the service account. resp = requests.request( method, url, headers={'Authorization': 'Bearer {}'.format( google_open_id_connect_token)}, **kwargs) if resp.status_code == 403: raise Exception('Service account does not have permission to ' 'access the IAP-protected application.') elif resp.status_code != 200: raise Exception( 'Bad response from application: {!r} / {!r} / {!r}'.format( resp.status_code, resp.headers, resp.text)) else: return resp.text # END COPIED IAP CODE You can use the same Client ID and Webserver ID you used above. Again, you'll need to create requirements.txt in the same folder: requests_toolbelt==0.9.1 google-auth==1.24.0 These two cloud functions are largely identical. However, because these functions are being deployed independently you cannot DRY out this code by abstracting the common code into a shared module. A potential enhancement is to create a single Cloud Function to trigger DAGs, and infer the DAG name from the event data, such as the name of the bucket a file was loaded into. The last piece of your batch pipelines is your Spark job for the competitor products data. This is necessary because the JSON schema contains an array-of-arrays, which is a structure that cannot be loaded into BigQuery. You are using Spark for this (as opposed to e.g. Pandas) because you suspect that this file will periodically be too large to fit into memory. Start by adding the pyspark_jobs folder to the top level of your repo. Inside pyspark_jobs create competitor_products.py : #!/usr/bin/env python \"\"\" This pyspark job takes a JSON file of competitor products and transforms it. Each record in that file contains the \"product_groups\" field, which is an array-of-arrays. This script will change that field to an array-of-strings by concatenating the inner array. The other fields will be unchanged. \"\"\" import argparse import os from pyspark.sql import SparkSession from pyspark.sql.functions import col, concat_ws def generate_clean_blob_path(bucket_name: str, blob_prefix: str) -> str: \"\"\" This function is used to generate the path that the clean file will be saved to. For example, the raw file may be read from \"gs://my-bucket/competitor_products/20211225/raw/competitor_products.json\", and the transformed file will be saved to \"gs://my-bucket/competitor_products/20211225/clean/\". ## PARAMETERS ## bucket_name: String. Name of GCS bucket where the data exists. blob_prefix: String. Name of the blob, including folders within the bucket, if any. ## returns: String. The GCS path to save the transformed data. \"\"\" new_blob_prefix = os.path.dirname(os.path.dirname(blob_prefix)) destination = os.path.join('gs://', new_blob_prefix, \"clean\") return destination def main(bucket_name: str, blob_prefix: str) -> str: \"\"\" This function reads a competitor_products.json file into a dataframe, transform the \"product_groups\" field, then saves the data back to GCS. ## PARAMETERS ## bucket_name: String. Name of GCS bucket where the data exists. blob_prefix: String. Name of the blob, including folders within the bucket, if any. ## returns: None \"\"\" spark = SparkSession.builder.appName(\"competitor_products\").getOrCreate() df = spark.read.json(f\"gs://{bucket_name}/{blob_prefix}\") df2 = df.withColumn(\"product_groups\", concat_ws(\"; \", col(\"product_groups\"))) destination = generate_clean_blob_path(bucket_name, blob_prefix) df2.write.json(destination, mode=\"overwrite\") if __name__ == \"__main__\": parser = argparse.ArgumentParser() parser.add_argument( \"--bucket_name\", dest=\"bucket_name\", required=True, help=\"Name of GCS bucket where the data exists.\" ) parser.add_argument( \"--blob_prefix\", dest=\"blob_prefix\", required=True, help=\"Name of the blob, including folders within the bucket, if any.\" ) args = parser.parse_args() main(bucket_name=args.bucket_name, blob_prefix=args.blob_prefix)","title":"Writing Your Event-Driven Pipelines with Cloud Functions and Dataproc/Spark"},{"location":"de-gcp-book/ch_13_up_and_running/#writing-your-streaming-pipeline-with-pubsub-cloud-functions-and-gke","text":"","title":"Writing your Streaming Pipeline with Pub/Sub, Cloud Functions, and GKE"},{"location":"de-gcp-book/ch_13_up_and_running/#deploying-your-code-with-cloud-build","text":"Now you need to get your code onto GCP so your data will start flowing. You need to give it code to execute so it can start moving and storing data. In particular you need your deployment process to do the following: 1. Execute your automated tests 2. Deploy your Cloud Functions 3. Deploy your Composer DAGs 4. Deploy your Dataproc job 5. Deploy your GKE image A note about GKE: You may have noticed that I did not cover Google Kubernetes Engine (GKE) previously in this book. GKE is a powerful tool, but it's also a big topic to cover, so I left it out for the sake of brevity. We'll be using it here, and this code should be a good starting point if you want to learn more. I mention this only so you don't waste your time looking for where I covered GKE in more detail in previous chapters, because I didn't. Before you can complete your Cloud Build file you'll need a few things. First, you'll need the bucket name where Composer will look for new DAGs: > gcloud composer environments describe prod_environment \\ --location us-central1 \\ --format=\"get(config.dagGcsPrefix)\" gs://us-central1-prod-environment-abc123456-bucket/dags In the top level of your repo create your cloudbuild.yaml file: # cloudbuild.yaml steps: # run automated tests - name: 'docker.io/library/python:3.7' id: Test entrypoint: /bin/sh args: [-c, 'python -m unittest dags/tests/'] # deploy DAGs - name: gcr.io/cloud-builders/gcloud id: Deploy entrypoint: bash args: [ '-c', 'gsutil -m rsync -d -r ./dags gs://${_COMPOSER_BUCKET}/dags'] substitutions: _COMPOSER_BUCKET: us-central1-prod-environment-abc123456-bucket # You retrieved this value above # build the GKE container image - name: \"gcr.io/cloud-builders/docker\" args: [\"build\", \"-t\", \"gcr.io/project-id/image:tag\", \".\"] # push the GKE container image - name: \"gcr.io/cloud-builders/docker\" args: [\"push\", \"gcr.io/project-id/image:tag\"] # deploy container image to GKE - name: \"gcr.io/cloud-builders/gke-deploy\" args: - run - --filename=kubernetes-resource-file - --image=gcr.io/project-id/image:tag - --location=location - --cluster=cluster","title":"Deploying Your Code with Cloud Build"},{"location":"de-gcp-book/ch_13_up_and_running/#final-thoughts","text":"","title":"Final Thoughts"},{"location":"de-gcp-book/ch_13_up_and_running/#cleaning-up","text":"","title":"Cleaning Up"}]}